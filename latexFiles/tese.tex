% vim: foldmethod=marker: foldmarker=\\beginfold,\\endfold: fdl=0
\documentclass[twoside,openright,titlepage,numbers=noenddot,headinclude,  lineheaders footinclude=true,cleardoublepage=empty,
                                BCOR=5mm,paper=a4,fontsize=12pt ]{scrbook} 

%%% Pacotes utilizados na dissertação
% \\beginfold Pacotes
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[unicode]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[eulerchapternumbers,beramono]{classicthesis} 
%\usepackage{estilo}
\usepackage[T1]{fontenc}                       
\usepackage{graphicx}
\usepackage[brazil,portuguese, english]{babel}
\usepackage{hyperref}
\usepackage{amsmath, bm}
\usepackage{bbm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{breqn}
\usepackage{pdfpages}
\usepackage{makeidx}
\usepackage{blindtext}
\usepackage{forest}
\usepackage{xcolor}
\usepackage{pgfplots}


\usepackage{mathrsfs}
\usepackage{tikz}
\usepackage{ifthen}
\usepackage{enumerate}
\usepackage{calc}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{positioning}

%% Estes pacotes abaixo eu adicionei 
\usepackage{mathrsfs}
\usepackage{enumerate}
%\usepackage{enumitem}
\usepackage[titles]{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\renewcommand{\cftsubsecleader}{\cftdotfill{\cftdotsep}}
\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}
%\renewcommand{\cftchapleader}{\cftdot{}}
\usepackage{epigraph}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=octave,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\usepackage{tikz-cd}
\usepackage{caption}
\usepackage{tikz}
\usetikzlibrary{automata,arrows,positioning,calc}

%\usepackage{bbm}
\DeclareMathOperator\tr{tr}

\addtolength{\headsep}{0.6cm}
\setlength{\textheight}{22.7cm}
\setlength{\textwidth}{16.2cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{0cm}

\linespread{1.3}
%%%%%%%%%%%%%IMPORTANTE%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Para acentos nos títulos use o comando latex%%%
%%% \'a, \~a \^e, etc%%%%%%%%%%%%%%%%%%%%%%
% \\endfold Pacotes

%%% Definicao de teoremas, lemas, corolários...
% \\beginfold Teoremas
\newtheorem{teo}{Teorema}[chapter]
\newtheorem*{teo*}{Teorema}
\newtheorem{lema}[teo]{Lema}
\newtheorem{prop}[teo]{Proposição}
\newtheorem*{lema*}{Lema}
\newtheorem*{prop*}{Proposição}
\newtheorem{cor}[teo]{Corolário}

\theoremstyle{definition}
\newtheorem{definicao}{Definição}[chapter]
\newtheorem{exmp}{Exemplo}[section]
\newtheorem*{obs*}{Observação}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Adj}{Ad}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\PPP}{PPP}
\DeclareMathOperator{\im}{Im}

% \\endfold Teoremas

%%% Configurações
% \\beginfold Configuracoes

\newcommand{\autor}{Rafael Polli Carneiro}
%  Se for do sexo feminino, descomente a linha a seguir.
% \def\femaleAuthor{}
%  Substituir 'Título da defesa' pelo título da defesa.
\newcommand{\titulo}{Uma Abordagem Geométrica no Estudo de Processos Markovianos}
%  Se estiver no programa de mestrado, descomente a linha a seguir.
 \def\mestrado{}
%  Substituir 'Nome completo do orientador' pelo nome completo do seu
% orientador.
\newcommand{\orientador}{Cristian Favio Coletti}
%  Se for orientado por uma mulher, descomente a linha a seguir.
% \def\femaleOrientador{}
%  Substituir 'Nome completo do coorientador' pelo nome completo do seu
% coorientador. Caso não tenha coorientador, comente a linha a seguir.
%\newcommand{\coorientador}{Sandra Maria Zapata Yepes}
%  Se for coorientado por uma mulher, descomente a linha a seguir.
\def\femaleCoorientador{}
%  Substituir 'Ano' pelo ano em que ocorreu sua defesa.
\newcommand{\fomento}{da ``CAPES''}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Versão
%%Se for a versão final, com as correções sugeridas pela banca descomente a linha abaixo
% \def\versaofinal{}

\newcommand{\ano}{2018}
\newcommand{\centro}{Centro de Matemática, Computação e Cognição \xspace}
\newcommand{\titulacao}{Mestre em Matemática \xspace}
\newcommand{\palavraschaves}{ Sistema de Partículas, Grupo de Matrizes de Lie, Coproduto}
\newcommand{\keywords}{Particle System, Matrix Lie Group, Coproduct} 
\makeindex
% \\endfold Configuracoes


%%% Início do texto da defesa 
\begin{document}

%%% Capa, folha de rosto, tabela de conteudo,...
% \\beginfold Capa, folha de rosto...
\input{capa}  % Não edite esse arquivo.
\thispagestyle{empty}\newpage\mbox{}\thispagestyle{empty}\newpage\thispagestyle{empty} % Pagina em branco.
\frontmatter

\input{folha-de-rosto}



%%  A ficha catalográfica deve estar no verso da folha de rosto.
%  O arquivo ficha-catalografica.pdf deve ser sobrescrito com uma cópia
% do arquivo pdf que a biblioteca lhe enviar.
%%%\includepdf{ficha-catalografica}
%
%  A folha de aprovação deve ser assinada pelos membros da banca apos a
% defesa.
%  Substitua o arquivo folha-de-aprovacao.pdf por uma copia escaneada.
%\includepdf{folha-aprovacao}
%
%  Se não for incluir a dedicatória, comentar a linha abaixo.
%%%%\input{dedicatoria}
%
%  Se não for incluir os agradecimentos, comentar a linha abaixo.
\input{agradecimentos}
%
%  Se não for incluir a epigrafe, comentar a linha abaixo.
\input{epigrafe}
%
\input{resumo}
\input{abstract}
%
\tableofcontents
%
\mainmatter
% \\endfold Capa, folha de rosto...


%%%%%%%%%%%%%%Capitulos

%%%%%% Início do Texto

%%% [TODO] Introdução 
% \\beginfold Introducao
\chapter{Introdu\c{c}\~ao}
Ao estudarmos cadeias de Markov a tempo discreto e a tempo contínuo nos
interessamos na seguinte pergunta: ''Existe alguma maneira de, através de uma matriz geradora de um processo de Markov
a tempo contínuo, gerarmos uma cadeia de Markov a tempo discreto?''. Além do mais, será que o caminho 
inverso também é possível? 
%Isto é, será que podemos ''gerar uma cadeia de Markov a tempo discreto através
%de uma cadeia de Markov a tempo contínuo?''. 
Em face destas perguntas, tentaremos estudar relações
entre estes processos por meio das matrizes estocásticas e das matrizes geradoras, que caracterizam
estas cadeias de Markov.

Jeremy G. Summer em seu artigo~\cite{paper1} observa, ao estudar as propriedades algébricas do
grupo de matrizes de Lie $\mathcal{G}$,  dado pelas matrizes $2 \times 2$ estocásticas, e de seu espaço tangente $T_1(\mathcal{G})$,
que nada mais é que uma álgebra de Lie, que é possível decompor um subgrupo de $\mathcal{G}$ em função de uma 
matriz geradora $Q$ de um processo de Markov a tempo contínuo, pela seguinte forma
\[ \mathcal{G}^0 = \bigcup_{t \in \mathbb{R}}  e^{Qt} \mathcal{H}, \]
com $\mathcal{G}^0$ dado pelas matrizes estocásticas de determinante positivo e
$\mathcal{H}$ um subgrupo normal de $\mathcal{G}^0$.
Desta forma, para matrizes quadradas $2 \times 2$, obtemos uma resposta positiva para a primeira
pergunta acima. 

Além do mais, Frank Redig no artigo~\cite{paper2} apresenta um procedimento de se construir
cadeias de Markov a tempo contínuo através de uma álgebra de Lie $\mathfrak{g}$ qualquer. Este procedimento consiste em
calcular o coproduto do elemento do Casimir da álgebra $\mathfrak{g}$, cujo resultado será uma cadeia de Markov a tempo contínuo.
Sendo assim, considerando que o espaço tangente do grupo das matrizes estocásticas é uma
álgebra de Lie, obtemos uma forma de construir cadeias de Markov a tempo contínuo através de uma 
estrutura algébrica que representa os processos markovianos a tempo discreto. 

Em posse destes resultados, este trabalho visa generalizar o resultado de \cite{paper1}
para grupo de matrizes estocásticas de dimensão qualquer e, através do espaço tangente deste grupo,
desejamos obter uma matriz geradora, calculada através do coproduto do
elemento do Casimir desta álgebra.

Para encerrar, iremos, a partir da matriz geradora $\bm{L}'$, obtida pelo coproduto do elemento do Casimir
discutido acima,
construir um sistema de partículas de dois sítios e cujo gerador seja dado pela própria $\bm{L}'$. 
Consequentemente, em posse do gerador do sistema de duas partículas, estabeleceremos um gerador
para um sistema de N-partículas, tomando como base o trabalho de \cite{Nparticulas}. 
Em seguida, estudamos operadores
autoduais para o sistema de partículas de N sítios, tomando como base para cálculo o autodual do sistema mais simples,
de duas partículas e de gerador dado pela matriz $\bm{L}'$. Para a construção dos autoduais nos baseamos
nos trabalhos de \cite{daniela} e \cite{redig}. Finalmente, concluímos nosso trabalho provando a versão
estocástica do Teorema de Noether.
Em suma, nosso objetivo de trabalho pode ser descrito pelo diagrama da Figura~\ref{obj}.

\begin{figure}[h]
%\begin{center}
\centering
\begin{forest}
for tree={
  draw,
  minimum height=2cm,
  anchor=north,
  align=center,
  child anchor=north
},
[{Inicialmente, construímos o Grupo\\das matrizes estocásticas}, align=center, name=SS
  [{Através deste grupo,\\ obtemos o Espaço Tangente $\mathfrak{g}$}, name=PDC
    [{\textbf{Procedimento 1}\\Dada uma matriz geradora\\do Espaço Tangente $\mathfrak{g}$,\\calculamos matrizes estocásticas}]
    [{\textbf{Procedimento 2}\\Calculamos o elemento do Casimir $C$\\ desta álgebra e definimos uma coálgebra $\Delta$\\
      em $\mathfrak{g}$ tal que tenhamos uma estrutura de biálgebra}
        [{Obtemos uma matriz geradora\\ através do coproduto do Casimir $\Delta(C)$}
			[{\textbf{Procedimento 3}\\ Dada a matriz geradora acima\\
			  construímos um sistema de duas partículas\\
			  e definimos uma função autodual para este sistema.}
			  [{Estendemos o sistema de duas para N-partículas.\\
			    Assim como estabelecemos um operador autodual para o mesmo}]
			  ]
		]
    ]
  ]
]
\end{forest}
%\end{center}
\caption{Objetivos do trabalho}
\label{obj}
\end{figure}

Os capítulos desta dissertação distribuem-se da seguinte forma: 
\begin{description}
    \item[Cap\'itulo 2:] Breve discussão sobre cadeias de Markov a tempo discreto
	e contínuo. 
    \item[Cap\'itulo 3:] Aqui descrevemos as propriedades básicas de grupos, grupos topológicos e
    mostramos que a componente conexa da identidade de um grupo topológico é um subgrupo normal, fechado
    e conexo. Além do mais, mostramos que o grupo linear geral satisfaz a condição de ser um grupo topológico.
    Em seguida, definimos os grupos de matrizes de Lie.
    \item[Cap\'itulo 4:] Introduzimos a noção de álgebras, álgebras de Lie e espaços tangentes.
    Mostramos também a importante propriedade do espaço tangente ser uma álgebra de Lie.
    \item[Cap\'itulo 5:] Em posse dos resultados dos capítulos anteriores, mostramos que o
    grupo das matrizes estocásticas é um grupo de matrizes de Lie e calculamos seu espaço
    tangente. Em seguida, propriedades referentes a componente conexa do grupo das matrizes estocásticas
    são demonstradas. Tais propriedades nos possibilitam decompor a componente conexa por meio
    de uma matriz geradora, utilizando a exponencial de matrizes. Esta decomposição, como desejado
    no \textbf{Procedimento 1} (vide Figura~\ref{obj}), nos possibilita gerar matrizes estocásticas através de uma
    matriz geradora. Em seguida, damos alguns exemplos de como gerar matrizes de transição por
    meio da decomposição da componente conexa do grupo das matrizes estocásticas.
	Para finalizar o capítulo, obtemos uma matriz geradora, como descrito no \textbf{Procedimento 2} (Figura~\ref{obj}),
    a partir de uma subálgebra semi-simples do espaço tangente  do grupo das matrizes $3 \times 3$
	estocásticas.
	\item[Cap\'itulo 6:] Inicialmente abordamos os conceitos básicos de um sistema de partículas
	e como este pode ser visto como um processo de Markov a tempo contínuo. Em seguida, 
	de acordo com o desejado no \textbf{Procedimento 3} (vide Figura~\ref{obj}) construiremos
	um sistema de duas partículas através da matriz geradora obtida no final do capítulo 5.
	Com o sistema de duas partículas estabelecido, estendemos este sistema para um sistema
	de N-partículas, o qual nos  baseamos no trabalho de \cite{Nparticulas}. Dados que a
	partir desta metodologia, somos capazes de criar sistemas de N-partículas, iremos
	estudar operadores de autodualidade para estes sistemas. Tomando como referência
	os trabalhos de \cite{daniela} e \cite{redig}, estabelecemos uma função autodual
	para nosso sistema de duas partículas e, a partir do gerador de um sistema de
	N-partículas, já estabelecido, estendemos o autodual do sistema de duas
	para o sistema de N-partículas. Para encerrar o capítulo, enunciamos e provamos
	o Teorema de Noether, versão estocástica.
 
\end{description}

% \\endfold Introducao

%%%% [TODO] introdução a processos markovianos
%%% \beginfold
\chapter{Uma abordagem sucinta sobre processos markovianos}
Neste capítulo introduziremos os alicerces básicos de probabilidade
para enunciarmos o conceito de cadeias de Markov a tempo discreto e contínuo.
As ideias e demonstrações apresentadas aqui podem ser vistas nos 
\mbox{livros \cite{barry}, \cite{feller} e \cite{durret1}}.

\section{Espa\c{c}o de Probabilidades}
Para estudarmos a noção da probabilidade de que certos eventos ocorram, como,
por exemplo, a probabilidade de uma moeda lançada $10$ vezes dar cara em todos
lançamentos, devemos estabelecer certas condições para termos esta medida de probabilidade
bem definida. Para isto, consideremos o conjunto $\Omega$ como o conjunto de todos os
resultados possíveis de um certo experimento, em nosso exemplo teríamos $\Omega
= \{ \text{cara}, \text{ coroa} \}$, 
ou, um outro exemplo, poderíamos ter $\Omega = \mathbb{R}$, com $\Omega$ representando o tempo de vida possível
de uma população qualquer. (observe que o conjunto $\Omega$ pode englobar muito mais valores do que alguém
esperaria na prática.)

Associado ao conjunto $\Omega$ temos a $\sigma$-álgebra $\mathscr{A}$ (vide o texto de medida \cite{fernandez} ou \cite{isnard})
 representando todos os eventos possíveis de um experimento. Utilizando ainda
o exemplo do lançamento de uma moeda $10$ vezes, podemos ilustrar como eventos possíveis deste
experimento os eventos: ``a quantidade de vezes que a moeda deu cara foi um número par'' ou
``o número de vezes que a moeda deu cara é maior que o número de vezes que a moeda deu
coroa''... Uma $\sigma$-álgebra $\mathscr{A}$ é um subconjunto do conjunto 
de partes de $\Omega$, i.e., $\mathscr{A} \subseteq \mathscr{P}(\Omega)$, que satisfaz as condições:
\begin{enumerate}[(i)]
\item $\Omega, \emptyset \in \mathscr{A}$;
\item Dada uma sequência de eventos $(A_i)_{i \in \mathbb{N}}$, com $A_i \in \mathscr{A}$,
para todo natural $i$. Então, 
\[
\bigcup_{i \in \mathbb{N}} A_i \in \mathscr{A};
\]
\item Dado o evento $A \in \mathscr{A}$ então $\Omega \setminus A \in \mathscr{A}$.
\end{enumerate}

Uma $\sigma$-álgebra importante em probabilidade é a $\sigma$-álgebra de Borel, definida
da seguinte forma: seja $\Omega$ um espaço topológico e $S$ o conjunto formado
por todos os abertos deste espaço, então, a $\sigma$-álgebra  gerada por $S$
\[
\mathscr{B} = \bigcap_{ \substack{ \mathscr{A} \subseteq \mathscr{P}(\Omega);\\  
\text{e }S \subseteq \mathscr{A} } } \mathscr{A}, \qquad \text{ com } \mathscr{A} \text{ uma $\sigma$-álgebra },
\]
é denotada
por $\sigma$-álgebra de Borel, cujos elementos são chamados de borelianos. Segue abaixo uma caracterização,
cuja demonstração pode ser vista em~\cite{isnard},
da $\sigma$-álgebra de Borel para o $\mathbb{R}^n$:
\begin{prop}
Todo intervalo da reta real é um boreliano, e o conjunto de todos intervalos limitados $I$ em $\mathbb{R}^n$
gera a $\sigma$-álgebra de Borel no $\mathbb{R}^n$.
\end{prop}

Seja $\Omega$ um conjunto não vazio, $\mathscr{A}$ uma $\sigma$-álgebra de subconjuntos de $\Omega$.
Dizemos que $P: \mathscr{A} \rightarrow [0,1]$ é uma medida de probabilidade se
\begin{enumerate}[(i)]
\item $P[\Omega] = 1$;
\item Para toda sequência enumerável de eventos dois a dois disjuntos, $(A_i)_{i \in \mathbb{N}}$, com $A_i \in \mathscr{A}$ para todo 
natural, tivermos
\[
P[ \cup_{i = 1}^{\infty} A_i ] = \sum_{i=1}^{\infty} P[A_i].
\]
\end{enumerate}
Em outras palavras, $P$ deve ser uma medida $\sigma$-aditiva. Denotaremos a terna
$(\Omega, \mathscr{A}, P)$ por espaço de probabilidade e, salvo quando não houver confusão,
denotaremos nosso espaço de probabilidade por apenas $\Omega$.

Para encerrar esta seção, definimos o conceito de eventos independentes e probabilidade condicional.
Seja $(\Omega, \mathscr{A}, P)$ um espaço de probabilidade e $A, B \in \mathscr{A}$ eventos
quaisquer. Dizemos que os eventos $A$ e $B$ são independentes quando a igualdade
\[
P[A \cap B] = P[A] P[B]
\]
é satisfeita. Além do mais, definimos a probabilidade condicional $P[A|B]$ como sendo a probabilidade de um evento $A$ ocorrer dado 
que o evento $B$ já ocorreu. Esta probabilidade é definida por
\[
P[A | B] = \frac{ P[A \cap B] }{P[B]},
\]
com $P[B] \neq 0$. No caso em que tivermos $P[B] = 0$ definimos a probabilidade condicional
como sendo $P[A|B] = P[A]$.


\section{Vari\'aveis Aleat\'orias}
Em probabilidade é comum associar a espaços de probabilidade $(\Omega, \mathscr{A}, P)$ funções que avaliam, de 
alguma maneira, os resultados possíveis do experimento descrito por $\Omega$. Por exemplo, suponhamos que certo pesquisador
deseja estudar o tempo de vida de uma população de bactérias armazenadas em ensaios clínicos. Sabemos que
o tempo de vida destas bactérias não será o mesmo, ou seja, existe uma aleatoriedade a ser considerada quando
estudamos o tempo de vida destas bactérias. Portanto, considerando $\Omega$ como o conjunto que descreve todo tempo de vida 
possível das bactérias
em estudo, podemos, então, associar uma função $X: \Omega \rightarrow \mathbb{R}$ que nos informa o tempo
de vida das bactérias, e tal função será chamada de variável aleatória. 

Mais formalmente, uma  variável aleatória
é uma função $X: (\Omega_1, \mathscr{A}_1) \rightarrow (\Omega_2, \mathscr{A}_2)$ entre $\sigma$-álgebras,
$\mathscr{A}_1 \subseteq \mathscr{P}(\Omega_1)$ e $\mathscr{A}_2 \subseteq \mathscr{P}(\Omega_2)$, satisfazendo
\[
\forall B \in \mathscr{A}_2 \quad \text{tem-se } X^{-1}(B) \in \mathscr{A}_1,
\]
ou seja, são funções mensuráveis (vide \cite{isnard}).
Comumente, as variáveis aleatórias são classificadas como discretas ou contínuas e, tais 
denominações, tem relação com a forma como avaliamos as probabilidades dos eventos ocorrerem para
cada variável.

Uma variável aleatória $X$ é classificada como discreta se o conjunto dos possíveis resultados de um
experimento $\Omega$ for enumerável, denotado por $\{x_1, x_2, \ldots\}$, e, para cada resultado possível, tivermos
%seu espaço de probabilidade $\Omega$ for
%enumerável e, para cada $x_i \in \Omega$ tivermos
\[
\sum_{i = 1}^{|\Omega|} P[ X = x_i ] = 1
\]
enquanto que, uma variável aleatória é chamada de contínua quando o conjunto de resultados possíveis
puder ser descrito pela reta real, caso unidimensional, ou pelo $\mathbb{R}^n$, caso
multidimensional, e existir uma função $f: \mathbb{R}^n \rightarrow \mathbb{R}$ (com $n \in \mathbb{N}$
um natural fixado), chamada de função densidade, tal que, para todo boreliano $B$, tenhamos
\[
P[ X \in B] = \int_B f\, dP,
\]
com $[X \in B] = X^{-1}(B)$. Segue
abaixo alguns exemplos das variáveis aleatórias mais comuns:
\begin{exmp}
Uma variável aleatória $X$ tem distribuição de probabilidade bernoulli de parâmetro $\theta$,
denotada por $X \sim bernoulli(\theta)$, quando os únicos valores possíveis de $X$ são os
valores $1$ ou $0$, com probabilidade
\[
P[X = 1] = \theta \qquad \text{e} \qquad P[X = 0] = 1 - \theta.
\]
\end{exmp}
 
\begin{exmp}
Dizemos que a variável $X: \Omega \rightarrow \mathbb{N}\cup \{0\}$ tem distribuição Poisson de parâmetro $\theta \geq 0$,
denotada por $X \sim Poisson(\theta)$, quando, para todo $i = 0, 1, 2, \ldots$, tivermos
\[
P[ X = i ] = e^{-\theta} \frac{\theta^i }{i!}.
\]
\end{exmp}
Estes dois exemplos foram de variáveis aleatórias discretas. A seguir, fornecemos
exemplos de variáveis contínuas:
\begin{exmp}
Uma variável aleatória $X$ tem distribuição uniforme no intervalo $[a,b]$,
$X \sim U[a,b]$, com $a < b$, se $X$ tiver função de densidade $f: \mathbb{R} 
\rightarrow \mathbb{R}$ valendo
\[
f(x) = 
\begin{cases}
\frac{1}{b-a} & \text{se } x \in [a,b];\\
0 & \text{caso contrário}.
\end{cases}
\]
\end{exmp}

\begin{exmp}
$X$ tem distribuição exponencial com parâmetro $\theta > 0$, $X \sim exp(\theta)$,
se $X$ tiver função de densidade $f: \mathbb{R} \rightarrow \mathbb{R}$
satisfazendo
\[
f(x) = 
\begin{cases}
\theta e^{-\theta x} & \text{se } x \geq 0;\\
0 & \text{caso contrário}.
\end{cases}
\]
\end{exmp}
Segue da variável $X$ ter distribuição exponencial com parâmetro $\theta$ a importante propriedade, chamada de perda de memória,
que diz: a probabilidade do evento ``$X > a + b''$, com $a, b$ constantes, dado que o evento
``$X > a$'' já ocorreu é igual a probabilidade do evento ``$X > b$'' ocorrer. Em outras palavras,
\[
P[ X > a + b| X > a] = P[ X > b].
\]
Esta propriedade ocorre pois
\begin{align*}
P[ X > a + b| X > a] &= \frac{ \int_{a+b}^{\infty} \theta e^{-\theta x} dx }{ \int_{a}^{\infty} \theta e^{-\theta x} dx }\\
					 &= \frac{ e^{-\theta (a+b) }}{ e^{-\theta a}}\\
					 &= e^{-\theta b}\\
					 &= P[ X > b].
\end{align*}
Para encerrar nossos exemplos de variáveis contínuas, enunciamos a variável
de uso mais rotineiro em aplicações:
\begin{exmp}
Uma variável aleatória tem distribuição normal de parâmetros $\mu \text{ e }\sigma$,
$X \sim N(\mu, \sigma^2)$, se sua função de densidade for
\[
f(x) = \frac{1}{\sigma \sqrt{2 \pi} } e^{ \frac{ -(x - \mu)^2 }{2 \sigma^2} }
\]
\end{exmp}
A função de densidade da variável normal é conhecida pelo formato de sino, como
exemplificada na Figura~\ref{fig:normal}.

\begin{figure}[h]
\centering
\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\begin{tikzpicture}
\begin{axis}[
  no markers, domain=0:8, samples=100,
  axis lines*=left, xlabel=$x$, ylabel=$y$,
  every axis y label/.style={at=(current axis.above origin),anchor=south},
  every axis x label/.style={at=(current axis.right of origin),anchor=west},
  height=5cm, width=12cm,
  xtick={4}, ytick=\empty,
  enlargelimits=false, clip=false, axis on top,
  grid = major
  ]
  %\addplot [fill=cyan!20, draw=none, domain=0:5.96] {gauss(6.5,1)} \closedcycle;
  \addplot [very thick,cyan!50!black] {gauss(4,1)};
  %\addplot [very thick,cyan!50!black] {gauss(6.5,1)};


%\draw [yshift=-0.6cm, latex-latex](axis cs:4,0) -- node [fill=white] {$1.96\sigma$} (axis cs:5.96,0);
\end{axis}

\end{tikzpicture}
\caption{Função densidade de uma normal $X \sim N(4,1)$.}
\label{fig:normal}
\end{figure}

Seja $X$ uma variável aleatória discreta ou contínua, com $f$ a função
densidade no caso de variável contínua, e $\Omega$ o espaço de probabilidade. 
Definimos como média ou esperança de $X$, denotada por $\E[X]$, e
variância de $X$, denotada por $\Var[X]$, as seguintes medidas resumo:
\[
\E[X] = \int_{\Omega} X dP = \int_{\Omega} xf(x)dx \quad \text{e} \quad \Var[X] = \E[ (X - \E[X] )^2].
\]
A esperança acima foi definida para uma variável aleatória contínua, no caso da variável ser discreta,
a média será dada por:
\[
\E[X] = \sum_{i \in X(\Omega)} i P[X = i].
\]
A esperança pode ser interpretada como uma medida que nos informa onde os valores em 
estudo se concentram, enquanto a variância calcula a dispersão dos dados ao redor da média.
Outra medida muito usada é o desvio padrão, que nada mais é que a raiz quadrada da variância.

\begin{exmp}
Uma variável aleatória $X \sim Poisson(\theta)$ tem média e variância dados por
$\E[X] = \theta$  e $\Var[X] = \theta$. Isto é verdade, pois
basta observar que para
\[
\E[X] = \sum_{i=0}^{\infty} i e^{-\theta} \frac{\theta^i}{i!}
\]
temos 
\begin{align*}
\E[X] &= \sum_{i=1}^{\infty} e^{-\theta} \frac{\theta^i}{(i-1)!}\\
     &= \theta \sum_{i=1}^{\infty} e^{-\theta} \frac{\theta^{(i-1)}}{(i-1)!}\\
	 &= \theta.
\end{align*}
Para calcular a variância, utilizamos a igualdade $\Var[X] = \E[X^2] - ( \E[X] )^2$ (vide~\cite{barry}).
Logo, para $\E[X^2]$ teremos
\begin{align*}
\E[X^2] &= \sum_{i=0}^{\infty} i^2 e^{-\theta} \frac{\theta^i}{i!}\\
       &= \theta \sum_{i=0}^{\infty}(i+1) e^{-\theta} \frac{\theta^{i}}{i!}\\
	   &= \theta (\theta + 1).
\end{align*}
Desta forma,
\[
\Var[X] = \theta ( \theta +1) - \theta^2 = \theta.
\]
\end{exmp}

\begin{exmp}
Uma variável aleatória com distribuição exponencial de parâmetro $\theta$, $X \sim exp(\theta)$,
tem esperança $\E[X] = 1/\theta$ e variância $\Var[X] = 1/\theta^2$. Para a 
demonstração deste fato,  vide~\cite{barry}.
%%. Para calcular a esperança devemos
%%resolver a integral
%%\[
%%E[X] = \int_0^\infty x \theta e^{-\theta x} dx.
%%\]
\end{exmp}

Finalmente, dizemos que duas variáveis aleatórias $X_1$ e $X_2$, definidas no mesmo espaço de probabilidade
$(\Omega, \mathscr{A}, P)$, são independentes se, para quaisquer eventos $A_1, A_2 \in \mathscr{A}$,
tivermos
\[
P( [X_1 \in A_1] \cap [X_2 \in A_2] ) = 
P[X_1 \in A_1] \cdot P[X_2 \in A_2].
\]
Da mesma forma, dizemos que uma sequência de variáveis aleatórias $(X_i)_{i \in \mathbb{N}}$, definidas no mesmo espaço
de probabilidade,
são independentes e identicamente distribuídas, denotadas por i.i.d, se as variáveis $X_i$
forem conjuntamente independentes e possuírem mesma distribuição de probabilidade. Por exemplo, dizemos que a sequência
$(X_i)_{i \in \mathbb{N}}$ é i.i.d com distribuição uniforme $U[0,1]$ quando as variáveis forem
independentes entre si e $X_i \sim U[0,1]$, para $i \in \mathbb{N}$ um valor qualquer.
\section{Processo de Poisson}
Definiremos abaixo o conceito de um Processo de Poisson de taxa $\lambda$ na reta real, e ilustraremos algumas
de suas importâncias.

\begin{definicao}
%$Um processo estocástico é dito ser um Processo de Poisson de taxa $\lambda$ se existir
Dada uma sequência de variáveis aleatórias independente  $T_1, T_2, T_3, \ldots$ com distribuição
exponencial de parâmetro $\lambda$. Dizemos que a variável aleatória
definida por
\[
N(t)= \max \{ n \in \mathbb{N}; T_1 + T_2 + \ldots + T_n \leq t\},
\]
com $t \geq 0$ e $N(0) = 0$, satisfaz o Princípio de Processo de Poisson de taxa $\lambda$, denotado por $\PPP(\lambda)$.
\end{definicao}

Um Processo de Poisson pode ser visto como a contagem de incidências de um certo evento num dado
intervalo de tempo. Um exemplo interessante, o qual pode ser visto com mais detalhes em~\cite{barry},
é o processo de contagem de telefonemas recebidos numa central telefônica que,
de fato, será um processo de Poisson se considerarmos
as seguinte hipóteses:
\begin{enumerate}[(i)]
\item A probabilidade da central telefônica receber telefonemas no intervalo $[s, s+t)$ não
depender do valor $s$, para $t > 0$ qualquer;
\item Os eventos de chamadas telefônicas ocorridas em intervalos de tempo disjuntos são independentes;
\item Chamadas telefônicas não ocorrem ao mesmo tempo.
\end{enumerate}
%obtemos que a variável aleatória que conta as ocorrências de chamadas telefônicas 
%é um Processo de Poisson de taxa $\lambda$.

As demonstrações dos resultados abaixo podem ser vistas em~\cite{durret1}.

\begin{prop}
Seja $N(t)$, com $t \geq 0$, um Processo de Poisson de taxa $\lambda$. Então,
$N(t)$ tem como distribuição de probabilidade uma Poisson de taxa $t \lambda$, i.e.,
\[
P[ N(t) = i] = e^{-t \lambda} \frac{ (t \lambda)^i}{i!}.
\]
Em particular, a quantidade de ocorrências medidas pela variável $N$ no intervalo $[a,b]$ é dada por uma Poisson
de taxa $(b-a) \lambda$. Em outras palavras, $N[a,b] \sim Poisson( (b-a) \lambda )$.
\end{prop}

\begin{exmp} Suponhamos que um site de comércio eletrônico tenha uma quantidade de acessos
por minuto modelada por um processo de Poisson de taxa $10^3$ pessoas por minuto. Dessa forma,
podemos modelar a quantidade média de pessoas que acessam o site em intervalos de tempo
quaisquer. Por exemplo, no período do meio-dia até às 15h temos que a quantidade média
de acessos será da ordem de $3 \cdot 10^3$ pessoas.
\end{exmp}

Outro fato importante é o seguinte:
\begin{prop}
Seja $N(t)$, com $t \geq 0$, um Processo de Poisson de taxa $\lambda$. Então,
as quantidades de ocorrências em intervalos disjuntos são dadas por variáveis
independentes, ou seja, dados os valores positivos $a,b$, temos que a variável
de contagem $N(a+b) - N(a)$ é independente da variável $N(c)$, para todo $c \leq a$.
\end{prop}

\section{Cadeias de Markov}
Seja $\Omega$ um conjunto enumerável, o qual chamaremos de espaço de estados,
$(U_n)_{n \in \mathbb{N}}$ uma sequência i.i.d com $U_1 \sim U[0,1]$, e
$F: \Omega \times [0,1] \rightarrow \Omega$ uma função. Dizemos que a 
sequência de variáveis aleatórias $(X_n)_{n \in \mathbb{N}}$, com espaço de 
estados $\Omega$ (i.e., a variável aleatória $X_n$, para qualquer natural
$n$, toma valores no espaço $\Omega$), é uma Cadeia de Markov a tempo
discreto se a sequência $(X_n)_{n \geq 0}$ satisfazer
\[
X_n = F(X_{n-1}, U_n), \quad \text{para } n \geq 1,
\]
com ponto inicial $X_0$. O ponto inicial do processo markoviano pode ser um valor fixado inicialmente ou pode
ser dado por uma distribuição de probabilidade.
%seguindo uma distribuição de probabilidade $\pi$, ou seja, teremos, por exemplo,
%como estado inicial, $X_0 = a$, com probabilidade $\pi(a)$, para algum $a \in \Omega$.

Decorre, da definição de cadeias de Markov, a seguinte caracterização
dos processos markovianos a tempo discreto:

\begin{prop}
Dado $\Omega$ um conjunto enumerável e $(X_n)_{n \in \mathbb{N}}$
uma sequência de variáveis aleatórias de espaço de estados $\Omega$. Então,
$(X_n)_{n \in \mathbb{N}}$ é uma Cadeia de Markov a tempo discreto se, e somente se, 
para qualquer natural $n$ e para quaisquer valores $a_0, a_1, \ldots, a_n$ 
pertencentes ao espaço de estados, tivermos
\[
P[ X_n = a_n| X_{n-1} = a_{n-1}, X_{n-2} = a_{n-2}, \ldots, X_0 = a_0] = 
P[ X_n = a_n| X_{n-1} = a_{n-1}] = p(a_{n-1}, a_{n}),
\]
com $p: \Omega \times \Omega \rightarrow [0,1]$ a matriz que satisfaz
$\sum_{b \in \Omega} p( a, b) = 1$, para todo $a \in \Omega$. Dizemos, também,
que toda matriz cujas linhas somam um é 
uma matriz de transição.
\end{prop}

Desta caracterização segue o seguinte exemplo
\begin{exmp}\label{exmp:graph}
Um centro meteorológico, situado em São Paulo, avalia 
a probabilidade dos seguintes eventos ocorrerem: o dia 
iniciar chuvoso, ensolarado ou com forte neblina. Denotando por
$0,1,2$ os eventos acima, respectivamente, o centro meteorológico
conclui que a condição climática no início de cada dia pode ser
descrita por uma cadeia de Markov $(X_n)_{n \in \mathbb{N}}$ com
espaço de estados $\Omega = \{0,1,2\}$ 
e matriz de transição dada por
\[
p= 
\bordermatrix{~ & 0 & 1 & 2 \cr
   0 &0,6 &  0,3 & 0,1 \cr 
   1 &0,05 &  0,7 & 0,25 \cr
   2 &0,3 &  0,3 & 0,4\cr}.
\]
Esta cadeia de Markov pode ser visualizada pelo  grafo da Figura~\ref{fig:graph}.
\begin{figure}[h]
\centering
\begin{tikzpicture}[->, >=stealth', auto, semithick, node distance=3cm]
\tikzstyle{every state}=[fill=white,draw=black,thick,text=black,scale=1]
\node[state]    (1)                     {$0$};
\node[state]    (2)[above right of=1]   {$1$};
\node[state]    (3)[below right of=2]   {$2$};
\path
(1) edge[loop left]           node{$0,60$}    (1)
    edge[bend left]           node{$0,30$}    (2)
    edge[bend left,below]     node{$0,10$}    (3)
(2) edge[loop above]          node{$0,70$}     (2)
    edge[near start]          node[below, rotate=45]{$0,05$}     (1)
    edge[midway]          node[near start, below, rotate=-45]{$0,25$}     (3)
(3) edge[loop right]          node{$0,40$}     (3)
    edge[bend right,right]    node{$0,30$}     (2)
    edge[bend left,above]     node{$0,30$}     (1);
\end{tikzpicture}
\caption{Grafo da matriz de transição $p$ da Cadeia de Markov a tempo discreto
com espaço de estados $\Omega = \{0, 1, 2\}$, descrita
no Exemplo~\ref{exmp:graph}.}
\label{fig:graph}
\end{figure}
Além do mais, se considerarmos que os eventos ``chuvoso'', ``ensolarado'' e ``com forte neblina'', 
tem probabilidade de ocorrer, no dia seguinte, dada por
\[\pi(0) = 0,5, \quad \pi(1) = 0,3 \quad \text{ e } \quad  \pi(2) = 0,2,\]
temos que a probabilidade de, daqui a 10 dias, o dia amanhecer chuvoso é igual a 
\[
P[X_{10} = 0] = \pi(0) p^{10}(0,0) + \pi(1) p^{10}(1,0) + \pi(2) p^{10}(2,0) = 0,25005.
\]
\end{exmp}

Dado  uma cadeia de Markov $(X_n)_{n \in \mathbb{N}}$ 
associada ao espaço de estados $\Omega$ e com matriz de transição $p$. Dizemos que uma distribuição de probabilidade
$\pi: \Omega \rightarrow [0,1]$ é invariante se
\[
\sum_{b \in \Omega} \pi(b) P[X_1 = a| X_0 = b] = \pi(a), \text{para todo } a \in \Omega.
\]
Em outras palavras, uma medida de probabilidade sobre o espaço de estados $\Omega$ é invariante
se, para qualquer $n \in \mathbb{N}$, e para qualquer $a \in \Omega$, tivermos
\[
P[X_n = a] = \pi(a),
\]
ou seja, a probabilidade de estar num estado qualquer de seu grafo é um invariante em relação
a qualquer instante de tempo discreto $n \in \mathbb{N}$.
\begin{exmp}
Consideremos a cadeia de Markov do Exemplo~\ref{exmp:graph}. Então, uma medida invariante $\pi$ para esta 
cadeia deve satisfazer
\[
\sum_{b =0}^2 \pi(b) P[X_1 = a| X_0 = b] = \pi(a), \forall a \in \Omega = \{0, 1, 2\}. \\
\]
Logo, a igualdade acima é equivalente a
\[
[\pi(0) \; \pi(1) \; \pi(2)] 
\begin{pmatrix}
0,6 &  0,3 & 0,1 \\ 
0,05 &  0,7 & 0,25 \\
0,3 &  0,3 & 0,4
\end{pmatrix}
= [\pi(0) \; \pi(1) \; \pi(2) ].
\]
Resulta, da igualdade acima, que a medida invariante para o processo markoviano
em questão será dada por: 
\[
\pi(0) = 1/4 \qquad \pi(1) = 1/2 \qquad \pi(2) = 1/4.
\]
\end{exmp}
Abaixo, estabelecemos um critério o qual auxilia no cálculo de medidas invariantes
para cadeias de Markov a tempo discreto.
\begin{definicao}
Dada uma cadeia de Markov $(X_n)_{n \in \mathbb{N}}$ com espaço de estados $\Omega$. 
Dizemos que uma medida de probabilidade $\pi$ sobre $\Omega$ é reversível se,
para todo elemento $a, b \in \Omega$, tivermos
\[
\pi(a) P[X_1 = b| X_0 = a] = \pi(b) P[X_1 = a| X_0 = b],
\]
como ilustrado na Figura~\ref{fig:rev}.
\begin{figure}[h]
\centering
\begin{tikzpicture}[->, >=stealth', auto, semithick, node distance=7cm]
\tikzstyle{every state}=[fill=white,draw=black,thick,text=black,scale=1]
\node[state]    (1)               {$a$};
\node[state]    (2)[right of=1]   {$b$};
\path
(1) edge[bend left]          node{ $\pi(a)$ P[$X_1$ = b| $X_0$ = a]}    (2)
(2) edge[bend left]          node{$\pi(b)$ P[$X_1$ = a| $X_0$ = b]}     (1);
\end{tikzpicture}
\caption{Exemplo de transição entre estados $a$ e $b$ tal que a probabilidade
de se iniciar no ponto $a$ e ir para o ponto $b$ é igual ao caminho reverso, i.e.,
a probabilidade de começar no estado $b$ e ir para o estado $a$.}
\label{fig:rev}
\end{figure}
\end{definicao}
Decorre de uma probabilidade $\pi$ ser reversível para a cadeia $(X_n)_n$ que a mesma, também, será uma medida 
invariante, pois, para todos estados $a,b \in \Omega$, temos
\[
\pi(a) P[X_1 = b| X_0 = a] = \pi(b) P[X_1 = a| X_0 = b]
\]
onde, tomando a somatória em ralação aos estados $b \in \Omega$ em ambos os lados
da igualdade, obtemos
\[
\pi(a) \sum_{b \in \Omega} P[X_1 = b| X_0 = a] = \sum_{b \in \Omega} \pi(b) P[X_1 = a| X_0 = b],
\]
e, como $\sum_{b \in \Omega} P[X_1 = b| X_0 = a] = 1$, temos a igualdade desejada. Portanto,
$\pi$ é uma medida invariante.
%e a probabilidade destes estados
%ocorrerem, no momento em que se inicia o estudo, é dada por:
%este centro considera que 

Uma abordagem natural a cadeias de Markov a tempo discreto é estender o tempo
em que as transições de estados ocorrem para algo contínuo. Sob esta perspectiva,
definimos os processos markovianos a tempo contínuo:
\begin{definicao}
Seja $\Omega$ o conjunto representando os espaços de estado e $(X_t, t \geq 0)$ uma sequência
de variáveis aleatórias, indexadas pelos valores reais e positivos $t \geq 0$, assumindo
valores em $\Omega$. Dizemos que $(X_t, t \geq 0)$ é uma cadeia de Markov a tempo 
contínuo se, para toda sequência de tempos positivos
$s_1 < s_2 < \ldots < s_n$, tivermos
\[
P[ X_{t + s_n} = a| X_{s_1} = a_1, \ldots, X_{s_{n-1}} = a_{n-1}, X_{s_n} = b] = P[ X_t = a_t| X_0 = b],
\]
para $a, b, a_1, a_2, \ldots, a_{n-1} \in \Omega$, e $t \geq 0$.
\end{definicao}

Decorre desta definição o seguinte exemplo.
\begin{exmp} \label{cont}
Consideremos o processo de Poisson de taxa $1$, dado por $N(t)$ com $t \geq 0$, e a cadeia
de Markov a tempo discreto $X_n$, dada no Exemplo~\ref{exmp:graph}. Logo, seja $t \geq 0$
um instante de tempo,
definimos o processo
\[
Y_t = X_{N(t)}.
\]
Tal processo será uma cadeia de Markov a tempo contínuo o qual espera tempos aleatórios de transição,
dados por exponenciais de taxa $1$, para mudar de estados. Além do mais, as transições são dadas por
processos de Markov a tempo discreto.
\end{exmp}

Dada uma cadeia de Markov a tempo contínuo $(X_t, t \geq 0)$ estaremos interessados
em estudar as seguintes taxas
\[
\lim_{s \rightarrow 0} \frac{ P[ X_{t+s} = j| X_t = i] }{s},
\]
para $i, j$ elementos do espaço de estados do processo. Caso o limite acima exista, 
denotaremos, então, este limite por taxa de transição do estado $i$ para o
estado $j$, denotado por $c(i, j)$, i.e.,
\[
c(i, j) = \lim_{s \rightarrow 0} \frac{ P[ X_{t+s} = j| X_t = i] }{s}.
\]
Em posse destas taxas de transição definimos a seguinte matriz 
\[
Q = (Q_{ij})_{i,j \in \Omega}, \text{ com } \quad
Q_{ij} = 
\begin{cases}
c(i, j) & \text{se } i \neq j;\\
-\sum_{l \neq i} c(i, l) & \text{c.c.},
\end{cases}
\]
com $\Omega$ o espaço de estados.
A matriz $Q$, definida acima,  será chamada de matriz geradora ou L-matriz. 
\begin{obs*}
A matriz geradora $Q$ satisfaz a condição de que todas linhas somam zero.
\end{obs*}

Em posse de uma L-matriz qualquer somos capazes de criar um processo de
Markov a tempo contínuo. Para a construção de processos markovianos a tempo
contínuo, a partir de uma matriz geradora, consulte
\cite{durret1}, capítulo 4.

Um resultado importante para cadeias de Markov a tempo contínuo são as equações
de Kolmogorov. Considerando o processo de Markov $(X_t, t \geq 0)$, com espaço
de estados $\Omega$, definimos para $i, j \in \Omega$ a função:
\[
p_t(i, j)  = P[X_t = j| X_0 = i].
\]
Por meio desta função $p_t(i,j)$ e da matriz geradora $Q$, associada ao processo markoviano,
valem as seguintes igualdades:
\begin{align*} 
p'(t) = Q p_t; \tag{Equação Regressiva de Kolmogorov} \\
p'(t) =  p_t Q. \tag{Equação Progressiva de Kolmogorov}
\end{align*}
Para as demonstrações destas igualdades veja \cite{durret1}.

%, ou seja,
%ao invés de considerarmos os naturais iremos considerar os reais positivos

%%% \endfold

%%%% TODO ESPAÇOS TOPOLOGICOS
%% \beginfold Cap Topologia Geral
%\chapter{Topologia Geral }
%Conceitos básicos de topologia geral serão descritos neste capítulo, com ênfase em
%conexidade. Para maiores informações sobre topologia geral, vide~\cite{elon2}.
%
%%Neste capítulo descreveremos brevemente conceitos gerais de topologia, com o enfoque maior em conexidade e grupos topológicos.
%%A importância de grupos topológicos será evidente quando estivermos estudando grupos de matrizes de Lie, que nada mais são do que subconjuntos 
%%fechados do grupo topológico das matrizes $nxn$ invertíveis, i.e., um subconjunto fechado do grupo linear geral, $GL(n,\mathbb{R})$. Quando o grupo topológico
%%for conexo, várias propriedades interessantes se destacarão neste grupo. Uma delas, por exemplo, é que a componente conexa da identidade será um
%%subgrupo normal e fechado.
%
%\section{Espa\c{c}os Topol\'ogicos}
%
%Dado um conjunto qualquer $X$ e uma família $\mathscr{F}$ de subconjuntos de $X$ 
%(i.e., $\mathscr{F} \subseteq \mathscr{P}(X)$, com $\mathscr{P}(X) = \{A; A \subseteq X\}$),
%dizemos que a tupla $(X, \mathscr{F})$ é um espaço topológico se as seguintes condições forem satisfeitas:
%\begin{enumerate}[(i)]
%	\item $\emptyset, X \in \mathscr{F}$;
%	\item Seja $\mathscr{U}$ uma família de elementos de $\mathscr{F}$, i.e., $\mathscr{U} \subseteq \mathscr{F}$, então 
%		\[\bigcup_{X \in \mathscr{U}} X \in \mathscr{F};\]
%	\item Dados dois elementos $A,B \in \mathscr{F}$, então $A \cap B \in \mathscr{F}$.
%\end{enumerate}
%\index{espaço!topológico}
%
%Seja $(X, \mathscr{F})$ um espaço topológico, dizemos que a família $\mathscr{F}$ de subconjuntos de $X$ é uma topologia sobre $X$ e, 
%salvo menção explícita, iremos nos referir ao espaço topológico $(X, \mathscr{F})$ como apenas o espaço topológico $X$. Além do mais,
%dado $Y$ um subconjunto qualquer de $X$, temos que, se considerarmos a coleção de conjuntos
%\[ \mathscr{A} = \{ Y \cap Z; Z \in \mathscr{F} \}, \]
%podemos ver a tupla $(Y, \mathscr{A})$  como um espaço topológico, e dizemos que $\mathscr{A}$ é a topologia induzida 
%por $X$.
%
%
%%Seja $(X, \mathscr{F})$ um espaço topológico e $Y$ um subconjunto qualquer de $X$. Temos que, se considerarmos a coleção de conjuntos
%%\[ \mathscr{A} = \{ Y \cap Z; Z \in \mathscr{F} \}, \]
%%podemos ver a tupla $(Y, \mathscr{A})$  como um espaço topológico, e dizemos que $\mathscr{A}$ é a topologia induzida 
%%por $X$.
%
%%%%Da definição de espaços topológico, decorrem dois exemplos triviais, são eles: o espaço topológico discreto $(X, { \emptyset, X})$ e
%%%%o espaço topológico caótico $(X, \mathscr{P}(X))$.
%
%Para finalizar, os elementos da topologia $\mathscr{F}$ de um espaço topológico X são chamados de abertos 
%de $X$ e, todo subconjunto de $X$,
%tal que o complementar seja um elemento de $\mathscr{F}$, são chamados de subconjuntos fechados de $X$.
%\index{conjunto!aberto}
%\index{conjunto!fechado}
%
%\section{Espa\c{c}os M\'etricos}
%
%Seja $M$ um conjunto não vazio qualquer e $d: M \times M \rightarrow \mathbb{R}$ uma função satisfazendo:
%\begin{enumerate}[P1)]
%	\item $d(a, a) = 0$, para todo $a \in M$;
%	\item $d(a, b) = d(b, a)$, para todo $a, b \in M$;
%	\item $d(a, b) > 0$, se $a, b \in M$ e $ a \neq b$;
%	\item $d(a, b) \leq d(a, c) + d(c, a)$, para $a, b, c \in M$ quaisquer.
%\end{enumerate}
%Dizemos que o conjunto $M$ é um espaço métrico sempre que existir uma função $d: M \times M \rightarrow \mathbb{R}$,
%chamada de métrica, satisfazendo as quatro propriedades acima.
%\index{espaço!métrico}
%
%\begin{exmp} \label{exmp_matrizes}
%O conjunto $\mathcal{M}_{n}(\mathbb{R})$ das matrizes $n \times n$ com entradas nos reais é um espaço métrico. Para isto, consideremos a
%métrica
%\begin{align*}
% d:  \mathcal{M}_n(\mathbb{R}) \times \mathcal{M}_n(\mathbb{R})  & \rightarrow  \; \mathbb{R} \\
%      (A, B)	 & \mapsto  \; \sup\left\{\| (A - B) \cdot x \|_2; \; x \in \mathbb{R}^n \text{ e } \|x\|_2 = 1 \right\},
%\end{align*}
%com $\|\|_2$ a norma euclideana em $\mathbb{R}^n$. A função acima é uma métrica, pois,
%para matrizes quaisquer $A, B, C \in \mathcal{M}_n(\mathbb{R})$, temos:
%\begin{enumerate}[(i)]
%	\item $d(A, A) = \sup_{ \|x\|_2 = 1} \{ \| (A - A) \cdot x \|_2 \} = 0$;
%	\item $d(A, B) = d(B, A)$, imediato;
%	\item Se $A \neq B$, então existe um par ordenado
%        $(i, j) \in \{1, \ldots, n\} \times \{1, \ldots, n\}$ tal que $a_{ij} - b_{ij} \neq 0$. Logo,
%		tomando o vetor $x_j$, cuja j-ésima coordenada vale $1$ e $0$  nas demais coordenadas, obtemos
%		\[ \| (A - B) \cdot x_j \|_2 \; \geq \; | a_{ij} - b_{ij} | > 0. \]
%		Portanto, $d(A, B) > 0$.
%	\item Para todo $x \in \mathbb{R}^n$, com $\|x\|_2 = 1$, vale a seguinte desigualdade
%		\begin{align*}
%			\| (A - B) \cdot x \|_2 & = \| A\cdot x - B\cdot x + C\cdot x - C\cdot x  \|_2 \\
%					      & \leq \| A\cdot x - C\cdot x  \|_2 + \| B\cdot x  - C\cdot x  \|_2 \\
%					      & \leq d(A, C) + d(C, B).
%		\end{align*}
%		Portanto, $ d(A, B) \leq d(A, C) + d(C, B)$.
%\end{enumerate}
%\end{exmp}
%
%\begin{obs*}
%Note que, o espaço das matrizes $n \times n$ com entradas nos números reais pode ser visto como
%um espaço vetorial normado. Para isso, tome a seguinte norma
%\begin{align*}
% \|\|: \mathcal{M}_n(\mathbb{R})  \rightarrow & \; \mathbb{R} \\
%                       A              \mapsto & \; \sup\left\{\| A \cdot x \|_2; \; x \in \mathbb{R}^n \text{ e } \|x\|_2 = 1 \right\}.
%\end{align*}
%Note também que, a métrica usada em $\mathcal{M}_n(\mathbb{R})$ nada mais é que a métrica induzida pela norma acima.
%\end{obs*}
%\index{norma de matrizes}
%
%Da norma de matrizes, temos a importante desigualdade
%\begin{prop}
%Dadas as matrizes $A, B \in \mathcal{M}_n(\mathbb{R})$, temos que
%\[ \|A \cdot B\| \leq \|A\| \cdot \|B\|.\]
%\end{prop}
%\begin{proof}
%Dado $x \in \mathbb{R}^n$ um vetor com norma igual a $1$, vale 
%\begin{align*}
% \|(A \cdot B) \cdot x\|_2 & =  \left\|A \cdot \frac{B \cdot x}{\| B \cdot x \|_2} \cdot \| B \cdot x \|_2 \right\|_2\\
%			 & =  \left\|A \cdot \frac{B \cdot x}{\| B \cdot x \|_2} \right\|_2 \cdot \| B \cdot x \|_2.
%\end{align*}
%Como o vetor $\frac{B \cdot x}{\| B \cdot x \|_2}$ tem norma $1$, obtemos
%\[  \|(A \cdot B) \cdot x\|_2 \leq \|A\| \cdot \|B\|. \]
%Portanto, tomando o $\sup$ do lado esquerdo da desigualdade acima, para todo vetor $x$ de norma 1, conseguimos a desigualdade desejada.
%\end{proof}
%
%\begin{definicao}
%Seja $M$ um espaço métrico e $d: M \times M \rightarrow \mathbb{R}$ sua métrica. Para todo ponto $x \in M$ e, para todo real positivo 
%$\delta > 0$, definimos a bola aberta de centro $x$ e raio $\delta$ como sendo o conjunto
%\[ B(x, \delta) = \left\{ a \in M; \, d(a, x) < \delta \right\}. \]
%\end{definicao}
%\index{bola aberta}
%Com esta definição, podemos ver todo espaço métrico $M$ como um espaço topológico, dado pela seguinte topologia:
%\[ \mathscr{F} = \{ A \subseteq M; \text{  para todo } x \in A \; \exists \delta > 0 \text{ tal que } B(x, \delta) \subseteq A \}. \]
%
%Note que, todo espaço vetorial $E$ munido de uma norma $\|\|$ pode ser visto como um espaço métrico. Para isto,
%basta considerarmos a 
%métrica $ (x, y) \mapsto \| x - y \|$, para todo $x, y \in E$. 
%Sendo assim, todo espaço vetorial normado é um espaço métrico e, consequentemente,
%é um espaço topológico.
%
%\section{Continuidade de Fun\c{c}\~oes}
%
%Sejam $(X, \mathscr{F}_1)$ e $(Y, \mathscr{F}_2)$ espaços topológicos e $a \in X$ um ponto qualquer. Dizemos que uma função 
%$f: (X, \mathscr{F}_1) \rightarrow  (Y, \mathscr{F}_2)$ é contínua no ponto $a$ se,
%para todo aberto $\mathscr{U} \in \mathscr{F}_2$ contendo o
%ponto $f(a)$, existir um aberto $\mathscr{U}' \in \mathscr{F}_1$ contendo o ponto $a$ tal que
%\[ f(\mathscr{U}') \subseteq \mathscr{U}. \]
%Sobre as mesmas condições, dizemos que $f$ é uma função contínua se a mesma for contínua em todos os seus pontos.
%\index{continuidade de funções}
%
%Uma propriedade importante de funções contínuas é a seguinte:
%\begin{prop}
%Sejam $X$ e $Y$ espaços topológicos e $f: X \rightarrow Y$ uma função contínua. Então, para todo aberto $\mathscr{U}$ de $Y$ temos que
%$f^{-1}(\mathscr{U})$ será um aberto em $X$. (Observe também que a imagem inversa de um conjunto fechado 
%por uma função contínua também será um conjunto fechado no domínio de f.)
%\end{prop}
%
%Para encerrar esta seção, apresentamos a seguinte definição:
%\begin{definicao}
%Sejam $X$ e $Y$ espaços topológicos e $f: X \rightarrow Y$ uma função bijetora.
%Se $f$
%e $f^{-1}$ forem funções contínuas,
%dizemos que $f$ é um homeomorfismo entre $X$ e $Y$ 
%\end{definicao}
%
%\section{Conexidade}
%
%Um espaço topológico $(X, \mathscr{F})$ é dito ser conexo se o conjunto $X$ não pode ser escrito como uma
%união de abertos não nulos dois a dois disjuntos, i.e.,
%não existem abertos $A,B \in \mathscr{F} \backslash \{\emptyset\}$ tal que $X = A \cup B$. 
%%%%Caso contrário, se existirem abertos $A, B$ disjuntos e não nulos tal
%%%%que $X = A \cup B$, 
%Dado um espaço topológico $X$ e um subconjunto $Y \subset X$. 
%Dizemos que $Y$ é conexo se $Y$, munido com a topologia induzida por $X$, é um espaço conexo. Para espaços conexos, seguem os 
%seguintes resultados importantes
%\index{espaço!conexo}
%
%\begin{teo}
%Seja $X$ um espaço topológico e $Y \subseteq X$ um subconjunto. Então, se $Y$ for um espaço conexo, 
%teremos que o fecho de $Y$ também será um espaço conexo.
%\end{teo}
%\begin{proof}
%Vide~\cite{elon2}.
%\end{proof}
%
%\begin{teo}
%Dada uma função contínua $f: (X, \mathscr{F}_1) \rightarrow (Y, \mathscr{F}_2)$ entre espaços topológicos. 
%Se $X$ for um espaço conexo, então a imagem $f(X)$ também será
%um espaço conexo.
%\end{teo}
%
%\begin{proof}
%Suponhamos, por absurdo, que $f(X)$ não seja conexo. Sabemos que a função
%\begin{align*}
%\widetilde{f}: (X, \mathscr{F}_1)  & \rightarrow (f(X), \widetilde{\mathscr{F}}_2) \\
%                          x    & \mapsto f(x),   
%\end{align*}
%com $\widetilde{\mathscr{F}}_2$ a topologia induzida por $Y$, é uma uma função contínua. Como $f(X)$ não é conexo, existem abertos
%$A,B \in \widetilde{\mathscr{F}}_2 \backslash \{\emptyset\}$ tal que $f(X) = A \cup B$. Porém,
%\[ X \subseteq \widetilde{f}^{-1} ( f(X) ) = \widetilde{f}^{-1} ( A \cup B ) = \widetilde{f}^{-1} ( A )
%\cup \widetilde{f}^{-1} ( B ) \subseteq X. \]
%Como $\widetilde{f}$ é contínua, temos que $\widetilde{f}^{-1} ( A )$ e $\widetilde{f}^{-1} ( B )$
%são abertos em $X$ e, decorre de $A$ e $B$ serem 
%não nulos e dois a dois disjuntos, que $\widetilde{f}^{-1} ( A )$ e $\widetilde{f}^{-1} ( B )$ também 
%serão não nulos e dois a dois disjuntos. Porém,
%isso contradiz a hipótese de que $X$ é conexo e, portanto, a imagem $f(X)$ será um espaço conexo.
%\end{proof}
%
%\begin{teo} \label{uniao_conexos} 
%Dado o espaço topológico $(X, \mathscr{F})$ e uma família não nula de subconjuntos conexos de $X$, denotada por
%$\mathscr{A} $, com $\cap_{U \in  \mathscr{A}} U  \neq \emptyset$.
%Então, $\cup_{U \in  \mathscr{A}} U $ é conexo.
%\end{teo}
%
%\begin{proof}
%Suponhamos, por absurdo, que $\cup_{U \in  \mathscr{A}} U $ não é conexo. 
%Logo, existem dois abertos $A$ e $B$ não nulos em $\cup_{U \in  \mathscr{A}} U $ tal que
%\[ \bigcup_{U \in  \mathscr{A}} U = A \cup B \;\; \text{e} \;\; A \cap B = \emptyset.\]
%Decorre de $\cap_{U \in  \mathscr{A}} U \neq \emptyset$ que existe um ponto $w \in X$ tal que, 
%para todo $U \in \mathscr{A}$, tem-se $w \in U$. Supondo, sem perda de 
%generalidade, que $w \in A$, temos $U \cap A \neq \emptyset$, $\forall U \in \mathscr{A}$. 
%Dado $\widetilde{w} \in B$ um ponto qualquer, existe 
%$\widetilde{U} \in \mathscr{A}$ satisfazendo $\tilde{w} \in \widetilde{U}$. Portanto,
%\[ \widetilde{U} \cap B \neq \emptyset \qquad \text{e} \qquad \widetilde{U} \cap A \neq \emptyset. \]
%Logo, $\widetilde{U} = (\widetilde{U} \cap A) \cup (\widetilde{U} \cap B)$. 
%Como $A$ e $B$ são abertos em $\cup_{U \in  \mathscr{A}} U$ temos que 
%$(\widetilde{U} \cap A)$ e $(\widetilde{U} \cap B)$ são abertos em $\widetilde{U}$.
%Ou seja, $\widetilde{U}$ pode ser escrito como uma união de abertos não nulos dois a dois disjuntos.
%Porém, isto contradiz a hipótese de $\widetilde{U}$ ser conexo. Logo, 
%$\cup_{U \in  \mathscr{A}} U$ é um espaço
%conexo.
%%%$\widetilde{A}$, $\widetilde{B}$ em $X$ tal que 
%%%\[ A = \bigcup_{U \in  \mathscr{A}} U \cap \widetilde{A} \qquad \text{e} \qquad B =
%%%  \bigcup_{U \in  \mathscr{A}} U \cap  \widetilde{B}, \]
%%%ou seja,
%%%\begin{align*}
%%%	\widetilde{U} & = \left(\widetilde{U} \cap \bigcup_{U \in  \mathscr{A}} U \cap \widetilde{A} \right) 
%%%    \cup \left(\widetilde{U} \cap \bigcup_{U \in  \mathscr{A}} U \cap \widetilde{B} \right)\\
%%%		      & = \left(\bigcup_{U \in  \mathscr{A}} U \right)  \cap \left[ \left(\widetilde{U} \cap 
%%%              \widetilde{A} \right) \cup \left(\widetilde{U} \cap \widetilde{B} \right) \right]\\
%%%		      & = \left(\widetilde{U} \cap \widetilde{A} \right) \cup \left(\widetilde{U} \cap \widetilde{B} \right).
%%%\end{align*}
%%%Observe que, $\widetilde{U} \cap \widetilde{A}$ e $\widetilde{U} \cap \widetilde{B}$ são abertos não nulos e disjuntos em $\widetilde{U}$. 
%%%Porém, isto contradiz a hipótese de $\widetilde{U}$ ser conexo, logo, por contradição,
%%%concluímos que $\cup_{U \in  \mathscr{A}} U$ é um espaço
%%%conexo.
%\end{proof}
%
%
%Dado um espaço topológico $X$ e um ponto $a \in X$ qualquer, definimos a componente conexa, $C_a$, do ponto $a$, como sendo o conjunto:
%\[ C_a = \bigcup \{ Z \subset X; \; Z \text{ é um conexo contendo o ponto a} \}. \]
%Como visto pelo Teorema~\ref{uniao_conexos}, a componente conexa do ponto $a$ é um espaço conexo. 
%Ainda mais, é o maior conexo contido em $X$ que contém o 
%ponto $a$. \index{componente conexa}
%
%\begin{teo} \label{homeo_comp}
%Dado uma função $f: (X, \mathcal{F}_1) \rightarrow (Y, \mathcal{F}_2)$ entre espaços topológicos e $C_a$ 
%a componente conexa do ponto $a \in X$.
%Se a função $f$ for um homeomorfismo, teremos que a imagem $f(C_a)$ será a componente conexa do ponto $f(a)$ em $Y$.
%\end{teo}
%
%\begin{proof}
%Observe que o conjunto $f(C_a)$ será um espaço conexo, pois é a imagem de uma função contínua e $C_a$ é conexo. 
%Logo, $f(C_a)$ é um conexo e contém o ponto
%$f(a)$, ou seja,
%\[ f(C_a) \subseteq C_{f(a)}.\]
%Dado $w \in C_{f(a)}$, um ponto qualquer. Por $C_{f(a)}$ ser a componente conexa de $f(a)$, existe 
%um conexo $H$ tal que $f(a)$ e $w$ estão em $H$. Como
%$f$ é um homeomorfismo, $f^{-1}$ será contínua e, portanto, $f^{-1}(H)$ será um conexo contendo o ponto $a$. Logo,
%\[ f^{-1}(H) \subseteq C_a \implies H \subseteq f(C_a), \]
%ou seja, $w \in f(C_a)$. A partir daí, concluímos que $C_{f(a)} \subseteq f(C_a)$. Assim sendo, $f(C_a) = C_{f(a)}$.
%\end{proof}
%
%Outra importante noção sobre conexidade é a seguinte: um espaço topológico $X$ 
%é conexo por caminhos se, para quaisquer dois pontos $a,b \in X$, existir uma função 
%contínua $f: [0,1] \rightarrow X$ tal que $f(0) = a$ e $f(1) = b$. \index{espaço!conexo por caminhos}
%
%\begin{teo}
%Dado um espaço topológico $X$. Se $X$ for um espaço conexo por caminhos então $X$ também será um espaço conexo.
%\end{teo}
%
%\begin{proof}
%Primeiramente, note que o conjunto dos números reais $\mathbb{R}$ com a topologia usual satisfaz a 
%seguinte propriedade: $I \subseteq \mathbb{R}$ é 
%um subconjunto conexo se, e somente se, $I$ é um intervalo. A demonstração deste fato pode ser vista em \cite{elon}. 
%Sendo assim, fixado um ponto $a \in X$  temos
%que, para $b \in X$ qualquer, existe uma função $f_b: [0,1] \rightarrow X$ contínua, com $f(0) = a$ e $f(1) = b$. 
%Como o intervalo $[0,1]$ é conexo, temos que  imagem
%$f_b( [0,1] ) $ também será um conexo. Logo,
%\[ X = \bigcup_{b \in X} f_b([0,1]), \]
%e, por $a \in f_b([0,1])$, para todo $b \in X$, temos pelo Teorema~\ref{uniao_conexos} que a união será um 
%conexo, ou seja, $X$ é um espaço conexo.
%\end{proof}
%
%O próximo teorema estabelece condições para que um espaço conexo seja conexo por caminhos:
%\begin{teo}
%Seja $U$ um aberto, não nulo, de um espaço vetorial normado $E$ de dimensão finita. 
%Então, $U$ é conexo se, e somente se, $U$ é conexo por caminhos.
%\end{teo}
%
%\begin{proof}
%Iremos demonstrar apenas a ida, pois a volta foi demonstrada no Teorema anterior. 
%Suponhamos, por absurdo, que $U$ não seja conexo por caminhos e definamos
%a seguinte relação em $U$:
%\[ \text{dados } x, y \in U \text{ temos } xRy \iff \exists g:[0,1] \rightarrow U \text{ contínua com } g(0) = x \text{ e } g(1) = y. \]
%Esta relação satisfaz as seguintes propriedades
%\begin{enumerate}[i)]
%	\item Para todo $x \in U$ vale $xRx$, pois a função $g:[0,1] \rightarrow U$ dada por $t \mapsto x$ é sempre contínua;
%	\item $xRy \iff yRx$. Pois, se $g:[0,1] \rightarrow U$, com $g(0) = x$ e $g(1) = y$, for uma função contínua, teremos que a função
%	      $h:[0,1] \rightarrow U$ dada por $h(t) = g(1 - t)$, para $t \in [0,1]$, será contínua, com $h(0) = y$ e $h(1) = x$;
%	\item Dados $x, y, z \in U$ pontos quaisquer. Se tivermos $xRy$ e $yRz$ então $xRz$. Isto é verdade, pois dadas as funções contínuas
%	      $g: [0,1] \rightarrow U$ e $h: [0,1] \rightarrow U$ satisfazendo
%		\[
%		    \begin{cases}
%		     g(0)  = x\\
%		     g(1)  = y 
%		   \end{cases}
%			\qquad 
%			\text{ e }
%			\qquad
%		    \begin{cases}
%		     h(0)  = y\\
%		     h(1)  = z 
%		   \end{cases}
%		   ,
%		\]
%		temos que a função $f: [0,1] \rightarrow U$ dada por
%		\[
%		   f(t) =
%		    \begin{cases}
%		     g(2t), \text{ se } t \in [0, 1/2];\\
%		     h(2t -1), \text{ se } t \in [1/2, 1];
%		   \end{cases}
%		\]		
%		é uma função contínua com $f(0) = x$ e $f(1) = z$.
%\end{enumerate}
%Desta forma, a relação $R$ é uma relação de equivalência e podemos decompor o espaço $U$ como a união 
%\[ U = \bigcup_{x \in U}R_x; \;\; R_x = \{y \in U; \; xRy\},\]
%tal que, para todo $x, y \in U$, temos: $R_x = R_y$ ou $R_x \cap R_y = \emptyset$.
%
%Mostremos que as classes $R_x$, para todo $x \in U$, são conjuntos abertos em $U$. 
%Fixado $x \in U$, temos, para todo $w \in R_x$, que existe $r > 0$
%tal que $B(w, r) \subseteq U$. Como estamos trabalhando em um $\mathbb{R}$-espaço vetorial de dimensão finita,
%temos que para todo $z \in B(w, r)$, a seguinte
%função é contínua
%\begin{align*}
%	g: [0,1] & \rightarrow U\\
%	      t  & \mapsto w + t (z - w) 
%\end{align*}
%e, $g([0,1]) \subseteq B(w, r)$. Desta forma, obtemos que $xRw$ e $wRz$ e, portanto, pela transitividade conseguimos $xRz$. 
%Logo, $B(w, r) \subseteq R_x$, ou seja,
%$R_x$ é um conjunto aberto. Como estamos supondo que $U$ não é conexo por caminhos, existem os pontos $a, b \in U$ tal 
%que não temos $aRb$. Escrevemos,
%então, $U$ da seguinte forma
%\[ U = \left( \bigcup \{R_x; \text{ não temos } xRb\} \right) \cup R_b. \]
%Dessa maneira, $U$ pode ser escrito como a união de abertos não vazios e disjuntos. 
%Todavia, $U$ é conexo por hipótese. Logo, por contradição, concluímos que $U$ deve 
%ser conexo por caminhos
%\end{proof}
%
%
%% \endfold Cap Topologia Geral
%

%%% TODO Grupos
% \beginfold
\chapter{Grupos Cl\'assicos}
Neste capítulo formularemos o conceito algébrico de grupos e algumas de suas propriedades.
Em seguida, as estruturas de grupos topológicos e  de grupo de matrizes de Lie serão discutidos.
A importância de se estudar os grupos topológicos manifesta-se pelo fato do grupo linear geral
ter posse dessa estrutura e, sendo assim, todo grupo de matrizes de Lie também será um grupo topológico.
O principal resultado deste capítulo, dado pelo Teorema~\ref{propriedades_G0}, nos fornecerá informação
sobre a componente conexa do elemento identidade de um grupo topológico. Graças a este resultado,
sabemos que a componente conexa da identidade será um subgrupo normal, fechado e conexo do grupo em
questão. As demonstrações dos Teoremas, assim como os conceitos propostos neste capítulo,
são baseados nas seguintes referências: \cite{stillwell} (para o estudo de grupos de matrizes de Lie), 
\cite{kelley} (para o tratamento de grupos topológicos) e \cite{agozzine} (capítulo 1, noções básicas
de grupos).


%%% TODO Conceitos Gerais
% \beginfold
\section{Conceitos Gerais} \label{sec:grupos}
Seja $G$ um conjunto não nulo munido de um operador $\cdot: G \times G \rightarrow G$. Dizemos que a tupla $(G, \cdot)$
é um grupo se as seguintes condições forem satisfeitas:
\begin{enumerate}[(i)]
   \item $a \cdot (b \cdot c) = (a \cdot b) \cdot c$ para todo $a, b, c \in G$ (\textit{associatividade});
   \item Existe um elemento $\mathbf{e} \in G$ tal que, para todo $a \in G$, vale $a \cdot \mathbf{e} = \mathbf{e} \cdot a = a$
   (\textit{existência de identidade});
   \item Para todo $a \in G$ existe um elemento $a^{-1} \in G$ tal que $a \cdot a^{-1} = a^{-1} \cdot a = e$
   (\textit{existência do elemento inverso}).
\end{enumerate}
Daqui em diante, sempre quando não houver risco de confusão, denotaremos o grupo $(G, \cdot)$ por apenas o grupo $G$.
\index{grupo}

%\begin{exmp}
%Um exemplo natural de grupo é o conjunto dos inteiros $\mathbb{Z}$ munido com a operação usual de soma. Neste
%exemplo, a identidade é dada pelo elemento $0$.
%\end{exmp}

\begin{exmp}
\textit{(O Grupo Linear Geral)} Um exemplo importante, o qual iremos trabalhar bastante no decorrer deste texto, é o do
grupo linear geral, denotado por $GL(n, \mathbb{R})$. Este grupo é definido pelo conjunto
\[ GL(n, \mathbb{R}) = \{ A = (a_{ij})_{n \times n}; \; a_{ij} \in \mathbb{R} \text{ e } \det(A) \neq 0 \}\]
munido da multiplicação usual entre matrizes. 
\end{exmp}
\index{grupo!linear geral}

\begin{definicao}
Dado o grupo $G$ e um subconjunto não nulo $H \subseteq G$. Dizemos que $H$ é um subgrupo de $G$, denotado por
$H \leq G$, se:
\begin{enumerate}[(i)]
    \item O elemento identidade do grupo $G$ estiver contido em $H$;
    \item Para todo $a \in H$ tivermos $a^{-1} \in H$;
    \item Dados dois elementos quaisquer de $H$, digamos $a, b \in H$, tivermos $ a \cdot b \in H$.
\end{enumerate}
\end{definicao}
\index{subgrupo}

Abaixo, alguns exemplos de subgrupos:
\begin{exmp}
Seja $G$ um grupo, dois exemplos imediatos decorrem da definição de subgrupo, são eles: $\{e\}$ e o próprio
grupo $G$. Estes subgrupos são chamados de subgrupos triviais.
\end{exmp}
\index{subgrupo!trivial}

\begin{exmp}
Seja o grupo $(\mathbb{Z}, +)$ dos inteiros munido da soma usual. O conjunto 
\[ H = \{ \dots, -6, -4, -2, 0, 2, 4, 6, \dots \} \]
munido da soma usual dos inteiros é um subgrupo de $\mathbb{Z}$.
\end{exmp}

\begin{exmp}
Seja $G$ um grupo qualquer. Denotamos por centro do grupo $G$ o conjunto
\[
Z(G) = \{ x \in G; \; \forall y \in G, xy = xy \}.
\]
Decorre desta definição que $Z(G)$ é um subgrupo de $G$. Para que $Z(G)$ seja um subgrupo de $G$
devemos checar apenas a propriedade (ii), pois as propriedades (i) e (iii) são
imediatas. Portanto, dados os elementos $x \in Z(G)$ e $y \in G$ quaisquer, temos:
\[
(xy)(xy)^{-1} = \bm{e} \implies (yx)(y^{-1}x^{-1}) = \bm{e} \implies xy^{-1} = y^{-1}x.
\]
Logo, $Z(G)$ é um subgrupo de $G$.
\end{exmp}
%\begin{exmp}\label{subgrupoH}
%Dado o grupo linear geral $GL(n, \mathbb{R})$, temos que o conjunto
%\[ H = \{ A \in GL(n, \mathbb{R}); \; AA^t = A^tA = I_n  \} \]
%munido da multiplicação usual entre matrizes é um subgrupo de $GL(n, \mathbb{R})$ (aqui, $A^t$ denota
%a transposta da matriz $A$ e $I_n$ denota a matriz identidade). Para que $H$ seja um subgrupo do
%grupo linear geral devemos checar apenas a propriedade (iii), pois as propriedades (i) e (ii) são
%imediatas. Portanto, dadas as matrizes $A, B \in H$, temos:
%\[ (AB)(AB)^t = ABB^tA^t = I_n = B^tA^tAB = (AB)^t(AB). \]
%Logo, $H$ é um subgrupo de $GL(n, \mathbb{R})$.
%\end{exmp}

Dado um subgrupo $H \leq G$ de um grupo $G$ qualquer e os elementos $x, y \in G$. Denotaremos
por $xH$, $Hy$ e $xHy$ como sendo os seguintes conjuntos:
\begin{align*}
xH := & \{ x \cdot a; \; a \in H\}; \\
Hy := & \{ a \cdot y; \; a \in H\}; \\
xHy := & \{ x \cdot a \cdot y; \; a \in H\}.
\end{align*}
Em função destas operações, temos a seguinte definição:
\begin{definicao}
Seja $G$ um grupo e $H$ um subgrupo de $G$. Dizemos que $H$ é um subgrupo normal de $G$, denotado
por $H \triangleleft G$, se, para todo $x \in G$, tivermos a inclusão
\[ xHx^{-1} \subseteq H. \]
Observe que a inclusão acima é equivalente a dizer que $ xHx^{-1} = H. $
\end{definicao}
\index{subgrupo!normal}

\begin{exmp}
Os subgrupos triviais de um grupo qualquer são exemplos imediatos de subgrupos normais.
\end{exmp}

\begin{exmp}
Dado um grupo $G$ qualquer, o centro $Z(G)$ é um subgrupo normal de $G$, pois, dados $x \in G$
e $y \in Z(G)$ elementos quaisquer, temos
\[
xyx^{-1} = xx^{-1}y \in Z(G)
\]
e, portanto, $xZ(G)x^{-1} \subseteq Z(G)$.
\end{exmp}
%\begin{exmp} \label{exmp:det1}
%Dado o grupo linear geral $GL(n, \mathbb{R})$. O subgrupo 
%\[ H := \{ A \in GL(n, \mathbb{R}); \; \det(A) = 1\} \]
%é um subgrupo normal. Isto é verdade pois, dado $X \in GL(n, \mathbb{R})$ e $Y \in H$
%matrizes quaisquer, temos que
%\[\det(XYX^{-1}) = \det(X) \det(Y) \det(X)^{-1} = 1. \]
%Ou seja, $XHX^{-1} \subseteq H$ e, portanto, $H \triangleleft G$.
%\end{exmp}

\begin{definicao}
Seja $G$ um grupo qualquer e $H \leq G$ um subgrupo de $G$. Definimos, então, por o conjunto das coclasses de
$H$ em $G$ como sendo o conjunto
\[
G/H = \{ Hx; \; x \in G\}.
\]
\end{definicao}

Ao estudar as coclasses, ou classes laterais, dadas pelo subgrupo $H \leq G$, obtemos a seguinte decomposição
do grupo $G$:
\[
G = \bigcup_{x \in G} Hx,
\]
que nada mais é que a união de todos elementos do conjunto $G/H$. Além do mais, se $H$ for um subgrupo normal 
de $G$ teremos que $G/H$ terá estrutura de grupo se considerarmos o seguinte produto 
\begin{align*}
\odot: G/H \times G/H &\rightarrow G/H\\
(Hx, Hy) & \mapsto H(xy).
\end{align*}

Para encerrar esta seção, enunciaremos o significado de uma função entre grupos ser um homomorfismo e,
decorrente desta definição, enunciaremos o Teorema do Isomorfismo para grupos.
\begin{definicao}
Seja $f: (G, \cdot) \rightarrow (G', \odot)$ uma função entre grupos. Então, dizemos que $f$ é um homomorfismo de grupos
se, para todo $x, y \in G$, tivermos
\[
f(x \cdot y) = f(x) \odot f(y).
\]
Além do mais, se $f$ for bijetora, dizemos que $f$ é um isomorfismo de grupos,  denotados por $G \cong G'$. 
\end{definicao}

Decorre desta definição que o núcleo de todo homomorfismo $f: G \rightarrow G'$ entre grupos, denotado 
por $\ker(f)$, é um subgrupo normal de $G$. Em posse destas definições, e do fato do núcleo
de um homomorfismo ser um subgrupo normal, enunciamos o seguinte Teorema:
\begin{teo} \label{teo:isomorfismo}
Dado um homomorfismo entre grupos $f: G \rightarrow G'$. Então, o seguinte diagrama 
\begin{displaymath}
\begin{tikzcd}
    G \arrow{d }{\pi} \arrow{r}{f} & \im(f) \\
    G/\ker(f) \arrow{ur}{\widetilde{f}} \\
\end{tikzcd}
\end{displaymath}
comuta, onde $\pi(x) = \ker(f)x$ e $\widetilde{f}(\ker(f)x) = f(x)$, para todo $x \in G$. Caso $f$ seja um isomorfismo teremos,
então, que $G' \cong G/\ker{f}$.
\end{teo}
\begin{proof} Vide \cite{agozzine}. \end{proof}
%Decorrente desta estrutura algébrica que o conjunto das coclasses $G/H$ herda quando $H$ é um subgrupo normal,
%enunciamos o seguinte resultado:
% \endfold

%%% Grupos Topológicos
% \beginfold
\section{Grupos topol\'ogicos}

%%\subsection{Defini\c{c}\~ao e exemplos}
Seja $G$ um conjunto não nulo e $\mathscr{F}$ uma topologia de $G$. 
Dizemos que a terna $(G, \mathscr{F}, \cdot)$ é um grupo topológico se:
\begin{enumerate}[(i)]
	\item As tuplas $(G, \mathscr{F})$ e $(G, \cdot)$ forem um espaço topológico\footnote{Vide \cite{elon} ou
\cite{elon2} para o estudo mais aprofundado de temas relacionados a topologia ou espaços métricos.} e um grupo,
 respectivamente;
%	\item a tupla  for um grupo;
	\item As funções $m: G \times G \mapsto G$ e $i: G \mapsto G$ dadas por $m(x,y) = x \cdot y$ e $i(x) = x^{-1}$ 
    forem funções contínuas.   
    \index{grupo!topológico}
\end{enumerate}
Observe que, fixado um ponto $a \in G$ qualquer, as funções de multiplicação à esquerda e à direita por $a$, 
denotadas por $L_a$ e $R_a$, respectivamente,

\begin{minipage}{0.5\linewidth}
\centering
\begin{align*}
L_a:G  & \rightarrow G \\
  x    & \mapsto a \cdot x   \\
\end{align*}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\centering
\begin{align*}
R_a:G  & \rightarrow G \\
  x    & \mapsto x \cdot a  
\end{align*}

\end{minipage}
são homeomorfismo de espaços topológicos.

\begin{exmp}
O grupo linear geral é um grupo topológico. 
Como já sabemos, vide \cite{elon}, o conjunto das matrizes $\mathcal{M}_n(\mathbb{R})$ tem estrutura de 
espaço topológico e, portanto,
o grupo linear geral é um espaço topológico com topologia induzida por $\mathcal{M}_n(\mathbb{R})$. Finalmente,
como as funções\\
\begin{minipage}{.5\textwidth}
\centering
\begin{align*}
m: GL(n, \mathbb{R}) \times GL(n, \mathbb{R}) &\rightarrow GL(n, \mathbb{R}) \qquad \qquad \text{e} \\
 (A, B) & \mapsto A \cdot B
\end{align*}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
\begin{align*}
i: GL(n, \mathbb{R}) &\rightarrow GL(n, \mathbb{R})\\
 A & \mapsto A^{-1}
\end{align*}

\end{minipage}
são contínuas, obtemos que $GL(n, \mathbb{R})$ é, de fato, um grupo topológico.
\end{exmp}

%%\subsection{Propriedades Gerais}
Abaixo segue o Teorema que nos permitirá avaliar as componentes conexas de grupos topológicos como
subgrupos normais, fechados e conexos. Porém, antes de enunciarmos o Teorema, proporemos
um Lema, cuja demonstração pode ser vista em \cite{elon}, que nos auxiliará na prova do
Teorema~\ref{propriedades_G0}.
\begin{lema} \label{homeo_comp}
Dado uma função $f: (X, \mathcal{F}_1) \rightarrow (Y, \mathcal{F}_2)$ entre espaços topológicos e $C_a$ 
a componente conexa do ponto $a \in X$.
Se a função $f$ for um homeomorfismo teremos, então, que a imagem $f(C_a)$ será a componente conexa do ponto $f(a)$ em $Y$.
\end{lema}

\begin{teo}\label{propriedades_G0}
Dado um grupo topológico $(G, \mathcal{F}, \cdot)$ e, denotemos por $\mathbf{e}$, o elemento identidade do grupo. 
Então, a componente conexa da identidade, denotada por $G^0$, será um subgrupo normal, fechado e conexo de $G$.
\end{teo}

\begin{proof}
Sabemos que $G^0$ é um espaço conexo e fechado em $G$. Resta provarmos que $G^0$ é um subgrupo normal.
Para provarmos que $G^0$ é um subgrupo, devemos checar que
\begin{enumerate}[P1)]
	\item $\mathbf{e} \in G^0$;
	\item $x \in G^0$ implica $x^{-1} \in G^0$;
	\item $x, y \in G^0$ implica $x \cdot y \in G^0$.
\end{enumerate}
A propriedade P1) é imediata. Para todo $x \in G^0$, tem-se que, como já observado, o operador $L_{x^{-1}}: G \rightarrow G$
com $L_{x^{-1}}(y) = x^{-1} \cdot y$ é uma função contínua. Logo, a imagem $L_{x^{-1}}( G^0 )$ é
um conexo contendo a identidade, pois $L_{x^{-1}}(x) = \mathbf{e}$. Portanto,
\[ L_{x^{-1}}( G^0 ) \subseteq G^0 \]
e, como $L_{x^{-1}}(\mathbf{e}) \in L_{x^{-1}}( G^0 )$, obtemos que $x^{-1} \in G^0$, mostrando que P2) é satisfeita.
Para a propriedade P3), observe que, para todo $x, y \in G^0$, temos $x^{-1} \in G^0$, pela propriedade P2). 
Então, concluímos
que $\mathbf{e} \in L_x(G^0)$ e, por $L_x(G^0)$ ser um conexo contendo a identidade, obtemos
\[ \mathbf{e} \in L_x(G^0) \subseteq G^0, \]
ou seja, $L_x(y) \in L_x(G^0)$. Sendo assim, $x \cdot y \in G^0$ e a propriedade P3) está provada.

Finalmente, provemos que $G^0$ é um subgrupo normal de G. 
Dado $x \in G$, um elemento qualquer, temos que os operadores de multiplicação à 
esquerda, $L_x$, e à direita, $R_x$, são funções homeomorfas entre $G$. 
Pelo Lema~\ref{homeo_comp}, obtemos, por $G^0$ ser a componente conexa de $\mathbf{e}$,
que
\[ x G^0 x^{-1} = R_{x^{-1}} \circ L_x(G^0) = R_{x^{-1}}(C_{x \cdot e}) = C_{x \cdot e \cdot x^{-1}}, \]
i.e., $x G^0 x^{-1} = G^0$ e, a partir daí, concluímos que $G^0$ é um subgrupo normal de $G$.
\end{proof}


% \endfold

%%% Grupo de matrizes de Lie
% \beginfold
\section{Grupo de Matrizes de Lie} \label{sec:liegroup}

Iniciamos esta seção estabelecendo a definição de grupos de Lie:
\begin{definicao}[Grupos de Lie] 
Um grupo de Lie é uma variedade diferenciável que
admite uma estrutura de grupo onde as operações de multiplicação e inversão são deriváveis.
\end{definicao}

\begin{exmp}
O grupo $GL(n, \mathbb{R})$ é um grupo de Lie, visto que a multiplicação (resp. inversão) de matrizes
é contínua.
\end{exmp}

Em posse da estrutura de grupos de Lie, estudaremos os grupos de matrizes de Lie.
Dizemos que um subgrupo $\mathcal{H}$ do grupo topológico $GL(n, \mathbb{R})$ é um grupo de matrizes de Lie se $\mathcal{H}$ 
for um subconjunto fechado. \index{grupo!de matrizes de Lie}
Desta forma, pelo Teorema~\ref{propriedades_G0}, concluímos que a componente conexa da identidade é um grupo de matrizes de Lie.

Abaixo segue alguns exemplos clássicos de grupo de matrizes de Lie.
\begin{exmp} \label{exmp:det1}
Dado o grupo linear geral $GL(n, \mathbb{R})$. O subgrupo 
\[ SL(n, \mathbb{R}) := \{ A \in GL(n, \mathbb{R}); \; \det(A) = 1\}, \]
denominado por grupo linear especial, é um grupo de matrizes de Lie. Isto é verdade
pois 
$SL(n, \mathbb{R})$ é a
imagem inversa de um fechado, o conjunto $\{1\}$, por uma função contínua, o determinante.
\end{exmp}
%\begin{exmp}\label{subgrupoH}
%O subgrupo $H \leq GL(n, \mathbb{R})$ do Exemplo~\ref{exmp:det1}, dado pelas matrizes invertíveis de determinante $1$, i.e.,
%\[ H = det^{-1}(\{1\})\]
%é um grupo de matrizes de Lie. Isto é verdade pois 
%$H$ é a
%imagem inversa de um fechado por uma função contínua, o determinante.
%\end{exmp}

%\begin{exmp}
%??Outro exemplo de grupo de matrizes de Lie pode ser dado pelo conjunto
%\end{exmp}

\begin{exmp}\label{subgrupoH}
Dado o grupo linear geral $GL(n, \mathbb{R})$, temos que o conjunto
\[ O(n, \mathbb{R}) = \{ A \in GL(n, \mathbb{R}); \; AA^t = A^tA = I_n  \} \]
munido da multiplicação usual entre matrizes é um grupo de matrizes Lie, denotado por grupo ortogonal.
%a transposta da matriz $A$ e $I_n$ denota a matriz identidade). Para que $H$ seja um subgrupo do
%grupo linear geral devemos checar apenas a propriedade (iii), pois as propriedades (i) e (ii) são
%imediatas. Portanto, dadas as matrizes $A, B \in H$, temos:
%\[ (AB)(AB)^t = ABB^tA^t = I_n = B^tA^tAB = (AB)^t(AB). \]
%Logo, $H$ é um subgrupo de $GL(n, \mathbb{R})$.
\end{exmp}

\begin{exmp}
Seguindo o Exemplo acima, dos grupos ortogonais, temos que o grupo dado por
\[
SO(n, \mathbb{R}) = \{ A \in GL(n, \mathbb{R}); \; AA^t = A^tA = I_n \text{ e } \det(A) = 1  \} 
\]
é um grupo de matrizes de Lie, chamado de grupo ortogonal especial ou de grupo das rotações.
\end{exmp}

Veremos, a seguir, que com o auxílio do operador exponencial de matrizes podemos estabelecer relações com
os grupos de matrizes de Lie e seu espaço tangente (vide a subseção~\ref{subsec:espTg} para a definição de espaço
tangente). Para isso, iniciamos esta discussão definindo a exponencial de matrizes:

\begin{definicao}
A função exponencial de uma matriz quadrada $A \in \mathcal{M}_n( \mathbb{R} ) $ é dada por
\begin{align*}
\exp: \mathcal{M}_n( \mathbb{R} ) & \rightarrow \mathcal{M}_n( \mathbb{R} )\\
A & \mapsto \sum_{i=0}^{\infty} \frac{A^i}{i!}, \index{exponencial de uma matriz}
\end{align*} 
com $A^0 = I_n$. 
\end{definicao}

\begin{obs*}
Note que a série acima é absolutamente convergente pois, como para 
todo $A \in \mathcal{M}_n( \mathbb{R} )$ vale $\|A\| < \infty$,
temos que a norma da série é dada  pela seguinte desigualdade
\begin{align*}
 \left\|\sum_{i=0}^{\infty} \frac{A^i}{i!}\right\| &\leq \sum_{i=0}^{\infty} \frac{\|A^i\|}{i!}\\
					& = \sum_{i=0}^{\infty} \frac{\|A\|^i}{i!}\\
					& = e^{\|A\|}.
\end{align*}
\end{obs*}


Decorre da série ser absolutamente convergente que a função exponencial está bem definida.
Dada uma matriz $A \in \mathcal{M}_n( \mathbb{R} )$ qualquer, denotaremos, para simplificar a notação, por $e^A$
como sendo a matriz $\exp(A)$, i.e., $e^A := \exp(A)$.

\begin{prop}
Dadas as matrizes $A, B \in \mathcal{M}_n( \mathbb{R} )$ quaisquer
e $\lambda$ um autovalor da matriz $A$, com $v \in \mathbb{R}^n$ seu autovetor 
(i.e., $Av = \lambda v$).
Logo,  as seguintes propriedades para a exponencial são válidas:
\begin{enumerate}[P1)]
	\item $e^{\mathbf{0}} = I_n$;
	\item Se $AB = BA$, então $ e^{A + B} = e^A e^B$;
	\item $e^{BAB^{-1}} = B e^{A} B^{-1}$; 
	\item $ \det( \,e^{A} \, ) = e^{tr(A)}$; \label{detTr}
	\item $e^{t A}v = e^{t \lambda}v$, para $t$ um real qualquer. \label{expEig}
\end{enumerate}
\end{prop}

\begin{proof}
vide~\cite{stillwell}
\end{proof}

A partir da função exponencial podemos relacionar grupos de matrizes de Lie e seus espaços tangentes. 
Estas relações são dadas pelos seguintes
teoremas (cujas demonstrações podem ser encontradas em~\cite[capítulo 7, p.~143 e 149]{stillwell} ):
\begin{teo}\label{imagem_tg}
Seja $G$ um grupo de matrizes de Lie e $T_1(G)$ seu espaço tangente. Então, a imagem do espaço
tangente pela função exponencial é um subconjunto de $G$. Isto é:
\[ \exp( T_1(G) ) \subseteq G .\]
\end{teo}
\begin{teo}\label{bijecao_exp}
Seja $G$ um grupo de matrizes de Lie e $T_1(G)$ seu espaço tangente. 
Então, existe um número real, $r > 0$, suficientemente pequeno
e um aberto $U$ de $T_1(G)$ contendo a matriz $\mathbf{0} \in T_1(G)$ tal que
%a bola $B(I_n, r)$, de centro na identidade e raio $r > 0$, tal que 
\[ \exp: U \subseteq T_1(G) \rightarrow B(I_n, r) \subseteq G\]
é uma bijeção.
\end{teo}


% \endfold

% \endfold


%%% TODO Álgebras 
% \\beginfold Cap Algebras
\chapter{\'Algebras}
Introduziremos, neste capítulo, os conceitos necessários sobre álgebras para o nosso trabalho,
 tal qual, mostraremos como as
propriedades de associatividade e existência de unidade em álgebras, descritas com o auxílio
do produto tensorial, podem nos proporcionar uma forma de expressar estruturas algébricas
denotadas por coálgebras, cuja importância, para nós, se dará pelo fato do
coproduto do elemento do Casimir de uma certa álgebra de Lie (mais precisamente, do espaço tangente relativo ao
grupo de matrizes de Lie dado pelas matrizes estocásticas) ser uma matriz que, por uma perspectiva de processos
markovianos, será uma matriz
geradora de uma cadeia de Markov a tempo contínuo. Abordaremos, também, a relação existente entre álgebras de Lie
com os espaços tangentes de grupos de matrizes de Lie.
Este capítulo segue de perto os textos de \cite{algebra}, para o estudo das álgebras de Lie,
as notas de aula de \cite{coalgebra}, para a abordagem de coálgebras e biálgebras, assim como o 
texto de \cite{stillwell} para a demonstração do fato de que o espaço
tangente do grupo de matrizes de Lie é uma álgebra de Lie.

%%% conceitos basicos
% \\beginfold
\section{Conceitos B\'asicos}
Um $\mathbb{K}$-espaço vetorial $\mathcal{A}$ ($\mathbb{K}$ denotando um corpo) 
munido de uma multiplicação $M: \mathcal{A} \times \mathcal{A} \rightarrow \mathcal{A}$ é dito ser uma álgebra
se , para todo $x, y, z \in \mathcal{A}$ e $\lambda, \beta \in \mathbb{K}$, as seguintes propriedades forem satisfeitas:
\begin{enumerate}[(i)]
    \item $M(x, y + z) = M(x, y) + M(x, z)$ \textit{(distributiva à direita)};
    \item $M(x + y, z) = M(x, y) + M(y, z)$ \textit{(distributiva à esquerda)};
    \item $M( \lambda x, \beta y) = \lambda \beta M(x,y)$.
\end{enumerate} \index{álgebra}
Caso o operador de multiplicação satisfaça a propriedade
\[ M( M(x, y), z) = M(x, M(y, z)), \forall x, y, z \in \mathcal{A} \quad \text{ \textit{(associatividade)} }\]
dizemos que a álgebra em questão é associativa. Outra propriedade importante que algumas álgebras
podem apresentar é a existência de unidade, i.e., existe um elemento $1_{\mathcal{A}} \in \mathcal{A}$ tal que,
para todo $x \in \mathcal{A}$, vale a seguinte igualdade
\[ M(x, 1_{\mathcal{A}}) = M(1_{\mathcal{A}}, x) = x. \] 
Quando uma álgebra $\mathcal{A}$ possuir identidade dizemos que  $\mathcal{A}$ é uma 
álgebra com unidade. Mais a frente, quando estivermos 
estudando coálgebras e biálgebras, estaremos apenas interessados em álgebras associativas com unidade.
\index{álgebra!associativa com unidade}

Observe que o operador multiplicação nada mais é que um operador bilinear. Desta forma, pela propriedade
universal do produto tensorial (vide \cite{tensor}), existe uma aplicação linear 
$\widetilde{M}:\mathcal{A} \otimes \mathcal{A} \rightarrow \mathcal{A}$ tal que o diagrama
\begin{displaymath}
\begin{tikzcd}
    \mathcal{A} \times \mathcal{A} \arrow{rd }{M} \arrow{r}{ \otimes} & \mathcal{A} \otimes \mathcal{A}
    \arrow{d} {\widetilde{M}} \\
    & \mathcal{A}
\end{tikzcd}
\end{displaymath}
comuta. Além do mais, podemos definir uma aplicação linear $u: \mathbb{K} \rightarrow \mathcal{A}$ tal que
$u( 1_{ \mathbb{K} }) = 1_{ \mathcal{A} }$, ou seja, a função $u$ irá satisfazer, para todo $x \in \mathcal{A} $
e $\lambda \in \mathbb{K}$, a igualdade
\[ M(u(\lambda), x) = M(x, u(\lambda)). \]
Em outras palavras, uma álgebra associativa com unidade pode ser vista como uma terna $(\mathcal{A}, M, u)$
o qual, $\mathcal{A}$ é um $\mathbb{K}$-espaço vetorial e, $M:\mathcal{A} \otimes \mathcal{A} \rightarrow \mathcal{A} $ e
$u: \mathbb{K} \rightarrow \mathcal{A} $ são aplicações lineares cujos
diagramas 

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
\centering
%\begin{displaymath}
\begin{tikzcd}
    \mathcal{A} \otimes \mathcal{A} \otimes \mathcal{A} \arrow{r}{Id \otimes M}  \arrow[swap]{dd}{ M \otimes Id} &
    \mathcal{A} \otimes \mathcal{A} \arrow{dd}{M} \\
    \\
    \mathcal{A} \otimes \mathcal{A} \arrow{r}{M} &
    \mathcal{A} 
\end{tikzcd}
\vspace{.55cm}
\caption*{Diagrama 1, associatividade.}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
%\begin{displaymath}
\begin{tikzcd}
    & \mathcal{A} \otimes \mathcal{A}  \arrow{dd}{M} & \\ 
    \mathbb{K} \otimes \mathcal{A} \arrow{ur}{u \otimes Id} \arrow[swap]{rd}{s_1} & &
    \mathcal{A} \otimes \mathbb{K} \arrow[swap]{ul}{Id \otimes u } \arrow{ld}{s_2} \\
    & \mathcal{A} &
\end{tikzcd}
%\end{displaymath}
\caption*{Diagrama 2, existência da unidade}
\end{minipage}
\end{figure}
comutam. Acima, temos que as funções $Id, s_1$ e $s_2$ representam

\begin{minipage}{.33\textwidth}
    \begin{align*}
        Id: \mathcal{A} & \rightarrow \mathcal{A}\\
        x & \mapsto x
    \end{align*}
\end{minipage}%
\begin{minipage}{.33\textwidth}
    \begin{align*}
        s_1: \mathbb{K} \otimes \mathcal{A}  & \rightarrow \mathcal{A}\\
        (\lambda, x) & \mapsto \lambda x
    \end{align*}
\end{minipage}%
\begin{minipage}{.33\textwidth}
    \begin{align*}
        s_2: \mathcal{A} \otimes \mathbb{K}  & \rightarrow \mathcal{A}\\
        (x, \lambda) & \mapsto \lambda x.
    \end{align*}
\end{minipage}

\begin{obs*}
Com relação aos diagramas acima podemos notar que

\begin{enumerate}[(i)]
    \item O Diagrama 1 comutar significa que a multiplicação é associativa pois, dados os elementos
    $x, y, z \in \mathcal{A}$, temos
    \begin{align*}
        M \circ ( Id \otimes M)(x \otimes y \otimes z) & = M( Id(x) \otimes M(y, z)) \\
                                                       & = M(x \otimes M(y, z))
    \end{align*}
    e
    \begin{align*}
        M \circ ( M \otimes Id)(x \otimes y \otimes z) & = M( M(x, y) \otimes Id(z)\\
                                                       & = M( M(x, y) \otimes z)
    \end{align*}
    Logo, pela igualdade $M \circ (Id \otimes M) = M \circ (M \otimes Id)$, concluímos que
    $M( M(x, y) \otimes z) = M( x \otimes M( y \otimes z))$, ou seja, $M$ é associativa.
    \item O Diagrama 2 representa a existência de identidade em $\mathcal{A}$. Esta afirmação é válida
    se observarmos que $\mathbb{K} \otimes \mathcal{A} \cong \mathcal{A} \otimes \mathbb{K}$ e, portanto, 
    para todo $x \in \mathcal{A}$ vale
    \[ M \circ (u \otimes Id)(1_{\mathbb{k}} \otimes x) =  M \circ (Id \otimes u)(x \otimes 1_{\mathbb{k}} ) = x\]
    ou seja,
    \[ M(u(1_{\mathbb{K}}) \otimes x) = M(x \otimes u(1_{\mathbb{K}})) = x.\]
    Desta forma, $u(1_{\mathbb{K}})$ é o  elemento unitário da álgebra.
\end{enumerate}
\end{obs*}

\begin{exmp}
\textit{(A álgebra tensorial)} Seja $V$ um $\mathbb{R}$-espaço vetorial. Consideremos o seguinte conjunto:
\[ T(V) := \bigoplus_{k \geq 0} V^{\otimes k}, \]
com $V^{\otimes k} = V \otimes \cdots \otimes V$, $k$-vezes. O conjunto $T(V)$ será um $\mathbb{R}$-espaço vetorial se
considerarmos como operadores de soma e multiplicação por escalar as seguintes aplicações

\begin{minipage}{0.6\linewidth}
\centering
\begin{align*}
+: T(V) \times T(V) &\rightarrow T(V)\\
 \left(\sum_k v_k, \sum_k w_k \right) & \mapsto \sum_k (v_k + w_k),
\end{align*}
\end{minipage}%
\begin{minipage}{0.4\linewidth}
%\centering
\begin{align*}
\odot: \mathbb{R} \times T(V) &\rightarrow T(V)\\
 \left(\lambda,  \sum_k w_k \right) & \mapsto \sum_k (\lambda w_k),
\end{align*}
\end{minipage}
com as somas acima finitas e $v_k, w_k \in V^{\otimes k}$.

Além do mais, podemos definir uma multiplicação neste espaço vetorial:
\begin{align*}
M: T(V) \otimes T(V) &\rightarrow T(V)\\
 \left(\sum_k v_1^k\otimes \cdots \otimes v_{i_k}^k, \sum_l w_1^l\otimes \cdots \otimes w_{i_l}^l \right)
 & \mapsto \sum_{k,l} (v_1^k\otimes \cdots \otimes v_{i_k}^k \otimes w_1^l\otimes \cdots \otimes w_{i_l}^l),
\end{align*}
com $v_1^k, \ldots, v_{i_k}^k, w_1^l \ldots, w_{i_l}^l \in V$. % e $v_k \otimes w_l \in V^{\otimes (k+l)}$.
Decorre do produto tensorial ser associativo e de $V^{\otimes k} \otimes \mathbb{R} \cong V^{\otimes k}   $,
para todo inteiro $ k \geq 0$, que o espaço vetorial $T(V)$ será uma álgebra associativa com unidade, 
chamada de \textit{álgebra tensorial} do espaço vetorial $V$. \index{álgebra! tensorial}
Aqui, a unidade da álgebra tensorial é dada pela unidade $1 \in \mathbb{R}$, do corpo dos reais.
\end{exmp}

Para a álgebra tensorial vale a seguinte propriedade universal:
\begin{prop}[Propriedade universal da álgebra tensorial] \label{propUniv}
Seja $V$ uma espaço vetorial e $\mathcal{A}$ uma álgebra associativa com unidade.
Então, para toda transformação linear $f: V \rightarrow \mathcal{A}$ existe um homeomorfismo 
de álgebras 
$\phi: T(V) \rightarrow \mathcal{A}$,
entre a álgebra tensorial do espaço $V$ e a álgebra $\mathcal{A}$, tal que o diagrama abaixo comuta.
\begin{displaymath}
\begin{tikzcd}
    V \arrow[swap]{rd }{f} \arrow{r}{ i} & T(V) \arrow{d} {\phi} \\
    & \mathcal{A}
\end{tikzcd}
\end{displaymath}
Aqui, $i: V \rightarrow T(V)$ representa a aplicação inclusão do espaço vetorial na álgebra tensorial.
\end{prop}

\begin{proof}
Basta considerar a função $\phi: T(V) \rightarrow \mathcal{A}$ tal que 
\[\sum_k v_1^k\otimes \cdots \otimes v_{i_k}^k \mapsto \sum_k f(v_1^k)\otimes \cdots \otimes f(v_{i_k}^k),\]
com $v_1^k, \ldots, v_{i_k}^k \in V$.
\end{proof}

% \\endfold


%%% TODO Algebras de Lie
% \\beginfold Algebras de Lie

\section{\'Algebras de Lie}
Nesta seção introduziremos os conceitos de álgebras de Lie, ideais, álgebras semi-simples e espaços tangentes, e 
mostraremos como estes se relacionam.
%Em seguida, definimos o
%conceito de espaços tangentes e provamos sua propriedade de ser uma
%álgebra de Lie.
%%%% melhorar esta introducao de algebras de lie
%As álgebras de lie desempenham um papel importante em nosso estudo pois, como veremos mais
%a frente, esp tangentes de grupo de matrizes de lie podem ser vistos como algebras de lie.
%Esta propriedade de esp tangentes nos permite estudar estes conjuntos sob uma perspctiva algebrica
%que nos confere mais simplicidade ao trabalhar com elas. Sendo assim, iremos introduzir o conceito
%das álgebras de lie, ideais e mostraremos como o espaço tangente teem estas caracteristicas...

%%Ao estudarmos os espaços tangentes de grupos de matrizes de Lie observaremos que estes apresentam, como estrutura
%%algébrica, o fato de serem uma álgebra de Lie.
%%Neste capítulo, apresentaremos a definição de álgebras de Lie, subálgebras e
%%ideais. 

%%% DONE Definicoes Gerais
% \\beginfold Definicoes gerais
\subsection{Defini\c{c}\~oes Gerais}
Seja $\mathfrak{g}$ um $\mathbb{R}$-espaço vetorial. Dizemos que $\mathfrak{g}$ é uma álgebra de Lie se existir
um operador $[,]: \mathfrak{g} \times \mathfrak{g} \rightarrow \mathfrak{g}$, chamado de colchete de Lie, satisfazendo
\begin{enumerate}[(i)]
	\item $[,]$ é um operador bilinear;
	\item $[X, X] = 0$, para todo $X \in \mathfrak{g}$;
	\item Para todo $X, Y, Z \in \mathfrak{g}$ vale a igualdade, chamada de identidade de Jacobi,
		\[ [X, [Y, Z] ] + [Z, [X, Y] ] + [Y, [Z, X] ] = 0 .\]
\end{enumerate} \index{álgebra! de Lie}
Uma propriedade que decorre imediatamente do colchete de Lie é a propriedade antissimétrica, i.e.,
para todo $X, Y \in \mathfrak{g}$ vale $[X, Y] = - [Y, X]$. Para averiguarmos esta igualdade, observemos que
%Uma propriedade que decorre imediatamente é a seguinte: para todo $X, Y \in \mathfrak{g}$, $\mathfrak{g}$ uma álgebra de Lie, 
%vale a propriedade
%antissimétrica, i.e., $[X, Y] = [Y, X]$, pois
\[ 0 = [X + Y, X + Y] = [X, X] + [X, Y] + [Y, X] + [Y, Y], \]
ou seja, $[X, Y] = -[Y, X]$.
%que implica a igualdade desejada: $[X, Y] = [Y, X]$.
\begin{exmp}
O $\mathbb{R}$-espaço vetorial das matrizes $n \times n$ com entradas nos números reais
$\mathcal{M}_n( \mathbb{R} )$ pode ser visto
como uma álgebra de Lie se considerarmos o seguinte colchete (denotado por \textit{comutador}): \index{comutador}
\begin{align*}
[,]: \mathcal{M}_n( \mathbb{R} ) \times \mathcal{M}_n( \mathbb{R} ) & \rightarrow \mathcal{M}_n( \mathbb{R} )\\
(A, B) & \mapsto AB - BA.
\end{align*}
O operador $[,]$ é um colchete de Lie pois, para toda matriz $A, B, C \in \mathcal{M}_n( \mathbb{R} )$
 e para todo $\lambda \in \mathbb{R}$, temos
\begin{enumerate}[(i)]
	\item \begin{align*}
			[ \lambda \cdot A + B, C] &= (\lambda \cdot A + B)C - C( \lambda \cdot A + B )\\
					       &= \lambda \cdot AC + BC - \lambda \cdot CA - CB )\\
					       &= \lambda \cdot ( AC - CA) + (BC - CB ),
		\end{align*}
		ou seja, $[\lambda \cdot A + B, C] = \lambda \cdot [A , C] + [B, C]$. A prova de que 
		$[ A,  \lambda \cdot B + C] = \lambda \cdot [A , B] + [A, C]$ é similar ao que foi feito acima. Sendo assim,
		$[,]$ é um operador bilinear;
	\item $[A, A] = 0$, imediato;
	\item Somando as três igualdades abaixo
		\begin{itemize}
			\item $[A, [B, C]] = [A, BC - CB] = ABC - ACB -BCA + CBA$;
			\item $[C, [A, B]] = [C, AB - BA] = CAB - CBA -ABC + BAC$;
			\item $[B, [C, A]] = [B, CA - AC] = BCA - BAC -CAB + ACB$
		\end{itemize}
		obtemos a identidade de Jacobi, i.e.,
		\[ [A, [B, C]] + [C, [A, b]] + [B, [C, A]] = 0.\]
\end{enumerate}
Desta forma, concluímos que $\mathcal{M}_n( \mathbb{R} )$ munido do colchete $[A,B] = AB -BA$, 
para todo $A, B \in \mathcal{M}_n( \mathbb{R} )$, é uma álgebra de Lie.
\end{exmp}

\begin{exmp}\label{algNOTideal}
O espaço vetorial $\mathbb{R}^3$ munido do produto vetorial também é uma álgebra de Lie.
\end{exmp}

\begin{definicao}
Seja $\mathfrak{g}$ uma álgebra de Lie e $\mathfrak{h} \subseteq \mathfrak{g}$ um subespaço vetorial. 
Dizemos que $\mathfrak{h}$ é uma subálgebra de $\mathfrak{g}$ se,
para todo $X, Y \in \mathfrak{h}$, tivermos $[X, Y] \in \mathfrak{h}$.
\end{definicao} \index{subálgebra}

Um exemplo de uma subálgebra é o seguinte: seja $\mathfrak{g}$ uma álgebra de Lie e $\mathfrak{h}$ o subconjunto
\[ \mathfrak{h} = \left\{ X \in \mathfrak{g}; \; [X, Y] = 0, \text{ para todo } Y \in \mathfrak{g} \right\}. \]
Então $\mathfrak{h}$ é uma subálgebra de $\mathfrak{g}$ chamada de \textit{centro da álgebra} de $\mathfrak{g}$. 
Para verificarmos que $\mathfrak{h}$ é uma subálgebra, devemos
checar que \index{centro da álgebra de Lie}
\begin{itemize}
	\item $\mathfrak{h}$ é um espaço vetorial;
	\item para todo $X, Y \in \mathfrak{h}$ temos $[X, Y] \in \mathfrak{h}$.
\end{itemize}
Que $\mathfrak{h}$ é um espaço vetorial é imediato, pois
\begin{enumerate}[(i)]
	\item $[\mathbf{0}, Y] = 0 \cdot [\mathbf{0}, Y] = 0$, para todo $Y \in \mathfrak{g}$. Portanto $\mathbf{0} \in \mathfrak{g}$;
	\item Dados $X, Y \in \mathfrak{h}$, temos que, para todo $Z \in \mathfrak{g}$, vale
		\[ [X + Y, Z] = [X, Z] + [Y, Z] = 0. \]
		Logo, $X+Y \in \mathfrak{h}$;
	\item Dados $X \in \mathfrak{h}$ e $\lambda \in \mathbb{R}$, vale, para todo $Y \in \mathfrak{g}$, que
		\[ [\lambda X, Y] = \lambda [X, Y] = 0. \]
		Então, $\lambda X \in \mathfrak{h}$.
\end{enumerate}
Finalmente, para provarmos que $\mathfrak{h}$ é uma subálgebra de $\mathfrak{g}$, devemos mostrar que,
 para todo $X, Y \in \mathfrak{h}$, vale $[X, Y] \in \mathfrak{h}$.
Para isso, observe que pela identidade de Jacobi temos, para todo $Z \in \mathfrak{g}$,  a seguinte igualdade
\[ [Z, [X, Y] ] + [Y, [Z, X] ] + [X, [Y, Z] ] = 0 .\]
Como $[Z, X] = [Y, Z] = 0$, obtemos $[Z, [X, Y] ] = 0$ e, portanto, concluímos que $[X, Y] \in \mathfrak{h}$.

Outro exemplo de subálgebra é o seguinte:
\begin{exmp}\label{tr_lie}
O espaço 
\[ sl(n) = \left\{ X \in \mathcal{M}_n( \mathbb{R} ); \; tr(X) =  0 \right\} \]
é uma subálgebra de $\mathcal{M}_n( \mathbb{R} )$. 
Isto é verdade, visto que, para todo $X, Y \in sl(n)$ e para todo $\lambda \in \mathbb{R}$, temos
\[ tr( \lambda \cdot X + Y) = \lambda \cdot tr(X) + tr(Y) = 0 \]
e $\mathbf{0} \in sl(n)$ (aqui, $\mathbf{0}$ representa a matriz com entradas todas nulas).
Portanto, $sl(n)$ é um subespaço vetorial e
\begin{align*}
tr( [X, Y] ) &= tr( XY - YX )\\
	     &= tr(XY) - tr(YX)\\
	     &= tr(XY) - tr(XY) = 0,
\end{align*}
ou seja, $[X, Y] \in sl(n)$.
\end{exmp}

\begin{definicao}
Seja $\mathfrak{g}$ uma álgebra de Lie e $\mathfrak{h}$ um subespaço vetorial. 
Dizemos que $\mathfrak{h}$ é um ideal de $\mathfrak{g}$ se, para todo $X \in \mathfrak{g}$ e 
para todo $Y \in \mathfrak{h}$, tivermos $[X, Y] \in \mathfrak{h}$.
\end{definicao} \index{ideal de uma álgebra de Lie}
Da definição de ideal decorre imediatamente que todo ideal também é uma subálgebra, porém, a volta não é verdadeira. 
Pelo Exemplo~\ref{algNOTideal}  podemos construir um exemplo de subálgebra que não é um ideal. Considere o subespaço vetorial
\[ \mathfrak{h} = \left\{ \lambda \cdot (1,0,0); \; \lambda \in \mathbb{R} \right\} .\]
Temos que $\mathfrak{h}$ é uma subálgebra de $\mathbb{R}^3$ pois, para todo $\lambda_1, \lambda_2 \in \mathbb{R}$, temos
\[ \lambda_1 \cdot (1,0,0) \times \lambda_2 \cdot (1,0,0) = 0 \in \mathfrak{h}.\]
Agora, tome o vetor $(0,1,0)$. O produto vetorial entre $(0,1,0)$ e $(1,0,0)$ será igual a
\[ (0,1,0) \times (1,0,0) = (0,0,-1) \notin \mathfrak{h}, \]
ou seja, $\mathfrak{h}$ não é um ideal.
%
% \\endfold Definicoes Gerais

%%%\subsection{O Elemento do Casimir de uma \'Algebra de Lie}
\subsection{\'Algebras de Lie semi-simples e a Representa\c{c}\~ao Adjunta}
%%%Dada a álgebra de Lie das matrizes 
%%%quadradas $\mathcal{M}_n(\mathbb{R})$, com colchete dado pelo comutador, e uma subálgebra
%%%$\mathfrak{h} \subseteq \mathfrak{g}$.
%%%Estaremos interessados em obter, nesta subseção, uma fórmula fechada 
%%%para um elemento não nulo $C \in \mathcal{M}_n(\mathbb{R})$
%%%tal que $[C,X] = 0$, para todo $X \in \mathfrak{h}$, i.e., queremos 
%%%que $C$ seja um elemento do centro da subálgebra $\mathfrak{h}$. Veremos, a seguir, que tal elemento existirá
%%%se a álgebra $\mathfrak{h}$ satisfazer a condição de ser semi-simples. Tal elemento será
%%%chamado de elemento do Casimir de $\mathfrak{g}$. Porém, antes devemos
%%%estabelecer alguns conceitos.
Nesta subseção estabeleceremos o conceito de álgebras de Lie semi-simples e o conceito
de representação adjunta. No estudo do elemento do Casimir de uma álgebra de Lie, i.e.,
no estudo de certos elementos do centro de uma álgebra, teremos que tais elementos
possuem uma fórmula fechada quando sua álgebra for semi-simples. Devido a importância do
elemento do Casimir em nosso estudo, nesta subseção iremos abordar estas propriedades.

\begin{definicao} \index{série derivada de uma álgebra de Lie}
\textit{(Série derivada)} Seja $\mathfrak{g}$ uma álgebra de Lie. Definimos por sua série
derivada como sendo a sequência de ideais de $\mathfrak{g}: \mathfrak{g}^{(1)}, \ldots, 
\mathfrak{g}^{(n)}, \mathfrak{g}^{(n+1)},\ldots$ satisfazendo
\[
\mathfrak{g}^{(1)} = \mathfrak{g} \quad \text{e} \quad \mathfrak{g}^{(n+1)} = [\mathfrak{g}^{(n)}, \mathfrak{g}^{(n)}],
\]
com $[\mathfrak{g}^{(n)}, \mathfrak{g}^{(n)}]$ sendo o seguinte conjunto:
\[
[\mathfrak{g}^{(n)}, \mathfrak{g}^{(n)}] = \{ [X,Y] \in \mathfrak{g}; \;
    X \in \mathfrak{g}^{(n)} \text{ e } Y \in \mathfrak{g}^{(n)}\}.
\]
\end{definicao}

\begin{obs*}\index{álgebra! de Lie solúvel}
Dizemos que uma álgebra de Lie $\mathfrak{g}$ é \textit{solúvel} se existir um natural $n > 1$ tal que
$\mathfrak{g}^{(n)} = \{0\}$.
\end{obs*}

\begin{exmp}
Seja a álgebra de Lie dada por
\[
\mathfrak{g} = 
\spn \left\{
X_1 =
\begin{pmatrix}
-1 & 1 \\
0 & 0 
\end{pmatrix}
, \quad
X_2 =
\begin{pmatrix}
0 & 0 \\
1 & -1 
\end{pmatrix}
\right\}
\]
com colchete de Lie dado pelo comutador. Fazendo os cálculos, obtemos que a série derivada
de $\mathfrak{g}$ é dada por:
$\mathfrak{g}^{(1)} = \mathfrak{g}, \mathfrak{g}^{(2)} = \spn\{X_2 - X_1\}$ e $\mathfrak{g}^{(3)} = \{0\}$.
Pela igualdade, $\mathfrak{g}^{(3)} = \{0\}$,
obtemos que $\mathfrak{g}$ é uma álgebra de Lie solúvel.
\end{exmp}

\begin{definicao}\index{álgebra!de Lie semi-simples}
Seja $\mathfrak{g}$ uma álgebra de Lie. Dizemos que $\mathfrak{g}$ é uma álgebra de Lie semi-simples 
se o único ideal solúvel $\mathfrak{h}$ de $\mathfrak{g}$ for o ideal dado por
$\mathfrak{h} = \{0\}$.
%\[
%\mathfrak{r(g)} = \{ \mathfrak{s} \subseteq \mathfrak{g}; \; \mathfrak{s} \text{ é um ideal de } \mathfrak{g}
%					 \text{ e } 
%\]
\end{definicao}

%Em posse destas definições e pelo fato do colchete de Lie ser uma forma bilinear, definimos 
%a seguir a representação adjunta de uma álgebra de Lie:
A seguir, definimos o conceito de representação adjunta:

%%, nos resta apenas o conceito de representação adjunta e o
%%conceito da Forma de Cartna-Killing para deduzirmos uma fórmula fechada para o elemento do
%%Casimir de uma álgebra semi-simples.

%elemento do centro desta álgebra
%Decorre das propriedades do colchete de Lie que podemos definir o seguinte homeomorfismo de álgebras:
\begin{definicao}
Seja $\mathfrak{g}$ uma álgebra de Lie e $\mathfrak{gl(g)}$ a álgebra de Lie dada pelas transformações
lineares $h: \mathfrak{g} \rightarrow \mathfrak{g}$  com colchete de Lie dado pelo comutador, i.e.,
para todo $f,h \in \mathfrak{g}$ vale $[f,h] = f \circ h - h \circ f$. Então, chamamos de	 \textit{representação
adjunta} de $\mathfrak{g}$ a seguinte aplicação: \index{representação adjunta}
\begin{align*}
\Adj: \mathfrak{g} & \rightarrow \mathfrak{gl(g)}                          \\
                X & \mapsto \Adj(X): \mathfrak{g}  \rightarrow \mathfrak{g}\\
                  &                             \qquad \qquad  \quad \; \; Y  \mapsto [X, Y].
\end{align*}
\end{definicao}

%A representação adjunta é, de fato, um homeomorfismo de álgebras pois, 
Dados $X, Y \in \mathfrak{g}$ e $\lambda$ um 
número real, temos, para todo $Z \in \mathfrak{g}$, que
\[
\Adj(X + \lambda Y)(Z) = [X + \lambda Y, Z] = [X,Z] + \lambda [Y, Z], 
\]
ou seja, $\Adj(X + \lambda Y) = \Adj(X) + \lambda \Adj(Y)$. Além do mais,
\begin{align*}
 \Adj([X,Y])(Z) & = [[X, Y], Z] \\ 
 				& =  -( [Y, [Z, X]] + [X, [Y, Z]] )\qquad (\text{\textit{igualdade obtida pela identidade de Jacobi}}) \\
 				& = -( -[Y, [X, Z]] + [X, [Y, Z]] ) \\
				& = -( - \Adj(Y) \circ \Adj(X) + \Adj(X) \circ \Adj(Y) ) (Z) \\
				& = [\Adj(X), \Adj(Y)](Z).
\end{align*}
Logo, obtemos que a representação adjunta é um homeomorfismo de álgebras de Lie.

%%%Em função da representação adjunta de uma álgebra de Lie, estabelecemos a seguinte forma
%%%bilinear e simétrica, chamada de \textit{Forma de Cartan-Killing}: \index{forma de Cartan-Killing}
%%%\begin{align*}
%%%<,>: \mathfrak{g} \times \mathfrak{g} &\rightarrow \mathfrak{g}\\
%%%(X,Y) &\mapsto \Tr( \Adj(X) \circ \Adj(Y) ),
%%%\end{align*}
%%%Veremos, no teorema a seguir, um critério
%%%para que forma de Cartan-Killing de uma álgebra de Lie de dimensão finita 
%%%seja não-degenerada: 
%%%\begin{teo}
%%%Seja $\mathfrak{g}$ uma álgebra de Lie de dimensão finita e semi-simples. Então, a forma de Cartan-Killing é não-degenerada.
%%%\end{teo}
%%%Além do mais, podemos determinar se uma álgebra de Lie é semi-simples a partir da matriz da forma de Cartan-Killing:
%%%\begin{teo}
%%%Seja $\mathfrak{g}$ uma álgebra de Lie de dimensão finita gerada pela base $X_1, X_2, \ldots, X_n$
%%% e $k$ a matriz da forma de Cartan-Killing
%%%\[
%%%k = [<X_i, X_j>]_{i,j = 1,\ldots,n}.
%%%\]
%%%Se a matriz $k$ for inversível então $\mathfrak{g}$ é uma álgebra semi-simples.
%%%\end{teo}
%%%As demonstrações destes dois Teoremas podem ser encontradas em \cite{algebra}. Note que que estamos trabalhando
%%%com o corpo dos números reais, cuja característica é zero.
%%%
%%%Finalmente, em posse destes resultados podemos definir o elemento do Casimir de uma subálgebra $\mathfrak{h}$ semi-simples
%%%da álgebra de Lie das matrizes quadradas $\mathfrak{g} = \mathcal{M}_n(\mathbb{R})$, munidas com o colchete dado pelo comutador.
%%%
%%%Supondo que nossa subálgebra $\mathfrak{h}$ seja gerada pela base $X_1, \ldots, X_n$. O elemento do Casimir
%%%$C$ de $\mathfrak{h}$ é dado por:
%%%\[
%%%C = \sum_{i=1}^n X_i Y_i,
%%%\]
%%%com $Y_1, \ldots, Y_n$ uma base dual de $\mathfrak{h}$ (cuja existência ocorre devido a subálgebra ser semi-simples)
%%% que  satisfaz: $<Y_i, X_j>= \delta_{ij}$. Aqui,
%%%$\delta_{ij}$ representa o delta de Kronecker. \index{elemento do Casimir}
%%%
%%%Mostremos que $C$ comuta com todos elementos da base $X_1, \ldots, X_n$.
%\subsection{Exemplos}

\subsection{A \'Algebra Linear Especial de Lie $sl_2(\mathbb{R})$}

Um caso específico de álgebra de Lie, que surgirá em nosso trabalho, será 
a álgebra linear especial 
$sl_2(\mathbb{R})$ sobre o corpo dos reais
(``special linear Lie algebra''). Abaixo segue sua definição:

\begin{definicao}
A álgebra $sl_2(\mathbb{R})$ é 
a álgebra de Lie das matrizes quadradas $2 \times 2$ com
colchete de Lie dado pelo comutador e o qual as matrizes 
satisfazem a condição de terem
seu traço igual a zero:
\[
sl_2(\mathbb{R}) = \{A \in M_2(\mathbb{R}); \; \tr(A) = 0\}.
\]
\end{definicao}
A álgebra $sl(2, \mathbb{R})$ admite como base os vetores
\[
E := 
\begin{pmatrix}
0&1\\
0&0
\end{pmatrix}
\qquad
F := 
\begin{pmatrix}
0&0\\
1&0
\end{pmatrix}
\qquad
H := 
\begin{pmatrix}
1&0\\
0&-1
\end{pmatrix}
\qquad
\]
com comutadores dados por
\[
[E,F] = H, \quad [H,F] = -2F, \quad [H,E] = 2E.
\]

Dentre as diversas	propriedades de $sl_2(\mathbb{R})$, as quais podem ser
vistas em \cite{lang}, destacamos as três abaixo:

\begin{prop}
A álgebra $sl_2(\mathbb{R})$ é uma álgebra simples, i.e., esta álgebra contém apenas
os ideais triviais, dados pela identidade e a própria álgebra.
\end{prop}

\begin{prop}
Um elemento de Casimir , denotado por $C$,
 da álgebra $sl_2(\mathbb{R})$(i.e., um elemento do centro da álgebra), considerando a base 
acima, é dado por
\[
C = H^2 + 2(E F  + F H).
\]
\end{prop}

\begin{prop}
A álgebra universal envelopante $U(sl_2(\mathbb{R})$, vide Seção,
é a álgebra livre gerada pelos elementos $e, f, h$ que satisfazem
as restrições
\[
e f -f h = h, \quad h f -f h = -2f, \quad h e - e h = 2e.
\]
\end{prop}

% \\beginfold Espaco Tangente e suas propriedades
\subsection{ O Espa\c{c}o Tangente e suas propriedades algébricas} \label{subsec:espTg}
Seja $G$ um subgrupo do grupo linear geral $GL(n, \mathbb{R})$. 
Dizemos que uma função \mbox{$g:[0,1] \rightarrow G$} é um caminho diferencial se
$g$ for diferenciável, ou seja, $g(t) = ( a_{ij}(t) )$ satisfaz a condição de que 
cada entrada $a_{ij}:[0,1] \rightarrow \mathbb{R}$  seja uma função diferenciável.
Com o conceito de caminhos diferenciáveis, definimos: \index{caminho diferenciável}
\begin{definicao}
Seja $G$ um subgrupo de $GL(n, \mathbb{R})$. Dizemos que o espaço tangente de $G$,  denotado por $T_1(G)$, é o seguinte conjunto
\begin{align*}
 T_1(G) = \{ A \in \mathcal{M}_n(\mathbb{R}); 	\; \exists g:[0,1] \rightarrow G & \text{ um caminho diferenciável} \\
						   & \text{tal que } g(0) = I_n \text{ e } g'(0) = A \}, 
\end{align*}
com $\mathcal{M}_n(\mathbb{R})$ o conjunto das matrizes $n \times n$ e $I_n$ a matriz identidade.
\end{definicao} \index{espaço!tangente}

Ou seja, o espaço tangente $T_1(G)$ de um subgrupo $G$ do grupo linear geral é o conjunto dos vetores tangentes a identidade em 
$G$. Além do mais, veremos a seguir que o espaço tangente satisfaz a propriedade de ser uma álgebra de Lie. Porém,
antes de provarmos este fato, ilustremos o conceito de espaço tangente pelo seguinte exemplo:
\begin{exmp}
Considere o grupo linear geral das matrizes $2 \times 2$:
\[ GL(2, \mathbb{R}) = \left\{ 
A= 
 \begin{pmatrix}
 a & b \\
 c & d
 \end{pmatrix}
; \;
ad - bc \neq 0. \right\} \]
Sabemos que o conjunto $H = \{ A \in GL(2, \mathbb{R}); \; \det(A) = 1 \}$ é um subgrupo do grupo linear geral.
Logo, todo caminho diferenciável $g:[0,1] \rightarrow H$, satisfazendo $g(0) = I_2$ ($I_2$ a matriz identidade),
deve satisfazer, se considerarmos
\[ g(t) =
 \begin{pmatrix}
 a(t) & b(t) \\
 c(t) & d(t)
 \end{pmatrix}
 ,
\]
a restrição $ ( a(t)d(t) - b(t)c(t) )' = 0$, ou seja, quando $t = 0$ devemos ter
\[ a'(0)d(0) + a(0)d'(0) = b'(0)c(0) + b(0)c'(0). \]
Porém, dada a igualdade $g(0) = I_2$, temos que
$a(0) = d(0) = 1$ e $c(0) = b(0) = 0$. Logo, nossa restrição para $g'(0)$ será dada por:
$a'(0) + d'(0) = 0$. Sendo assim, obtemos que
\[ g'(0) =
 \begin{pmatrix}
 a'(0) & b'(0) \\
 c'(0) & -a'(0)
 \end{pmatrix}
.
\]
Ou seja, o espaço tangente de $H$ é dado por:
\[ T_1(H) = 
\left\{
 \begin{pmatrix}
 x & y \\
 z & -x
 \end{pmatrix}
 ; \;
 x, y, z \in \mathbb{R}.
 \right\}
\]
\end{exmp}

\begin{teo}\label{tg_lie}
O espaço tangente $T_1(G)$ de um subgrupo $G$ de $GL(n, \mathbb{R})$ é uma álgebra de Lie.
\end{teo}

\begin{proof}
Inicialmente, mostremos que $T_1(G)$ é um $\mathbb{R}$-espaço vetorial. 
Dadas as matrizes $X, Y \in T_1(G)$, existem os seguintes caminhos 
diferenciáveis
\[ g_X: [0,1] \rightarrow G \qquad \text{e} \qquad g_Y: [0,1] \rightarrow G, \]
satisfazendo
\[
    \begin{cases}
     g_X(0)  = I_n\\
     g_X'(0)  = X 
   \end{cases}
	\qquad 
	\text{ e }
	\qquad
    \begin{cases}
     g_Y(0)  = I_n\\
     g_Y'(0)  = Y 
   \end{cases}
   .
\]
Observe que a função
\begin{align*}
h: [0,1] & \rightarrow G\\
     t   & \mapsto g_X(t) \cdot g_Y(t)
\end{align*}
é diferenciável e $h(0) = I_n$. Tomando a derivada de $h$ no ponto $0$, obtemos
\begin{align*}
h'(0) & = g_X'(0) \cdot g_Y(0) + g_X(0) \cdot g_Y'(0)\\
      & = X + Y,
\end{align*}
portanto, $X + Y = h'(0) \in T_1(G)$. Agora, dado um escalar $\lambda \in \mathbb{R}$ e uma matriz $X \in T_1(G)$ qualquer, obtemos, como anteriormente,
um caminho diferenciável $g_X:[0,1] \rightarrow G$, com $g_X(0) = I_n$ e $g_X'(0) \in X$. Tome a função
\begin{align*}
\psi: [0,1] & \rightarrow [0,1]\\
	t   & \mapsto 
    \begin{cases}
     \lambda t  \cdot \mathbb{1}\{ t \leq 1/ \lambda \}, & \text{ se } \lambda \neq 0; \\
     0, & \text{ se } \lambda = 0.
   \end{cases}
\end{align*} 
Vemos que $\psi$ é diferenciável e, portanto, a composta $g_X \circ \psi$ é um caminho diferenciável, com $(g_X \circ \psi) (0) = I_n$.
Tomando a derivada de $(g_X \circ \psi)$, obtemos, pela regra da cadeia, que
\begin{align*}
(g_X \circ \psi)'(0)  & =  g_X'( \psi(0) ) \cdot \psi'(0) \\
		      & = g_X'(0) \cdot \lambda \\
		      & = \lambda X.
\end{align*}
Portanto, $T_1(G)$ é um $\mathbb{R}$-espaço vetorial.
Agora, provemos que este espaço vetorial tem estrutura de uma álgebra de Lie. Como sabemos que o conjunto das matrizes $\mathcal{M}_n(\mathbb{R})$
munido do colchete
\begin{align*}
[,] : \mathcal{M}_n(\mathbb{R}) \times \mathcal{M}_n(\mathbb{R}) & \rightarrow \mathcal{M}_n(\mathbb{R})\\
(A, B) & \mapsto AB - BA
\end{align*}
é uma álgebra de Lie, devemos provar que o colchete $[,]$ restrito a $T_1(G)$ tem imagem ainda em $T_1(G)$, i.e., dados $A, B \in T_1(G)$, devemos provar que 
$[A, B] \in T_1(G)$.

Dadas as matrizes $A, B \in T_1(G)$, existem os caminhos diferenciáveis
\[ g_A: [0,1] \rightarrow G \qquad \text{e} \qquad g_B: [0,1] \rightarrow G, \]
satisfazendo
\[
    \begin{cases}
     g_A(0)  = I_n\\
     g_A'(0)  = A 
   \end{cases}
	\qquad 
	\text{ e }
	\qquad
    \begin{cases}
     g_B(0)  = I_n\\
     g_B'(0)  = B 
   \end{cases}
   .
\]
Definimos agora a função auxiliar
\begin{align*}
\xi: [0,1] \times [0,1] & \rightarrow G \\
(s,t) & \mapsto g_A(s) \cdot g_B(t) \cdot g_A^{-1}(s).
\end{align*}
Tomando a derivada de $\xi$ em relação a coordenada $t$, obtemos
\[
\frac{\mathrm d}{\mathrm d t} \xi(s,t) = g_A(s) \cdot g_B'(t) \cdot g_A^{-1}(s) .
% g_A'(s) \cdot g_B(t) \cdot g_A^{-1}(s) + g_A(s) \cdot g_B(t) \{ -g_A^{-1}(s) \cdot g_A'(t) \cdot g_A^{-1}(s)
\]
Logo, para todo $s \in [0,1]$, vale
\[
\xi(s, 0) = I_n \qquad \text{ e } \qquad \frac{\mathrm d}{\mathrm d t} \xi(s, 0) = g_A(s) \cdot B \cdot g_A^{-1}(s) \in T_1(G).
\]
Com isto, definimos o operador $\rho: [0,1] \rightarrow T_1(G)$, com $\rho(s) := \frac{\mathrm d}{\mathrm d t} \xi(s, 0)$.

Agora, observe que o espaço tangente é um subespaço vetorial de $\mathcal{M}_n(\mathbb{R})$, que por sua vez é um espaço vetorial normado de dimensão
finita. Logo, $T_1(G)$ é um subconjunto fechado em $\mathcal{M}_n(\mathbb{R})$. Tomando a derivada do operador $\rho$, obtemos
\[
\frac{\mathrm d}{\mathrm d s} \rho(s) = g_A'(s) \cdot B \cdot g_A^{-1}(s) + g_A(s) \cdot B \{ -g_A^{-1}(s) \cdot g_A'(t) \cdot g_A^{-1}(s) \}
%A \cdot g_B'(t) - g_B'(t) \cdot A.
\]
Esta derivada, calculada em $s = 0$, fornece $\rho'(0) = AB- BA$. Porém, a derivada de $\rho$ em relação ao ponto $0$ pode ser vista como
\begin{align*}
\frac{\mathrm d}{\mathrm d s} \rho(0) & = \lim_{h \rightarrow 0} \left\{ \frac{g_A(h) \cdot B \cdot g_A^{-1}(h) - g_A(0) \cdot B \cdot g_A^{-1}(0)}{h}\right\} \\
 & = \lim_{h \rightarrow 0} \left\{ \frac{g_A(h) \cdot B \cdot g_A^{-1}(h) - B  }{h} \right\}.
\end{align*}
Ou seja, para $n \in \mathbb{N}$, a sequência
\[
x_n = \frac{g_A(1/n) \cdot B \cdot g_A^{-1}(1/n) - B }{1/n} \in T_1(G)
\]
converge para $AB - BA$. Como $T_1(G)$ é um conjunto fechado, obtemos de 
\[ x_n \xrightarrow{n \rightarrow \infty} AB - BA,\] que:
\[ [A, B] = AB - BA \in T_1(G). \]
Sendo assim, concluímos que $T_1(G)$ é uma álgebra de Lie.
\end{proof}

% \\endfold Espaco Tangente e suas propriedades

% \\endfold Algebras de Lie

%%% TODO Coalgebras
% \\beginfold Coalgebras
%\section{Co\'algebras}
% \\endfold Coalgebras

%%% TODO Bialgebras
% \\beginfold Bialgebras
\section{Bi\'algebras} \label{bialg}
Antes de definirmos uma biálgebra precisamos estabelecer o conceito de coálgebras:
\begin{definicao}

Dada a terna $(\mathcal{C}, \delta, \varepsilon)$, onde $\mathcal{C}$ representa um $\mathbb{K}$-espaço vetorial munido
dos operadores 
$\Delta: \mathcal{C} \rightarrow \mathcal{C} \otimes \mathcal{C}$, chamado de coproduto, e 
$\varepsilon: \mathcal{C} \rightarrow \mathbb{R}$,
denotado por counidade. Dizemos que esta terna é uma coálgebra se os diagramas abaixo comutarem:
\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
\centering
%\begin{displaymath}
\begin{tikzcd}
    \mathcal{C} \otimes \mathcal{C} \otimes \mathcal{C}  &
    \mathcal{C} \otimes \mathcal{C}  \arrow[swap]{l}{Id \otimes \Delta} \\
    \\
    \mathcal{C} \otimes \mathcal{C} \arrow{uu}{\Delta \otimes Id} &
    \mathcal{C} \arrow{l}{\Delta} \arrow[swap]{uu}{\Delta}
\end{tikzcd}
\vspace{.55cm}
\caption*{Diagrama 1, coassociatividade.}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
%\begin{displaymath}
\begin{tikzcd}
    & \mathcal{C} \otimes \mathcal{C} \arrow[swap]{dl}{\varepsilon \otimes Id}  \arrow{dr}{Id \otimes \varepsilon } & \\ 
    \mathbb{K} \otimes \mathcal{C}  & &
    \mathcal{C} \otimes \mathbb{K} \\
    & \mathcal{C} \arrow{uu}{\Delta} \arrow{ul}{s^{-1}_1} \arrow[swap]{ru}{s^{-1}_2} &
\end{tikzcd}
%\end{displaymath}
\caption*{Diagrama 2, existência da counidade}
\end{minipage}
\end{figure}

Acima, temos que as funções $Id, s^{-1}_1$ e $s^{-1}_2$ são dadas por:

\begin{minipage}{.33\textwidth}
    \begin{align*}
        Id: \mathcal{C} & \rightarrow \mathcal{C}\\
        x & \mapsto x
    \end{align*}
\end{minipage}%
\begin{minipage}{.33\textwidth}
    \begin{align*}
        s^{-1}_1: \mathcal{C}& \rightarrow \mathbb{K} \otimes \mathcal{C}  \\
        x & \mapsto 1\otimes x
    \end{align*}
\end{minipage}%
\begin{minipage}{.33\textwidth}
    \begin{align*}
        s^{-1}_2:  \mathcal{C}&  \rightarrow  \mathcal{C} \otimes \mathbb{K} \\
        x & \mapsto x \otimes 1.
    \end{align*}
\end{minipage}
\end{definicao}

Uma coálgebra importante em nossos estudos é a \textit{álgebra universal envelopante}.
Esta álgebra, de certa forma, ``estende'' álgebras de Lie para álgebras associativas com unidade,
preservando o operador colchete. 
\begin{definicao} \index{álgebra! universal envelopante}
\label{algUniv}
 \textit{(álgebra universal envelopante)} Seja $\mathfrak{g}$ uma álgebra de Lie, $T(\mathfrak{g})$ sua álgebra tensorial e
$I(\mathfrak{g})$ o ideal dado por:

\begin{equation*} \tag{$\ast$} \label{idealUniv}
I(\mathfrak{g}) = \spn \left\{ a \otimes b - b \otimes a - [a,b] \in T(\mathfrak{g}); \;
								  a,b \in \mathfrak{g} \right\}. 
\end{equation*}
Denotamos por $U(\mathfrak{g}) = T(\mathfrak{g}) / I(\mathfrak{g})$, o quociente da álgebra 
tensorial pelo ideal $I(\mathfrak{g})$, como sendo a álgebra universal envelopante de $\mathfrak{g}$.
\end{definicao}

\begin{obs*}
Dado $\mathfrak{g}$ uma álgebra de Lie, $I(\mathfrak{g})$ o ideal dado acima
na Definição~\ref{algUniv} e $U(\mathfrak{g})$ sua álgebra universal envelopante, temos, para
todo $a, b \in \mathfrak{g}$, que
\[ ( a \otimes b - b \otimes a - [a, b]) + I(\mathfrak{g}) = I(\mathfrak{g}) \]
pois $a \otimes b - b \otimes a - [a, b] \in I(\mathfrak{g})$. Logo,
\[ ( a \otimes b - b \otimes a ) + I(\mathfrak{g}) = ([a,b]) + I(\mathfrak{g}), \]
\end{obs*}
ou seja, o operador colchete da álgebra de Lie $\mathfrak{g}$ é dado pelo comutador
 na álgebra universal envelopante. 

Considerando a álgebra de Lie dada pelo espaço das matrizes quadradas $n \times n$ com entrada nos reais munida
do comutador. 
Mostremos que
a álgebra universal envelopante $U(\mathfrak{g})$ pode ser vista como uma coálgebra.
Para simplificar a notação, denotaremos o espaço $\mathcal{M}_n(\mathbb{R})$
por apenas $\mathfrak{g}$ (i.e., $\mathfrak{g} = \mathcal{M}_n(\mathbb{R})$) e denotaremos por
$I(\mathfrak{g})$ o ideal descrito acima em ($\ast$).

Dada a transformação linear $\Delta': \mathfrak{g} \rightarrow U(\mathfrak{g} ) \otimes
 U(\mathfrak{g} )$, 
satisfazendo $x \mapsto x \otimes I_n + I_n \otimes x$,  com $I_n$ a matriz identidade, obtemos, para
todo $x \in \mathfrak{g} $, que
\begin{align*}
((Id \otimes \Delta')\circ \Delta')(x) &= (Id \otimes \Delta') (x \otimes I_n + I_n \otimes x)\\
								  &= x \otimes \Delta'(I_n) + I_n \otimes \Delta'(x)\\ 
								  &= 2 x \otimes I_n \otimes I_n + I_n \otimes x \otimes I_n + I_n \otimes I_n \otimes x.
\end{align*}
Note que $[x, I_n] = 0$, pois $[x, I_n] = x \cdot I_n - I_n \cdot x$. Logo, decorre de
\[ (x \otimes I_n - I_n \otimes x - [x, I_n]) + I( \mathfrak{g} ) = I( \mathfrak{g} ) \]
que $(x \otimes I_n ) + I( \mathfrak{g} ) = (I_n \otimes x ) + I( \mathfrak{g} )$ e,
portanto, na álgebra universal envelopante podemos escrever $x \otimes I_n \otimes I_n = I_n \otimes I_n \otimes x$ e
$ I_n \otimes I_n \otimes x = x \otimes I_n \otimes I_n$. Ou seja,
\[((Id \otimes \Delta') \circ \Delta')(x)=  2 I_n \otimes I_n \otimes x + I_n \otimes x \otimes I_n + x \otimes I_n \otimes I_n.\]
Todavia, temos que
\begin{align*}
(( \Delta' \otimes Id) \circ \Delta')(x) &= (\Delta' \otimes Id) (x \otimes I_n + I_n \otimes x)\\
								  &= \Delta'(x) \otimes I_n + \Delta(I_n) \otimes x\\
								  &=  x \otimes I_n \otimes I_n + I_n \otimes x \otimes I_n +  2 I_n \otimes I_n \otimes x.
\end{align*}

Sendo assim, a transformação linear $\Delta'$ satisfaz a condição $(Id \otimes \Delta') \circ \Delta' =
(\Delta' \otimes Id) \circ \Delta' $ para todo $x \in \mathfrak{g}$.
Agora, pela propriedade universal da álgebra tensorial, Proposição~\ref{propUniv}, existe um homeomorfismo
de álgebras $\Delta'': T( \mathfrak{g} ) \rightarrow U( \mathfrak{g} ) \otimes
 U( \mathfrak{g} ) $ tal que
$ \Delta'(x) = \Delta''(x)$, para todo $ x \in \mathfrak{g} $. Logo, obtemos o operador
$\Delta: U( \mathfrak{g} ) \rightarrow U( \mathfrak{g} ) \otimes
 U( \mathfrak{g} )$ dado por
\[ \Delta( x + I( \mathfrak{g} ) ) = \Delta''(x) \]
que satisfaz a restrição $(Id \otimes \Delta) \circ \Delta =(\Delta \otimes Id) \circ \Delta $.
Portanto, $\Delta$ é um coproduto da álgebra universal envelopante das matrizes quadradas.

Finalmente, para definirmos a counidade observamos que a transformação linear \mbox{$\varepsilon': 
\mathfrak{g} \rightarrow \mathbb{R} $} dada por $ x \mapsto 0$, para toda matriz 
quadrada $x$, estabelece, devido a propriedade universal da
álgebra tensorial, um homeomorfismo de álgebras
\mbox{$\varepsilon: U( \mathfrak{g} ) \rightarrow \mathbb{R}$} tal que, para toda matriz $x$, vale $\varepsilon(x) = \varepsilon'(x)$
e $\varepsilon(1) = 1$. Resta, mostrarmos
que a propriedade de counidade é preservada por $\varepsilon$. 

Note que, para todo $x$ 
diferente dos escalares, vale:
\begin{align*}
		((Id \otimes \varepsilon) \circ  \Delta) (x)  &= (Id \otimes \varepsilon) (x \otimes I_n + I_n \otimes x)\\
												&= x \otimes 0 + I_n \otimes 0\\
												& \cong 0
\end{align*}
e
\begin{align*}
		((\varepsilon \otimes Id) \circ \Delta ) (x)  &= (\varepsilon \otimes Id) (x \otimes I_n + I_n \otimes x)\\
												&= 0 \otimes I_n + 0 \otimes x\\
												& \cong 0.
\end{align*}
Logo, para todo elemento da álgebra universal das matrizes diferente dos reais vale a igualdade:
$ (Id \otimes \varepsilon) \circ \Delta = (\varepsilon \otimes Id) \circ \Delta$. De forma semelhante
mostramos que esta igualdade também é valida para os reais. Logo, a terna 
$( U(\mathfrak{g} ), \Delta, \varepsilon)$ é uma coálgebra.

Em posse destes resultados podemos definir o conceito de biálgebra.
\begin{definicao} \index{biálgebra}
Seja $H$ um espaço vetorial. Dizemos que $H$ é uma biálgebra se o mesmo possuir estrutura de álgebra
$(H, M, u)$, estrutura de coálgebra $(H, \Delta, \varepsilon)$ e os operadores de comultiplicação
e counidade forem homeomorfismos de álgebras.
\end{definicao}

Decorre desta definição que a algebra universal envelopante das matrizes quadradas é uma biálgebra,
pois a mesma é uma álgebra, uma coálgebra e a comultiplicação e counidade são homeomorfismo de álgebras.
% \\endfold Bialgebras

% \\endfold Cap Algebras

%%% TODO Matrizes estocasticas
% \beginfold Cap Grupos
\chapter{Matrizes Estoc\'asticas}
%A principal motivação desta dissertação consiste em obter meios de se obter cadeias de markov a tempo disecreto
%através de cadeias demarkov a tempo contínuo.

Cadeias de Markov a tempo discreto $(X_n)$  e a tempo contínuo
$(X_t, t \geq 0)$ com espaço de estados $\Omega$ podem ser caracterizadas por meio de suas
\textit{matrizes de transição} $M= (M_{ij})_{i,j \in \Omega}$ e por meio de suas matrizes geradoras
 $L = (L_{ij})_{i,j \in \Omega}$,
respectivamente. Estas matrizes devem satisfazer as seguintes restrições \index{matriz!de transição ou estocástica}
\begin{enumerate}[(i)]
    \item $M_{ij} \geq 0$ para todo $i, j \in \Omega$ e $\sum_j M_{ij} = 1$ para todo $i \in \Omega$.
    Tais matrizes são chamadas de \textit{matrizes de transição} ou \textit{matrizes estocásticas};\index{matriz!estocástica}
    \item $L_{ii} \leq 0$ para todo $i \in \Omega$ e $L_{ij} \geq 0$ para todos elementos $i \neq j$ no espaço
    de estado. Além do mais, deve valer a igualdade
    $\sum_j L_{ij} = 0$ para todo $j \in \Omega$. Matrizes que satisfazem estas condições são chamadas 
    de \textit{matrizes geradoras} ou \textit{L-matrizes}.\index{matriz!geradora ou L-matriz}
\end{enumerate}

Neste capítulo iremos estudar uma forma
de gerarmos cadeias de Markov a tempo discreto por meio de L-matrizes, e vice-versa. Por exemplo, dada uma L-matriz, que caracteriza uma 
cadeia de Markov a tempo contínuo, estaremos interessados em obter uma matriz de transição $M$ de um 
processo de Markov a tempo discreto, como por exemplo a cadeia dada na Figura~\ref{fig:Markov}.
\begin{figure}[h]
\centering
\begin{tikzpicture}[->, >=stealth', auto, semithick, node distance=3cm]
\tikzstyle{every state}=[fill=white,draw=black,thick,text=black,scale=1]
\node[state]    (1)                     {$1$};
\node[state]    (2)[above right of=1]   {$2$};
\node[state]    (3)[below right of=2]   {$3$};
\path
(1) edge[loop left]           node{$0,25$}    (1)
    edge[bend left]           node{$0,75$}    (2)
    edge[bend left,below, near end]     node{$0,25$}    (3)
(2) edge[loop above]          node{$0,6$}     (2)
    edge[near start]          node{$0,4$}     (1)
(3) edge[loop right]          node{$0,1$}     (3)
    edge[bend right,right]    node{$0,2$}     (2)
    edge[bend left,above]     node{$0,7$}     (1);
\end{tikzpicture}
\caption{Exemplo de uma cadeia de Markov a tempo discreto com espaço de estados $\Omega = \{1, 2, 3\}$.}
\label{fig:Markov}
\end{figure}

Acontece que, ao estudarmos o grupo de matrizes de Lie, dado pelas matrizes estocásticas,
 percebemos existir uma relação com seu espaço tangente, que 
contém as L-matrizes desejadas. 
Neste capítulo estudaremos tais relações, o qual foram
estudadas em \cite{paper1} para o caso de matrizes quadradas de dimensão $2$.

%Nesta capítulo exibiremos os resultados obtidos por nós ao estudarmos
%os grupos das matrizes de Lie $\mathcal{G}$ das matrizes estocásticas  $n \times n$ invertíveis. 
Nosso resultado principal foi generalizar o obtido em \cite{paper1} para 
matrizes estocásticas de dimensão qualquer, o qual provamos que a componente
conexa da identidade $\mathcal{G}^0$ em relação ao grupo $\mathcal{G}$ pode ser decomposta da seguinte
forma
\begin{equation} \tag{$\ast$}
\mathcal{G}^0 = \bigcup_{t \in \mathbb{R}} e^{Q_0}t \mathcal{H},
\end{equation}
com $Q_0$ uma L-matriz e $\mathcal{H}$ o subgrupo de $\mathcal{G}$ formado pelas matrizes de determinante um.

Para obtermos esta decomposição
provamos o seguinte resultado (cuja demonstração não encontramos na literatura e que
pode ser vista no Anexo A), o qual afirmamos que
a componente
conexa $\mathcal{G}^0 \leq \mathcal{G}$ nada mais é que o subconjunto de $\mathcal{G}$ formado pelas matrizes
estocásticas de determinante positivo. Tal afirmação estende o proposto em \cite{paper1}, o qual
 provamos sua validade para grupos estocásticos de matrizes de dimensão qualquer. Além do mais, esta
afirmação é fundamental para obtermos a decomposição $(\ast)$.

 Para encerrar
este capítulo calculamos uma L-matriz a partir do espaço tangente do grupo das matrizes de
Lie referente as matrizes estocásticas $3 \times 3$. Para isto, utilizaremos
como ferramenta, o operador coproduto da álgebra universal envelopante, como proposto
em \cite{paper2}.

%% O grupo das matrizes estocasticas
% \beginfold  O grupo das matrizes estocastica
\section{O grupo das Matrizes Estoc\'asticas e seu Espa\c{c}o Tangente}
Dado o vetor $\mathbf{1} = (1, 1, \ldots, 1)$, cujas coordenadas
valem $1$, temos que uma matriz $A  = (a_{ij})_{n \times n}  $ é estocástica se,
e somente se, a condição abaixo for satisfeita
\[ A \cdot \mathbf{1} = \mathbf{1} \qquad \text{e} \qquad 0 \leq a_{ij} \leq 1, \; \forall i,j \in \{1, 2, \ldots, n \}. \]

Logo, da definição de matrizes estocásticas, definimos o seguinte conjunto:
\[ \mathcal{G} = \left\{ A \in GL(n, \mathbb{R}); \; A \cdot \mathbf{1} = \mathbf{1} \right\}. \]
O conjunto $\mathcal{G}$, definido desta forma, é um subgrupo do grupo linear geral, pois:
%Para comprovar que $\mathcal{G}$ é um subgrupo de $GL(n, \mathbb{R})$, note que
\begin{itemize}
	\item $I_n \cdot \mathbf{1} = \mathbf{1}$, imediato;
	\item Para todas matrizes $A, B \in \mathcal{G}$, temos:
		$(AB)\cdot \mathbf{1} = A(B \cdot\mathbf{1}) = A \cdot \mathbf{1} = \mathbf{1}.$ Logo, $AB \cdot \mathbf{1} = \mathbf{1}$,
        e concluímos que $AB \in \mathcal{G}$;
	\item Dado $A \in \mathcal{G}$, vale
		\[ A^{-1} \cdot \mathbf{1} = A^{-1} \cdot (A \cdot \mathbf{1}) = (A^{-1} \cdot A) \cdot \mathbf{1} = \mathbf{1}, \]
		portanto, $A^{-1} \in \mathcal{G}$.
\end{itemize}
O subgrupo $\mathcal{G}$ será chamado de grupo das matrizes estocásticas.\index{grupo!das matrizes estocásticas}

Mostremos que $\mathcal{G}$ é um subconjunto fechado de $GL(n, \mathbb{R})$, ou seja, $\mathcal{G}$ é um grupo de matrizes de Lie. 
Para isto, tome a seguinte função
\begin{align*}
 \phi : GL(n, \mathbb{R}) &\rightarrow \mathbb{R}^n\\
	A & \mapsto A \cdot \mathbf{1}.
\end{align*}
Dadas $A, B \in GL(n, \mathbb{R} )$ matrizes quaisquer, esta função satisfaz a condição de que  
\[ \| \phi(A) - \phi(B) \| = \| (A - B) \cdot \mathbf{1} \| \leq \sqrt{n} \cdot \| A - B \|, \]
pois $\|\mathbf{1}\| = \sqrt{n}$. Consequentemente,
esta desigualdade significa que a função $\phi$ é uma função lipzchitziana e, portanto, $\phi$ é uma função contínua. 
Finalmente, observe que
\[ \mathcal{G} =  \phi^{-1}( \{ \mathbf{1} \} ). \]
Como o conjunto $\{ \mathbf{1} \}$ é fechado em $\mathbb{R}^n$ e $\phi$ é uma função contínua, 
concluímos que $\mathcal{G}$ é um subconjunto fechado do grupo
linear geral e, desta forma, $\mathcal{G}$ é um grupo de matrizes de Lie.

Estudemos agora o espaço tangente de $\mathcal{G}$. Dado um caminho diferenciável 
\[A: [0,1] \rightarrow \mathcal{G}, \text{ com } A(0)= I_n,\]
temos, pela restrição $A(t) \cdot \mathbf{1} = \mathbf{1}$,
para todo $t \in [0,1]$, que
\[ A'(t) \cdot \mathbf{1} = \mathbf{0}. \]
Logo, concluímos que $A'(0) \cdot \mathbf{1} = \mathbf{0}$ e, portanto, %o espaço tangente de $\mathcal{G}$ será dado por
a seguinte inclusão é verdadeira
\[ T_1(\mathcal{G}) \subseteq \left\{ X \in \mathcal{M}_n( \mathbb{R} ); \; X \cdot \mathbf{1} = \mathbf{0} \right\}. \]
Agora, devido a Propriedade~\ref{expEig}, seção \ref{sec:liegroup},
 sobre a exponencial de matrizes, o qual diz que, dado uma matriz $A$, de autovalor $\lambda$
e autovetor $v$, a seguinte igualdade é válida
\[
e^{t A}v = e^{t \lambda}v, 
\]
para $t$ um real qualquer,  obtemos, para toda matriz $A \in 
\left\{ X \in \mathcal{M}_n( \mathbb{R} ); \; X \cdot \mathbf{1} = \mathbf{0} \right\}$ (o qual 
admitem o vetor $\bm{1}$ como autovetor e $0$ como autovalor), a igualdade:
\[
e^{tA} \bm{1} = e^{t 0} \bm{1} = \bm{1}.
\]
Desta forma, para a
função $f(t) = e^{tA}$ vale a relação:
$f(t) = e^{tA} \in \mathcal{G}$, para todo $t \in [0,1]$. Consequentemente,
\[
f(0) = \bm{I}_n \qquad \text{ e } \qquad f'(0) = A \in T_1(\mathcal{G}).
\]
Sendo assim, concluímos que
\[ \left\{ X \in \mathcal{M}_n( \mathbb{R} ); \; X \cdot \mathbf{1} = \mathbf{0} \right\} \subseteq T_1(\mathcal{G}) \]
e, portanto, obtemos a igualdade desejada
\[ T_1(\mathcal{G}) = \left\{ X \in \mathcal{M}_n( \mathbb{R} ); \; X \cdot \mathbf{1} = \mathbf{0} \right\}. \]
Em outras palavras, toda matriz $X \in T_1(\mathcal{G})$ pode ser escrita da seguinte forma
\[
X = 
 \begin{pmatrix}
  x_{1,1} & x_{1,2} & \cdots & x_{1,n} \\
  x_{2,1} & x_{2,2} & \cdots & x_{2,n} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{n,1} & x_{n,2} & \cdots & x_{n,n} 
 \end{pmatrix}
, 
\text{ com  } \sum_{j = 1}^n x_{ij} = 0, \, \forall i \in \{1, 2, \ldots, n\}
\]
Portanto, da restrição $\sum_{j = 1}^n x_{ij} = 0$ obtemos para cada linha $i$ que $x_{in} = - \sum_{j = 1}^{n-1} x_{ij}$.
Assim, definindo as matrizes elementares $E_{ij}$ como sendo as matrizes com entrada $1$ na posição $(i,j)$ e $0$ no resto,
obtemos
\[ X = \sum_{ 
	\substack{ i \in \{1,\ldots,n\} \\
		   j \in \{1,\ldots, n-1\} } 
            } 
         x_{ij} \cdot ( E_{ij} - E_{in} ). \]
Desta forma, concluímos que a dimensão do espaço tangente será igual a
\[\dim_{\mathbb{R}} T_1(\mathcal{G}) = n(n - 1).\]
% \endfold

%%%% A exponencial de matrizes
%% \beginfold A exponencial de matrizes
%\section{A exponencial de matrizes}
%A função exponencial de uma matriz quadrada $A \in \mathcal{M}_n( \mathbb{R} ) $ é dada por
%\begin{align*}
%\exp: \mathcal{M}_n( \mathbb{R} ) & \rightarrow \mathcal{M}_n( \mathbb{R} )\\
%A & \mapsto \sum_{i=0}^{\infty} \frac{A^i}{i!}, \index{exponencial de uma matriz}
%\end{align*} 
%com $A^0 = I_n$. Observe que a série acima é absolutamente convergente pois, como para 
%todo $A \in \mathcal{M}_n( \mathbb{R} )$ vale $\|A\| < \infty$,
%temos que a norma da série é dada  pela seguinte desigualdade
%\begin{align*}
% \left\|\sum_{i=0}^{\infty} \frac{A^i}{i!}\right\| &\leq \sum_{i=0}^{\infty} \frac{\|A^i\|}{i!}\\
%					& = \sum_{i=0}^{\infty} \frac{\|A\|^i}{i!}\\
%					& = e^{\|A\|}.
%\end{align*}
%Decorre da série ser absolutamente convergente que a função exponencial está bem definida.
%Dada uma matriz $A \in \mathcal{M}_n( \mathbb{R} )$ qualquer, denotaremos, para simplificar a notação, por $e^A$
%como sendo a matriz $\exp(A)$, i.e., $e^A := \exp(A)$.
%\begin{prop}
%Algumas propriedades da exponencial são listadas abaixo. Dadas as matrizes $A, B \in \mathcal{M}_n( \mathbf{R} )$, vale
%\begin{enumerate}[P1)]
%	\item $e^{\mathbf{0}} = I_n$;
%	\item Se $AB = BA$, então $ e^{A + B} = e^A e^B$;
%	\item $e^{BAB^{-1}} = B e^{A} B^{-1}$; 
%	\item $ \det( \,e^{A} \, ) = e^{tr(A)}$. \label{detTr}
%\end{enumerate}
%\end{prop}
%
%\begin{proof}
%vide~\cite{stillwell}
%\end{proof}
%
%A partir da função exponencial podemos relacionar grupos de matrizes de Lie e seus espaços tangentes. 
%Estas relações são dadas pelos seguintes
%teoremas (cujas demonstrações podem ser encontradas em~\cite[capítulo 7, p.~143 e 149]{stillwell} ):
%\begin{teo}\label{imagem_tg}
%Seja $G$ um grupo de matrizes de Lie e $T_1(G)$ seu espaço tangente. Então, a imagem do espaço
%tangente pela função exponencial é um subconjunto de $G$. Isto é:
%\[ \exp( T_1(G) ) \subseteq G .\]
%\end{teo}
%\begin{teo}\label{bijecao_exp}
%Seja $G$ um grupo de matrizes de Lie e $T_1(G)$ seu espaço tangente. 
%Então, existe um número real, $r > 0$, suficientemente pequeno
%e um aberto $U$ de $T_1(G)$ contendo a matriz $\mathbf{0} \in T_1(G)$ tal que
%%a bola $B(I_n, r)$, de centro na identidade e raio $r > 0$, tal que 
%\[ \exp: U \subseteq T_1(G) \rightarrow B(I_n, r) \subseteq G\]
%é uma bijeção.
%\end{teo}
%
%% \endfold
%
%%% Propriedades topologocias e algebricas
% \beginfold Propriedades topologocias e algebrica
%%%\section{Propriedades Topol\'ogicas e Alg\'ebricas \textcolor{red}{(excluir esta secao)}}
%%%%Por uma questão de notação, daqui em diante, denotaremos por $\mathcal{G}_G^0$ como sendo a componente conexa 
%%%%da identidade em relação ao grupo de matrizes de Lie G. Quando tivermos
%%%%apenas $\mathcal{G}^0$, estaremos nos referindo a componente conexa da identidade em relação ao grupo linear geral.
%%%
%%%Pelo Teorema~\ref{propriedades_G0}, sabemos que a componente conexa $\mathcal{G}^0$ é um subgrupo normal, conexo e fechado de
%%%$\mathcal{G}$.
%%%Ainda mais,
%%%como $\mathcal{G}$ é fechado em $GL(n, \mathbb{R})$, temos que $\mathcal{G}^0$ é um grupo de matrizes de Lie. 
%%%Uma propriedade importante sobre $\mathcal{G}^0$ é a seguinte:
%%%\[\mathcal{G}^0 = \left\{ e^Q; \; Q \in T_1(\mathcal{G}) \right\}.\]
%%%Porém, antes de provarmos esta afirmação, provemos o seguinte Lema:
%%%\begin{lema}\label{gerador}
%%%Seja $G$ um grupo topológico conexo e $U$ um aberto contendo a identidade $\mathbf{e} \in G$. 
%%%Então, todo elemento de $G$ pode ser escrito como produto de elementos
%%%de $U$. \textcolor{red}{resultado usado apenas pelo teorema seguinte, logo desnecessário}
%%%\end{lema}
%%%
%%%\begin{proof}
%%%Observe que $i: G \rightarrow G$, dada por $i(x) = x^{-1}$, é um homeomorfismo. 
%%%Portanto, $i(U) =: U^{-1}$ é um aberto contendo $\mathbf{e}$. Logo, 
%%%\[ \emptyset \neq U \cap U^{-1} \subset U. \]
%%%Agora tome o conjunto
%%%\[ S = \left\{ g_1 \cdots g_n;\; g_1,\ldots, g_n \in U \cap U^{-1} \text{ e } n \in \mathbf{N} \right\}. \]
%%%Mostremos que $S = G$. 
%%%Para isso, devemos mostrar que o conjunto $S$ é aberto e fechado em $G$ que, por sua vez, como $G$ é conexo, isto equivale a 
%%%dizer que $S= G$. Dado $g \in S$ qualquer, temos que
%%%\[ g(U \cap U^{-1}) := \left\{ g \cdot h; \; h \in U \cap U^{-1} \right\} \subseteq S. \]
%%%Como já observado, a função $L_g: G \rightarrow G$ dada por $L_g(x) = g \cdot x$ é um homeomorfismo.
%%%Logo, $L_g(U \cap U^{-1}) = g(U \cap U^{-1})$ é um
%%%aberto em $G$ e, portanto, concluímos que para todo $g \in S$ existe um aberto $V$ em $G$ tal
%%% que $g \in V \subseteq S$, i.e.,  $S$ é um aberto em G.
%%%
%%%Agora, suponhamos, por absurdo, que $S$ não é um fechado em $G$. 
%%%Logo, $\overline{S} \setminus S \neq \emptyset$. Tomando $g \in \overline{S} \setminus S$,
%%%temos que $g(U \cap U^{-1})$ é uma vizinhança aberta de g e,
%%%como $g$ é ponto de aderência de S, obtemos $g(U \cap U^{-1}) \cap S \neq \emptyset$. Finalmente, tomemos um ponto qualquer $w$ nesta
%%%intersecção, i.e., $w \in g(U \cap U^{-1}) \cap S$. Este ponto pode ser escrito como $w = g \cdot u$, com $u \in U \cap U^{-1}$. Logo,
%%%\[ w \in S \implies w \cdot u^{-1} \in S \implies g \cdot u \cdot u^{-1} \in S, \]
%%%ou seja, $g \in S$. Porém, isto contradiz a hipótese de que $g \notin S$. 
%%%Desta forma, provamos por contradição que $S$ também é fechado em $G$ e, portanto, $S = G$.
%%%\end{proof}
%%%
%%%\begin{teo}\label{g0_exp(tg)}
%%%$\mathcal{G}^0 = \left\{ e^Q; \; Q \in T_1(\mathcal{G}) \right\}$
%%%\textcolor{red}{demonstração errada - substituir}.
%%%\end{teo}
%%%
%%%\begin{proof}
%%%Sabemos, pelo Teorema~\ref{bijecao_exp}, que existe uma bola aberta de centro em $I_n$, com raio $r \in [0,1]$, e um 
%%%aberto $U$ de $T_1(\mathcal{G})$ contendo $\mathbf{0}$ tal
%%%que
%%%\begin{equation*} \label{eq:exp}
%%% \exp: U \subseteq T_1(\mathcal{G}) \rightarrow B(I_n, r) \subseteq \mathcal{G} 
%%%\end{equation*}
%%%é uma bijeção. Porém, pelo Lema~\ref{gerador}, temos que a bola $B(I_n, r)\cap \mathcal{G}^0$ gera
%%%  o grupo $\mathcal{G}^0$, pois $\mathcal{G}^0$ 
%%%é um grupo topológico conexo. Sendo assim, para todo $X \in \mathcal{G}^0$, existem as matrizes
%%%$W_1, \ldots, W_n \in B(I_n, r)\cap \mathcal{G}^0$  
%%%tal que
%%%\[X = W_1 \cdots W_n.\]
%%%Todavia, decorre de $ \| W_i \| < r < 1$, para todo $i = 1, \ldots, n$, que:
%%%\[ \|X\| \leq \|W_1\| \cdots \|W_n\| < r. \]
%%%Portanto, $X \in B(I_n, r)$. Agora, como a função exponencial é uma bijeção entre $B(I_n,r)$ e $U$, 
%%%temos que existe $Q \in U \subseteq T_1(\mathcal{G})$ tal que $e^Q = X$. Desta forma, concluímos que 
%%%$\mathcal{G}^0 \subseteq \left\{ e^Q; \; Q \in T_1(G) \right\} $.
%%%
%%%Mostremos agora a volta. Pelo Teorema~\ref{imagem_tg}, temos que
%%%\[ \exp( T_1(\mathcal{G}) ) \subseteq G.\]
%%%Fixada uma matriz $Q \in T_1(G)$, observe que a função $g: [0,1] \rightarrow G$, dada por $g(t) = e^{Qt}$, é uma função
%%%contínua, com $g(0) = I_n$ e $g(1) = e^Q$. Como o intervalo $[0,1]$ é conexo na reta real, temos que $g([0,1])$ é um conexo
%%%contendo a identidade $I_n$ e, portanto, $g([0,1]) \subseteq \mathcal{G}^0$. Sendo assim, concluímos que $e^Q \in \mathcal{G}^0$.
%%%Logo, a inclusão  $ \left\{ e^Q; \; Q \in T_1(G) \right\} \subseteq \mathcal{G}^0$ é verdadeira e o teorema está provado.
%%%\end{proof}
%%%
%%%%%%\begin{cor}
%%%%%%$\mathcal{G}^0 = \left\{ Q \in \mathcal{G}; \; \det{Q} > 0 \right\}$.
%%%%%%\end{cor}
%%%%%%\begin{proof}
%%%%%%A inclusão 
%%%%%%\[\mathcal{G}^0 \subseteq \left\{ Q \in \mathcal{G}; \; \det{Q} > 0 \right\}\]
%%%%%%é imediata pois, pelo Teorema anterior, toda matriz de $\mathcal{G}^0$ pode ser escrita como $e^P$,
%%%%%%para $P$ uma matriz de seu espaço tangente, e, portanto, como $\det(e^P) = e^{tr(P)}$, temos
%%%%%%que toda matriz da componente conexa $\mathcal{G}^0$ terá determinante positivo.
%%%%%%
%%%%%%Para provarmos a outra inclusão, mostremos que o conjunto das matrizes de $\mathcal{G}$ de determinante
%%%%%%positivo é um conjunto conexo por caminhos e, portanto, conexo contendo a identidade.
%%%%%%Para isto, dada uma matriz estocástica $X$ de determinante positivo qualquer,
%%%%%% observe que o caminho $g_X:[0,1] \rightarrow \mathcal{M}_n(mathbb{R})$ dado por
%%%%%%\[ g_X(t) = tX + (1-t)I_n \]
%%%%%%satisfaz a seguinte condição:
%%%%%%\[ g_X(t) \cdot \mathbf{1} = tX \cdot \mathbf{1} + (1-t)I_n \cdot \mathbf{1} = \mathbf{1}.\]
%%%%%%Ou seja, $g_X( [0,1] ) \subset \mathcal{G}$. Resta mostrarmos que, para todo $t \in [0,1]$,
%%%%%%temos $\det(g(t)) > 0$. Sabemos que a função $\det \circ g_X : [0,1] \rightarrow \mathbb{R}$
%%%%%%é uma função contínua. Suponhamos, por absurdo, que existe $t_0 \in (0,1)$ tal que 
%%%%%%$\det(g_X(t_0)) < 0$. Porém, como $det(I_n) > 0$ e $\det \circ g_X$ é uma função contínua,
%%%%%%decorre do teorema do valor intermediário que existe $t_1 \in (0,t_0)$ tal que $\det(g(t_1))) = 0$.
%%%%%%\end{proof}
%%%
%%%Um resultado conhecido de Álgebra, diz o seguinte: o centro do grupo linear geral,
%%% denotado por $Z(GL(n, \mathbb{R}))$, é o conjunto das matrizes dadas
%%%como o produto de um escalar pela matriz identidade, i.e.,
%%%\[ Z(GL(n, \mathbb{R})) = \left\{ \alpha \cdot I_n; \; \alpha \in \mathbb{R} \right\}. \]
%%%Em meio a este resultado, provemos que o centro da componente conexa $\mathcal{G}^0$ 
%%%é o conjunto contendo apenas a identidade:
%%%\[ Z(\mathcal{G}^0)  = \left\{  I_n \right\}. \]
%%%\begin{teo}
%%%$Z(\mathcal{G}^0)  = \left\{  I_n \right\}.$
%%%\end{teo}
%%%\begin{proof}
%%%Definindo as matrizes $E_{ij}$ como sendo as matrizes com entrada $1$ na posição $(i,j)$ e $0$ no resto. 
%%%Observe que toda matriz $(a_{ij}) = A \in Z(\mathcal{G}^0)$ deve
%%%comutar com $E_{ij}$. Suponhamos que, para os índices $i \neq j$ tenhamos $a_{ij} \neq 0$. Então,
%%%\[ (b_{ij}) = A E_{ji} \neq E_{ji} A = (c_{ij}), \]
%%%pois $b_{ii} \neq 0$ e $c_{ii} = 0$. Desta forma, para uma matriz estar no centro, todas suas entradas
%%%fora da diagonal devem ser zero. Pois bem, mostremos agora que todas as entradas da diagonal devem ter os mesmos
%%%valores. Novamente, caso $A = (a_{ij})$ seja uma matriz no centro, com $a_{ii} \neq a_{jj}$, então, tomando a matriz elementar
%%%que permuta a linha
%%%$i$ pela linha $j$ de uma matriz qualquer $n \times n$, denotada por $P$, temos que $PA$ comuta as linhas $i$ e $j$ de $A$, 
%%%enquanto que, $AP$ comuta as colunas $i$ e $j$ de $A$.
%%%Ou seja, $PA \neq AP$. Logo, uma matriz está no centro da componente conexa se for uma matriz diagonal, 
%%%cujas entradas na diagonal tem o mesmo valor.
%%%Finalmente, como $Z(\mathcal{G}^0) \subseteq \mathcal{G}$, temos que
%%%\[ A \in Z(\mathcal{G}^0) \iff A = \alpha \cdot I_n \text{ para algum } \alpha \in \mathbb{R} \text{ e  }
%%%A\cdot\mathbf{1} = \mathbf{1}. \]
%%%%Portanto, sabemos que $A \in Z(\mathcal{G}^0)$ implica $A = \alpha I_n$, para algum $\alpha \in \mathbb{R}$. 
%%%Da restrição $A \cdot\mathbf{1} = \mathbf{1}$,
%%%obtemos $\alpha \cdot I_n \cdot\mathbf{1} = \mathbf{1}$, ou seja, $\alpha = 1$. Logo, o teorema está provado.
%%%\end{proof}
%%%
%%%Em posse destes resultados, concluímos que 
%%%a componente conexa  $\mathcal{G}^0$
%%% satisfaz as seguintes propriedades:
%%%\begin{enumerate}[(i)]
%%%	\item $\mathcal{G}^0$ é um grupo de matrizes de Lie conexo;
%%%	\item $T_1(\mathcal{G}^0) = T_1(\mathcal{G})$. 
%%%    Este fato decorre do Teorema~\ref{g0_exp(tg)} pois, para toda matriz $Q \in T_1(\mathcal{G})$, vale
%%%		$e^Q \in \mathcal{G}^0$. Portanto, o caminho diferenciável $g(t) = e^{Qt}$, para todo $t \in [0,1]$, nos fornece $g(0) = I_n$
%%%		e $g'(0) = Q \in T_1(\mathcal{G}^0)$. Consequentemente, $T_1(G) \subseteq T_1(\mathcal{G}^0)$. 
%%%        A inclusão no outro sentido é imediata.  
%%%	\item O centro de $\mathcal{G}^0$ é um espaço discreto.
%%%\end{enumerate}

% \endfold

%%% Relacao entre matrizes estocasticas e matrizes geradora
% \beginfold Relacao entre matrizes estocasticas e matrizes geradora
\section{Rela\c{c}\~ao entre Matrizes Estoc\'asticas e Matrizes Geradoras }
%Devido ao fato da componente conexa $\mathcal{G}^0$ ser um grupo de matrizes de Lie
%conexo, com centro discreto e espaço tangente não nulo, temos, pelo 
%Teorema~\ref{teoAp1} abaixo, que todo subgrupo normal e não discreto de
%$\mathcal{G}^0$ terá espaço tangente não nulo.
Nesta seção mostraremos como a componente conexa da identidade, denotada por $\mathcal{G}^0$, do grupo
das matrizes estocásticas $\mathcal{G}$ se relaciona
com matrizes geradoras de um processo markoviano a tempo contínuo.
Para isto, utilizaremos o seguinte resultado:
\begin{prop}\label{det}
O conjunto das matrizes estocásticas de determinante positivo
\[
\left\{ X \in \mathcal{G}; \; \det(X) > 0 \right\}
\]
é conexo por caminhos.
\end{prop}
\begin{proof}
A demonstração desta proposição pode ser vista no Apêndice~A.
\end{proof}

Em posse deste resultado obtém-se imediatamente que
\begin{cor} \label{cor:det}
A componente conexa $\mathcal{G}^0$ do grupo das matrizes estocásticas é igual
ao conjunto das matrizes estocásticas de determinante positivo, i.e.,
\[
\mathcal{G}^0  = 
\left\{ X \in \mathcal{G}; \; \det(X) > 0 \right\}
\]
\end{cor}
\begin{proof}
Sabemos, vide~\cite{elon}, que a componente conexa do grupo linear geral 
é igual ao conjunto das matrizes de determinante positivo. Logo, 
\[
\mathcal{G}^0  \subseteq
\left\{ X \in \mathcal{G}; \; \det(X) > 0 \right\}.
\]
Além do mais, decorre da Proposição anterior que o conjunto das matrizes
estocásticas de determinante positivo é conexo por caminhos. Portanto,
a seguinte inclusão é verdadeira
\[
\left\{ X \in \mathcal{G}; \; \det(X) > 0 \right\}
\subseteq \mathcal{G}^0,
\]
e encerramos a demonstração.
\end{proof}

%\begin{teo}\label{teoAp1}
%Seja $G$ um grupo de matrizes de Lie, conexo, com centro $Z(G)$ discreto e com espaço tangente $T_1(G) \neq \{\mathbf{0}\}$
%(aqui, $\mathbf{0}$ representa a matriz cuja todas entradas valem zero). Então,
%se $\mathcal{H} \subseteq G$ for um subgrupo normal e não discreto, teremos que o
% espaço tangente do subgrupo $\mathcal{H}$ será um ideal não trivial de $T_1(G)$, i.e.,
%$T_1(\mathcal{H}) \neq \{ \mathbf{0} \}$ e, para todo $A \in T_1(G)$ 
%e para todo $B \in T_1(\mathcal{H})$, vale $[A, B] \in T_1(\mathcal{H})$.
%\end{teo}
%\begin{proof}
%Vide o Apêndice 1.
%\end{proof}

Agora, tomemos o seguinte conjunto
\[ \mathcal{H} = \{A \in \mathcal{G}; \; \det(A) = 1 \}. \]
$\mathcal{H}$ será um subgrupo normal, pois 
\begin{itemize}
	\item A identidade está em $\mathcal{H}$;
	\item $A, B \in \mathcal{H}$ implica $\det(AB) = \det(A) \cdot \det(B)$ e, portanto, $AB \in \mathcal{H}$;
	\item Para $A \in \mathcal{H}$, temos $\det(A^{-1}) = \det(A)^{-1} = 1$, ou seja, $A^{-1} \in \mathcal{H}$;
	\item E, finalmente, para $A \in \mathcal{H}$ e $B \in \mathcal{G}$, temos 
		\[ \det(BAB^{-1}) = \det(B) \cdot \det(A) \cdot \det(B)^{-1} = 1,\] 
		logo $B \mathcal{H} B^{-1} \subseteq \mathcal{H}$.
\end{itemize}
Além do mais, como toda matriz $A \in \mathcal{G}$ de determinante positivo pertence a componente
conexa $\mathcal{G}^0$ (Corolário~\ref{cor:det}), temos que
$\mathcal{H}$ é um subgrupo normal de $\mathcal{G}^0$.
%Provemos que o espaço tangente de $\mathcal{H}$ é dado por
%\[ T_1(\mathcal{H}) = \{Q \in T_1(\mathcal{G}^0); \; \tr(Q) = 0 \} \]
%que, como já visto pelo Exemplo~\ref{tr_lie}, é uma álgebra de Lie. 
%
%\begin{prop}
%\[ T_1(\mathcal{H}) = \{Q \in T_1(\mathcal{G}^0); \; \tr(Q) = 0 \}. \]
%\end{prop}
%\begin{proof}
%Primeiramente, observe que $\mathcal{H}$ é um grupo de matrizes de Lie. Isto é verdade pois
%\[\mathcal{H} = det^{-1}(\{1\}) \cap \mathcal{G}^0, \]
%e, como $det^{-1}(\{1\})$ e $\mathcal{G}^0$ são subconjuntos fechados do grupo linear geral
%obtemos que $\mathcal{H}$ é um subgrupo fechado de $GL(n, \mathbb{R})$, i.e., $\mathcal{H}$
%é um grupo de matrizes de Lie. Logo, pelo Teorema~\ref{imagem_tg}, temos que
%\[ \exp(T_1(\mathcal{H})) \subseteq \mathcal{H}. \]
%Sendo assim, para toda matriz $Q \in T_1(\mathcal{H})$ temos $e^Q \in \mathcal{H}$. Ou seja,
%pela Propriedade~\ref{detTr} da exponencial de matrizes, obtemos
%\[ \det(e^Q) = 1 \implies e^{tr(Q)} = 1 \implies tr(Q) = 0. \]
%Logo, devido ao fato de
%\begin{enumerate}[(i)]
%    \item $Q$ ter traço $0$ e
%    \item $T_1(\mathcal{H}) \subseteq T_1(\mathcal{G}^0)$, pois $\mathcal{H} \subseteq \mathcal{G}^0$,
%\end{enumerate}
%chegamos a seguinte inclusão
%\[ T_1(\mathcal{H}) \subseteq \{Q \in T_1(\mathcal{G}^0); \; \tr(Q) = 0 \}. \]
%
%Para mostrarmos a inclusão no outro sentido basta notarmos que, dada uma matriz 
%$Q \in T_1(\mathcal{G}^0)$, com $\tr(Q) = 0 $, vale $\det(e^Q) = 1$ e $e^Q \in 
%\mathcal{G}^0$ (pelo Teorema~\ref{g0_exp(tg)}), ou seja,
%$e^Q \in \mathcal{H}$. Observe também que, para todo real $t \in [0,1]$, temos
%\[ \det(e^{tQ}) = e^{t \cdot tr(Q)} = 1.\]
%Logo, temos o caminho diferenciável $g: [0,1] \rightarrow \mathcal{H}$, com $ t \mapsto e^{tQ}$, satisfazendo
%$g(0) = I_n$ e $g'(0) = Q$. Portanto, $Q \in T_1(\mathcal{H})$ e concluímos que 
%\[ \{Q \in T_1(\mathcal{G}^0); \; \tr(Q) = 0 \} \subseteq T_1( \mathcal{H}). \]
%Consequentemente, a igualdade está provada.
%\end{proof}
%%Além do mais, pela Propriedade~P4) da exponencial de matrizes e pelo Teorema~\ref{g0_exp(tg)}, 
%%temos que, para toda matriz $A \in \mathcal{G}^0$ e 
%%toda matriz $Q \in T_1(\mathcal{G}^0)$ satisfazendo $A = e^Q$, vale
%%\[ \det(A) = 1 \iff e^{tr(Q)} = 1. \]
%%Logo, as seguintes inclusões são válidas
%%\begin{enumerate}[(i)]
%%    \item $T_1(\mathcal{H} \subseteq \{Q \in T_1(\mathcal{G}^0); \; \tr(Q) = 0 \}$ pois, dada uma matriz
%%    qualquer $Q \in T_1(\mathcal{H})$, temos que $e^Q
%%\end{enumerate}
%%o espaço tangente de $\mathcal{H}$ é dado por
%%\[ T_1(\mathcal{H}) = \{Q \in T_1(\mathcal{G}^0); \; \tr(Q) = 0 \}, \]
%Observe também que $T_1(\mathcal{H})$ é um ideal de $T_1(\mathcal{G}^0)$ pois, dadas as matrizes 
%$Q \in T_1(\mathcal{H})$ e $P \in T_1(\mathcal{G}^0)$, vale
%\[ \tr( [Q, P] ) = \tr( QP - PQ ) = \tr( QP ) - \tr( QP ) = 0, \]
%ou seja, $[Q, P] \in T_1(\mathcal{H})$.

%Como $\mathcal{H}$ é um subgrupo de $\mathcal{G}^0$, podemos escrever a componente conexa da identidade em relação a G como:
%\[ \mathcal{G}^0 = \bigcup_{A \in \mathcal{G}^0}  A \mathcal{H}, \]
%com $A \mathcal{H} := \{AP; \; P \in \mathcal{H} \}$ uma coclasse. Note também que 
%\[\dim_{\mathbb{R}} ( T_1(\mathcal{H}) ) = \dim_{\mathbb{R}} ( T_1(\mathcal{G}^0) ) - 1. \]
%Em vista disso e do Teorema~\ref{g0_exp(tg)},  fixada uma matriz $Q \in T_1(\mathcal{G}^0) \setminus T_1(\mathcal{H})$, teremos que
%para toda matriz $A \in \mathcal{G}^0$ valerá a seguinte condição: ou $A$ está contida em $\exp(T_1(\mathcal{H}))$ ou
%$A$ é da forma $e^{tQ}$ para algum $t$ real. Logo, a decomposição da componente conexa em classes de equivalências
%pode ser reescrita como
%\[ \mathcal{G}^0 = \bigcup_{t \in \mathbb{R}}  e^{Qt} \mathcal{H}. \]
%
Em posse destes resultados somos capazes de demonstrar o Teorema:
\begin{teo}
Seja $\mathcal{G}$ o  grupo das matrizes estocásticas e $\mathcal{G}^0$ a componente conexa da identidade.
Então, existe uma matriz geradora de uma cadeia de Markov a tempo contínuo (equivalentemente, uma L-matriz)
 $Q_0$ tal que a seguinte decomposição é válida:
\[
\mathcal{G}^0 = \bigcup_{t \in \mathbb{R}}  e^{Q_0t} \mathcal{H}.
\]
\end{teo}
\begin{proof}
Para realizar a demonstração, utilizemos a ideia proposta em~\cite{paper1}.
Dado o grupo $\mathbb{R}_{> 0}$ (dos números reais estritamente positivos) munido do operador multiplicação, e cuja
 identidade é dada pelo  real $1$.
Existe um homomorfismo entre os grupos $\mathcal{G}^0$ e $\mathbb{R}_{> 0}$, dado por
\begin{align*}
\psi: \mathcal{G} & \rightarrow \mathbb{R}_{> 0}\\
X & \mapsto \det( X ).
\end{align*}
Segue do Teorema do Isomorfismo, da teoria dos Grupos Clássicos (vide Teorema \ref{teo:isomorfismo}, Seção \ref{sec:grupos}),	
que o quociente de $\mathcal{G}^0$ pelo núcleo de $\psi$ é isomorfo a imagem de $\psi$, i.e.,
$\mathcal{G}^0/ \ker(\psi) \cong Im( \psi )$. Porém, segue do fato de
que $\im( \psi ) = \mathbb{R}_{> 0}$ (Corolário \ref{cor:det}), e do fato de que $\ker(\psi) = \mathcal{H}$, a seguinte
relação 
\[
\mathcal{G}^0/ \mathcal{H} \cong \mathbb{R}_{> 0}.
\]
Podemos, também, escrever a componente conexa como união de coclasses
\[
\mathcal{G}^0 = \bigcup_{Q \in \mathcal{G}^0}  Q\mathcal{H}.
\]
Finalmente, fixemos uma matriz $Q_0 \in T_1(\mathcal{G})$ com traço diferente de zero.
Sabemos que $e^{Q_0} \in \mathcal{G}$ e $\det(e^{Q_0}) = e^{\tr(Q_0)} > 0$,
ou seja, $e^{Q_0} \in \mathcal{G}^0$. Ademais, dadas duas matrizes $M, P
\in \mathcal{G}^0$, temos que suas coclasses serão iguais quando
\[
M\mathcal{H} = P \mathcal{H} \iff \det(M) = \det( P),
\]
devido ao homomorfismo $\psi$.
Consequentemente, dada uma matriz $Q \in \mathcal{G}^0$ qualquer, tomando o real 
$t_0 = \ln( \det(Q) ) / \tr( Q_0 )$, obtemos
\[
\det( e^{t_0 Q_0 }) = e^{t_0 \tr( Q_0 )} = \det(Q),
\]
ou seja, $Q\mathcal{H} = e^{(t_0 Q_0)} \mathcal{H}$ e, portanto, a igualdade abaixo
é verdadeira
\[
\mathcal{G}^0 = \bigcup_{t \in \mathbb{R}}  e^{Q_0t} \mathcal{H}.
\]
\end{proof}
%Observe que, dado uma L-matriz qualquer não nula, denotemos esta matriz por L, temos que L satisfaz a condição 
%$L \in T_1(\mathcal{G}^0) \setminus T_1(\mathcal{H})$, ou seja, a igualdade acima pode ser vista pelo 
%seguinte resultado: 
\begin{cor}\label{main}
Para toda matriz M, pertencente ao grupo das matrizes estocásticas, e de determinante positivo,
 existe um real $t_0 \in \mathbb{R}$, e uma matriz $P \in \mathcal{G}^0$
de determinante igual a um, tal que 
\begin{equation*} \label{mainEq}M = e^{Q_0 \cdot t_0} e^{P},\end{equation*}
onde $Q_0$ é uma L-matriz. Ou seja, através de uma matriz geradora de uma cadeia de Markov a tempo contínuo
podemos gerar uma matriz de transição de uma cadeia de Markov a tempo discreto.
\end{cor}
\begin{proof}
Segue imediatamente do Teorema acima.
\end{proof}

\subsection{Aplica\c{c}\~ao do Teorema 5.3}
Para ilustrar o Corolário~\ref{main}, iremos gerar, por meio de simulações computacionais, matrizes de transição a partir de 
matrizes geradoras $L, L_1$ e $L_2$, utilizando a igualdade $M = e^{L \cdot t_0} e^P$, onde $t_0$ é um real e $P$
uma matriz de traço zero com linhas que também somam zero.

\begin{exmp}
Sejam as matrizes geradoras
\[
L_1 = \frac{1}{3}
\begin{pmatrix}
-1 & 1 & 0\\
0 & -1 & 1\\
0 & 1 & -1
\end{pmatrix}
e \;\;
L_2 = 
\begin{pmatrix}
-0,2244& 0,0719& 1,1525\\
0,0309& -0,1419& 0,1110\\
0,1490& 0,2695& -0,4186
\end{pmatrix}
.
\]
Obtemos os seguintes resultados:
\begin{enumerate}[(i)]
\item Para a matriz geradora $L_1$, consideremos o real $t_0= 4,3329$ e a matriz
\[
P = 
\begin{pmatrix}
0,1070&  0,3447& -0,4517\\
0,0462&  0,1351& -0,1813\\
0,9554&  -0,7133& -0,2421
\end{pmatrix}
.
\]
Logo, realizando a transformação $M = e^{Lt_0} e^P$, obtemos a matriz de transição
\[
M = 
\begin{pmatrix}
0,4415&  0,5543& 0,0042\\
0,3771&  0,4091& 0,2138\\
0,4257&  0,3147& 0,2596
\end{pmatrix}
.
\]
\item Agora, para a matriz geradora $L_2$, se considerarmos o real $t_0= 2,5490$ e a matriz
\[
P = 
\begin{pmatrix}
0,0660& 0,0392& -0,1053\\
0,1219& 0,0189& -0,1408\\
0,0957& -0,0107& -0,0850
\end{pmatrix}
\]
obtemos a seguinte matriz de transição
\[
M = 
\begin{pmatrix}
0,6911 &0,2205 &0,0884\\
0,1928 &0,7799 &0,0274\\
0,2932 &0,3957 &0,3111

\end{pmatrix}
.
\]
\end{enumerate}
\end{exmp}

Para encerrarmos nossos exemplos, calculemos a matriz de transição a partir
de matrizes geradoras $4 \times 4$.
\begin{exmp}
Seja a matriz geradora 
\[
L =
\begin{pmatrix}
-2,0403& 0,5717 & 0,7233 & 0,7453\\
 0,9234& -1,4091& 0,1230 & 0,3627\\
 0,5302& 0,4849 & -1,6914& 0,6763\\
 0,9791& 0,3013 & 0,1422 & -1,4226
\end{pmatrix}
.
\]

Tomando a constante como $t_0 = 4,5445$ e a matriz $P$ como
\[
P =
\begin{pmatrix}
0,5348 & 0,6335 & 0,8027& -1,9710\\
-0,8745& 0,1085 & 0,2200& 0,5461\\
0,4856 & -1,3721& 0,0726& 0,8139\\
-0,6206& 0,7202 & 0,6163& -0,7159
\end{pmatrix}
\]
obtemos a matriz de transição
\[
M =
\begin{pmatrix}
0,3656& 0,0374& 0,5579& 0,0391\\
0,3653& 0,0377& 0,5578& 0,0392\\
0,3657& 0,0373& 0,5580& 0,0391\\
0,3657& 0,0373& 0,5579& 0,0391
\end{pmatrix}
.
\]
\end{exmp}
%\endfold

\section{Constru\'indo uma Matriz Estoc\'astica a partir do Espa\c{c}o Tangente} \label{sec:matrizL}
Dado o espaço tangente $T_1(\mathcal{G})$ do grupo das matrizes estocásticas calculado
na seção~5.1, estamos interessados em obter uma matriz geradora de um processo de Markov
a tempo contínuo através desta álgebra. Isto é possível se realizarmos os seguintes procedimentos:
\begin{enumerate}[(i)]
    \item Calcularmos o elemento do Casimir da álgebra de Lie $T_1(\mathcal{G})$, i.e., 
		  devemos achar um elemento no centro da álgebra $T_1(\mathcal{G})$;
    \item Definirmos uma coálgebra em $T_1(\mathcal{G})$ tal que tenhamos uma estrutura de biálgebra;
    \item Calcularmos o coproduto do elemento Casimir.
\end{enumerate}
O resultado obtido do coproduto do Casimir nos dará, finalmente, uma matriz geradora.
Para isto, trabalharemos com o espaço tangente do grupo das matrizes $3 \times 3$ com entradas nos reais:
\[
T_1(\mathcal{G}) =
\begin{pmatrix}
-x_{12} -x_{13}& x_{12}& x_{13}\\
x_{21}& -x_{21} -x_{23}& x_{23}\\
x_{31}& x_{32}& -x_{31} -x_{32}\\
\end{pmatrix}
\]

Todavia, ocorre que nossa álgebra de Lie $T_1(\mathcal{G})$ não é semi-simples e, desta
forma, não possuímos uma fórmula fechada para calcular o elemento Casimir da mesma. Porém, devido ao
fato de $T_1(\mathcal{G})$, também, não ser uma álgebra solúvel, temos garantido pelo
Teorema de Levi, descrito abaixo e cuja demonstração pode ser vista em~\cite{algebra},
a existência  de uma subálgebra semi-simples $\mathfrak{s}$ de $T_1(\mathcal{G})$. 

Sendo assim, ao invés de trabalharmos com o espaço tangente $T_1(G)$ iremos trabalhar com sua
subálgebra semi-simples $\mathfrak{s}$, pois a mesma, por ser uma álgebra semi-simples, 
terá uma fórmula fechada para o cálculo de seu elemento do Casimir,
facilitando nosso trabalho.
\begin{teo}[Teorema de Levi]
Seja $\mathfrak{g}$ uma álgebra de Lie de dimensão finita e $\mathfrak{r}(\mathfrak{g})$ o seu radical
($\mathfrak{r}(\mathfrak{g})$ é igual ao maior ideal solúvel da álgebra $\mathfrak{g}$).
Então, existe uma subálgebra semi-simples $\mathfrak{s}$ o qual $\mathfrak{g}$ se decompõe como soma
direta da subálgebra $\mathfrak{s}$ e do radical $\mathfrak{r}(\mathfrak{g})$. 
\end{teo}

Com o auxílio computacional, utilizando o software~\cite{octave}
 \footnote{O código do programa utilizado para obter a álgebra semi-simples $\mathfrak{s}$ pode ser
visto no Apêndice~B}, obtivemos a seguinte subálgebra
semi-simples do espaço tangente $T_1(\mathcal{G})$:
\[ \mathfrak{s} = \spn \left\{
X_1:=
\begin{pmatrix}
0 & -1 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{pmatrix}
,
X_2:=
\begin{pmatrix}
1 & -1 & 0 \\
1 & -1& 0 \\
0 & 0 & 0 
\end{pmatrix}
,
X_3:=
\begin{pmatrix}
1 & -1 & 0 \\
0 & -1& 1 \\
0 & 0 & 0 
\end{pmatrix}
.
\right\}
\]

\begin{obs*}
A álgebra de Lie $\mathfrak{s}$ é isomórfica a álgebra $sl_2(\mathbb{R})$. O isomorfismo decorre do 
fato de $\mathfrak{s}$ admitir a base
\[
e = X_2, \quad h = -(X_1 + X_3), \quad f = -X_1,
\]
cujas constantes estruturais valem
\[
[e,f] = h, \quad [h,f] = -2f, \quad [h,e] = 2e
\]
que são iguais as constantes estruturais de $sl_2(\mathbb{R})$, ou seja,
$\mathfrak{s} \cong sl_2(\mathbb{R})$.
\end{obs*}

O elemento do Casimir da álgebra semi-simples $\mathfrak{s}$ é dado, então,  pela fórmula (vide~\cite{algebra}, pg. 95):
\[
C = \sum_{i=1}^3 X_i Y_i,
\]
o qual $Y_i$ representa a base dual da base $(X_1, X_2, X_3)$, i.e., dada a forma bilinear não degenerada
de Cartan-Killing para a álgebra semi-simples $\mathfrak{s}$:
\begin{align*}
\langle, \rangle: \mathfrak{s} \times \mathfrak{s} &\rightarrow \mathfrak{s}\\
(X,Y) &\mapsto \Tr( \Adj(X) \circ \Adj(Y) ),
\end{align*}
com $\Adj(X)$ representando a matriz da representação adjunta calculada no elemento $X \in \mathfrak{s}$, temos 
que $Y_i$ é dado pela igualdade $ \langle Y_i, X_j \rangle = \delta_{ij}$, com $i,j = 1,2,3$ e $\delta$ o
símbolo de Kronecker. Realizando os cálculos, obtemos para nossa álgebra $\mathfrak{s}$ o 
seguinte elemento do Casimir: \index{elemento do Casimir}
\begin{align*}
  C  &= -\{ (X_1)^2 + (X_3)^2 -2(X_1X_2 + X_2X_1) + (X_1X_3 +X_3X_1) \}\\
&=
\begin{pmatrix}
   -3&   0 & 3\\
   0&   -3 & 3\\
   0&   0 &  0
\end{pmatrix}
.
\end{align*}


Sabemos, vide Seção~\ref{bialg}, que a álgebra universal envelopante das matrizes quadradas é uma biálgebra,
com coproduto dado por:
\[
\Delta(x) = x \otimes I_n + I_n \otimes x,
\]
para $x$ uma matriz quadrada qualquer. Como a álgebra $\mathfrak{s}$ está imersa naturalmente na álgebra 
universal envelopante das matrizes $3 \times 3$, podemos calcular o coproduto do elemento do Casimir $C$:
%Sendo assim,  o coproduto do Casimir é igual a
\[
\Delta(C) = 
\begin{pmatrix}
   -6 &  0 & 3 &  0 &  0 &  0 & 3 &  0 &  0\\ 
   0 &  -6 & 3 &  0 &  0 &  0 &  0 & 3 &  0\\
   0 &  0 &  -3 &  0 &  0 &  0 &  0 &  0 & 3\\
   0 &  0 &  0 &  -6 &  0 & 3 & 3 &  0 &  0\\
   0 &  0 &  0 &  0 &  -6 & 3 &  0 & 3 &  0\\
   0 &  0 &  0 &  0 &  0 &  -3 &  0 &  0 & 3\\
   0 &  0 &  0 &  0 &  0 &  0 &  -3 &  0 & 3\\
   0 &  0 &  0 &  0 &  0 &  0 &  0 &  -3 & 3\\
   0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0
\end{pmatrix}
.
\]

Portanto, como era esperado, a partir do elemento do Casimir $C$, da álgebra semi-simples $\mathfrak{s}$,
obtivemos uma matriz geradora $L = \Delta(C)$ de um processo markoviano a tempo contínuo.
%%%% Fim do texto 
% \\endfold Cap Grupos

%%% Sistemas de particulas
% \beginfold
\chapter{Sistemas de Part\'iculas}
Neste capítulo descreveremos o conceito de sistema de 
partículas e analisaremos como podemos gerar um sistema de partículas
a partir da matriz de taxas $\bm{L}'$ obtida na seção \ref{sec:matrizL}. Nossa abordagem 
seguirá de perto o exposto em~\cite{durret2} e em~\cite{Nparticulas}.

Como resultado obtivemos um sistema de partículas de dois sítios e
três estados a partir da matriz $\bm{L}'$.  Em seguida, estendemos nosso processo estocástico 
 para um sistema
de N sítios, utilizando o gerador do sistema de duas partículas dado pela matriz $\bm{L}'$ e calculamos
operadores de autodualidade para estes sistemas de partículas.

Abaixo segue o operador de autodualidade calculado por nós
\[
D(\eta, \xi) = \prod_{l=1}^N (2 - \eta_l) (2- \xi_l),
\]
com $\eta, \xi$ processos pertencentes ao espaço de estado $\Omega$
e $\eta_l$ (resp. $\xi_l$) denotando o estado do vértice $l$ relativo ao
processo $\eta$ (resp. $\xi$). Para finalizar, utilizando o trabalho
de \cite{noether}, provamos o Teorema de Noether para a versão estocástica.

Desta forma, neste capítulo estabelecemos uma conexão entre sistema de partículas de N sítios
e a álgebra de Lie $sl_2(\mathbb{R})$, álgebra esta obtida na Seção \ref{sec:matrizL} e que
nos possibilitou o cálculo da matriz $\bm{L}'$.

\section{Uma breve Introdu\c{c}\~ao}
Dado um conjunto enumerável $\mathbf{S}$, cujos elementos
$x \in \mathbf{S}$ serão denotados como o indivíduo ou a
partícula na posição $x$, 
%aqui está para modelar um espaço cujos elementos apresentam
%certas características, características estas denotadas
%pelo conjunto finito 
$\mathbf{F}$ um conjunto finito, representando os
 %que serão denotadas como
possíveis estados de uma partícula (por exemplo,
$\mathbf{F}$ pode representar o fato de uma partícula
estar ou não numa posição $x \in \mathbf{S}$),
%ponto qualquer do espaço
%$\mathbf{S}$,
 e $\xi: S \rightarrow F$
uma função que representa exatamente os estados de cada indivíduo
do espaço em estudo. Dizemos, então, que um sistema de partículas
consiste no estudo da evolução temporal dos estados de cada
ponto do espaço $\mathbf{S}$. Em outras palavras, dado
o conjunto
\[
\mathscr{D} = \left\{ \eta: \mathbf{S} \rightarrow \mathbf{F}
\right\}
\]
de todas 
%funções que representam todos as
configurações possíveis
de estados para nosso espaço, o estudo do sistema de partículas
consiste na análise da função
\begin{align*}
\xi^{\xi}:  \mathbb{R}_{\geq 0} & \rightarrow  \mathscr{D}\\
     t & \mapsto  \xi,
\end{align*}
onde $\xi^{\xi}(0) = \xi$ e, para qualquer tempo positivo $t$,
$\xi^{\xi}(t)$ representa os estados de cada partícula do nosso
sistema. Por comodidade, denotaremos sempre
$\xi^{\xi}_t$ ao invés de $\xi^{\xi}(t)$.

\begin{exmp} \label{exemplo1}
Suponha o conjunto $\mathbf{S}$  dado por
\[
\mathbf{S} = \{-5,-4,-3 \ldots, 3, 4, 5\}^2
\]
%Agora defina o conjunto 
e $\mathbf{F}$ como sendo o conjunto
$\mathbf{F}=\{0,1\}$. Podemos interpretar $\mathbf{F}$ como o fato de
um indivíduo, situado numa posição $x \in \mathbf{S}$,
ter ou não uma doença. (Aqui, $1$ representa que o indivíduo está
doente e $0$ representa o indivíduo sadio.)
O sistema de partículas definido pelo espaço $\mathbf{S}$ e 
estado $\mathbf{F}$ será representado pela função $\xi^{\xi}_t$,
com
\[
\xi^{\xi}_t(x) =
\begin{cases}
1 & \text{se o indivíduo na posição $x$ e no tempo $t$
          está doente};\\
0 & \text{se o indivíduo na posição $x$ e no tempo $t$
          não está doente}.\\
\end{cases}
\]
A Figura~\ref{fig:grafo0} fornece um exemplo de uma função
$\xi^{\xi}_t$, onde os pontos azuis representam indivíduos
sadios enquanto que os pontos vermelhos representam pessoas
doentes.
\begin{figure}[t!] 
\centering
\begin{tikzpicture}
\draw[->] (-5.5,0) -- (5.5,0);
\draw[->] (0,-5.5) -- (0,5.5);

\foreach \x in {-5, ..., 5}
	\draw[loosely dashed] (\x, -5) -- (\x, 5);

\foreach \x in {-5, ..., 5}
	\draw[loosely dashed] (-5, \x) -- (5, \x);

\foreach \x in {-5, ..., 5}{
	\foreach \y in {-5, ..., 5}{
	\pgfmathparse{rnd}
	\newdimen \z
	\z = \pgfmathresult pt
	\ifdim \z < 0.5pt
	\draw[fill=blue] (\x, \y) circle (0.1); 
	\else %
	\draw[fill=red]  (\x, \y) circle (0.1); 
	\fi
	}
}
\end{tikzpicture}
\caption{Exemplo de um sistema de partículas que modela a 
distribuição de uma certa doença no espaço dado por
$\mathbf{S} = \{-5,-4,\ldots,4,5\}^2$. Aqui, pontos azuis
representam pessoas saudáveis e os pontos vermelhos representam
pessoas doentes.}
\label{fig:grafo0}
\end{figure}

\end{exmp}

Dado um conjunto finito e não nulo $\mathscr{N} \subseteq
\mathbf{S}$ ($\mathbf{S}$ um espaço vetorial), dizemos 
que dois pontos $x, y \in \mathbf{S}$ são vizinhos se 
a relação
$x - y \in \mathscr{N}$ for satisfeita. 
Para o Exemplo~\ref{exemplo1}, podemos considerar o conjunto
$\mathscr{N}$ como o conjunto 
\[
\mathscr{N} = \{ (1,0), (0,1), (-1,0), (0,-1) \}.
\]
Em geral, quando o espaço $\mathbf{S}$ for um espaço vetorial
normado,
%e estiver munido de uma norma $\|\|$,
 o conjunto $\mathscr{N}$ pode ser definido como
o conjunto
\[
\mathscr{N}_r = \{ x \in \mathbf{S}; \; \|x\| \leq r \},
\]
com $\|\|$ uma norma.

No estudo de sistema de partículas estaremos
interessados em estudar o comportamento da função
$\xi^{\xi}$ ao decorrer do tempo, isto é, como 
o estado das partículas irá mudar com o passar do tempo.
O comportamento de $\xi^{\xi}_t$ será dado pela seguinte
regra:
dado um ponto $x \in \mathbf{S}$ qualquer,  
$x$ irá para o estado $i \in \mathbf{F}$ numa 
taxa $c_i(x, \xi^{\xi}_t) $, cujo valor
% taxa esta que está em função
depende dos estados dos pontos vizinhos de $x$, i.e.,
\[
c_i(x, \xi^{\xi}_t) = g_i( \xi^{\xi}_t(x + z_0), 
\xi^{\xi}_t(x + z_1), \ldots
\xi^{\xi}_t(x + z_n) ),
\]
com $g_i(\ldots)$ uma função definida nos pontos 
$z_0, z_1, \ldots, z_n \in \mathscr{N}$.
Ou seja, a taxa com que um ponto $x$ muda de estado
está descrita em função de sua vizinhança.
Suponhamos que $\xi^{\xi}_t(x) \neq i$, então
a taxa $c_i$ pode ser vista como o limite da 
razão da probabilidade de transição de estados
dividido pelo tempo para ocorrer a transição:
\[
\lim_{s \rightarrow 0}
\frac
{ P( \xi^{\xi}_{t+s}(x) = i| \xi^{\xi}_t(x) ) }
{s}
= c_i(x, \xi^{\xi}_t).
\]

%Ao estudarmos sistemas partículas estaremos interessados
%em estudar o mecanismo com o qual um ponto qualquer $x$
%no espaço $\mathbf{S}$ muda do estado $i \in \mathbf{F}$
%para o estado $j \in \mathbf{F}$ no tempo $t_0$. Em outras
%palavras, dado um $\varepsilon > 0$ suficientemente pequeno,
%suponhamos que para $t \in [t_0 - \varepsilon, t_0)$ tenhamos
%$\xi^{\xi}_t(x) = i$. Estamos interessados em saber a dinâmica
%do 
\section{Din\^{a}mica do Sistema de Part\'{i}culas}
Para estudarmos a evolução de um sistema de partículas,
com configuração inicial $\xi$,
consideraremos as duas regras:
\begin{enumerate}[(i)]

\item (\textit{Tempo de saltos entre estados})
Todo ponto $x \in \mathbf{S}$ deverá esperar
por instantes de tempo $T^{x,i}$ para ``decidir'' 
se irá mudar de seu
estado atual para o estado $i$.
Tais instantes de tempo $T^{x,i}$ serão dados por 
 processos de Poisson independentes
de taxa $1$. Isto quer dizer que, caso
no instante $s$ tenhamos $\xi^{\xi}_s(x) = j$, x deverá %``decidir
esperar por um tempo aleatório $T^{x,i}$, cuja distribuição
de probabilidade é dada por uma exponencial de parâmetro
$1$, para decidir se muda para o estado $i$ ou não.
A família de tempos de espera, dada por variáveis 
aleatórias de distribuição exponencial de taxa
$1$, é ilustrada  pela Figura~\ref{fig:espera}.

%se muda para um estado $i$%, condicionado a regra (ii) abaixo,
%em um istatnte aleatório $T^{x,j}$, cuja distribuição
%de probabilidade é dado por uma exponencial de parâmetro
%1. 

\begin{figure}[h] 
\centering
\begin{tikzpicture}
\draw[|->] (0,0) node[below] {$0$} -- (10,0) node[below] {$t$};
\draw[|-|] (0,0) -- (3,0) node[below] {$T^{x,i}_1$};
\draw[|-|] (0,0) -- (5,0) node[below] {$T^{x,i}_2$};
\draw[|-|] (0,0) -- (8,0) node[below] {$T^{x,i}_3$};
\draw[dotted, very thick] (9,-0.4) -- (9.4,-0.4);
\end{tikzpicture}
\caption{Representação do tempo de espera $T^{x,i}$, dado
por um processo de Poisson $\PPP(1)$, para um ponto $x$ mudar,
ou não, para o estado $i$.}
\label{fig:espera}
\end{figure}

\item (\textit{Critério para um ponto $x$ mudar de seu estado
para um estado $i$}). Após um tempo de espera $t = T^{x,i}$, 
o ponto $x$ muda de seu estado para o estado $i$ se, 
dado um valor $U$, definido por uma distribuição uniforme
$U[0,1]$, tivermos 
\[U < \frac{c_i(x, \xi^{\xi}_{t-})}{\sum_{l \neq \xi^{\xi}_{t-}(x)} c_l(x, \xi^{\xi}_{t-})},\]
com $c_i$ as taxas de transição, definidas na seção anterior.
Agora, caso a desigualdade acima não seja satisfeita, teremos
que x mantém-se no seu estado atual. Um exemplo pode ser visto
na Figura~\ref{fig:regra2}.
\end{enumerate}

\begin{figure}[ht] 
\centering
\begin{tikzpicture}
\draw[|->] (0,0) -- (10,0) node[below] {$t$};
\draw[-|] (0,0) -- (4,0) node [below] {$s$ };
\draw[]   (4,-0.6) node {$\xi^{\xi}_s(x) = j$};
\draw[|-|] (0,0) -- (6,0) node[below] {$T^{x,i}$};
\draw[-|] (6,0) -- (6,3);
\draw[-|, color=red] (6,0) -- (6,2);
\draw[-|] (6,2) -- (6,3);
\draw [decorate,decoration={brace,amplitude=8pt,mirror,raise=4pt},
yshift=0pt]
(6,0) -- (6,2) node [black,midway,xshift=2.5cm] 
{$\text{pula para o estado i}$};
\draw [decorate,decoration={brace,amplitude=8pt,mirror,raise=4pt},
yshift=0pt]
(6,2) -- (6,3) node [black,midway,xshift=2.6cm] 
{$\text{mantém-se no estado j}$};
\draw[->] (0,0) -- (0,4) node [right] {Probabilidade};
\draw[-|] (0,0)  node [left] {$0$} -- (0,3) node [left] {$1$};
\draw[] (0,1.8) node {$\mathbf{\ast}$};
\draw[loosely dashed] (0,1.8) node[below right] 
{$U \sim \mathscr{U}[0,1]$} -- (8,1.8);
\end{tikzpicture}
\caption{Exemplo de transição, no instante $s$, do ponto $x$ no estado j
 para o estado $i$, no instante de tempo $t = s + T^{x,i}$.}
%Neste exemplo, a uniforme $U$ é menor
%que $c_i(x, \xi^{\xi}_{t-})$.}
\label{fig:regra2}
\end{figure}


Para finalizarmos a construção de um sistema de partículas,
enunciamos o argumento de Harris (1972),
cuja ideia baseia-se em responder a seguinte pergunta:
``Dado que um ponto $x \in \mathbf{S}$ muda de estado
em função de taxas de 
transição $c_i(x, \xi^{\xi}_{t-})$,
cujos valores estão em função dos estados 
$\xi^{\xi}_{t-}(y)$ dados pelos pontos vizinhos de $x$.
Queremos saber, então, se dado um instante de tempo $t$ qualquer,
por quantos pontos  $x$ tem sua taxa de 
transição influenciada''. Por influenciada, queremos dizer
o seguinte: sabemos que a taxa de transição de $x$
para o estado $i$ é dada por
\[
c_i(x, \xi^{\xi}_{t-}) = g_i( \xi^{\xi}_{t-}(z_0), \ldots
							  \xi^{\xi}_{t-}(z_n) ),
\]
com $z_0, \ldots, z_n$ vizinhos de $x$ cujos estados
%dizemos que os pontos $z_0, \ldots, z_n$ 
influenciam
o valor da taxa de transição de $x$. Portanto, dizemos
que $z_0, \ldots, z_n$ exercem influencia no valor da
taxa de transição de $x$. Além do mais, temos, por
exemplo, que
\[
c_i(z_0, \xi^{\xi}_{t-}) = g_i( \xi^{\xi}_{t-}(w_0), \ldots
							  \xi^{\xi}_{t-}(w_n) ),
\]
com $w_0, \ldots, w_n$ vizinhos de $z_0$. 
Como os pontos $w_0, \ldots, w_n$ influenciam no valor da taxa
de transição de $z_0$ que, por sua vez, também influencia
na taxa de transição de $x$, obtemos, de forma indireta,
que os pontos 
$w_0, \ldots, w_n$ influenciam a taxa de transição do ponto $x$.
Desta forma, podemos ter uma sequência infinita de pontos 
que exercem influência nas taxas de transições do ponto
$x$, como exemplificado na Figura~\ref{fig:influencia}.

\begin{figure}[h!] 
\centering

\begin{tikzpicture}

\tikzstyle{every node}= [circle, fill=gray!30]

\node (x) at (0,0) {$x$};
\node (y) at (0,2) {$y$};
\node (z) at (-2,0) {$z$};
\node (w) at (0,-2) {$w$};
\node (x1) at (2,0) {$x_1$};
\node (x2) at (4,0) {$x_2$};
\node (x3) at (6,0) {$x_3$};
\node (x4) at (8,0) {$x_4$};

\draw[->] (z) -- (x);
\draw[->] (y) -- (x);
\draw[->] (w) -- (x);
\draw[->] (x1) -- (x);
\draw[->] (x2) -- (x1);
\draw[->] (x3) -- (x2);
\draw[->] (x4) -- (x3);

\draw[dotted, thick, ->] (10,0) -- (x4);
\end{tikzpicture}
\caption{Exemplo de uma sequência infinita de pontos,
$x_1, x_2, \ldots$, que 
influenciam as taxas de transições do ponto $x$.}
\label{fig:influencia}
\end{figure}

Agora, observe que, caso os pontos $w_0, \ldots, w_n$ não
tenham mudado de estado no intervalo de tempo $(0,t_0)$,
então, obtemos que estes pontos já não exercem
nenhum tipo de influencia na taxa de transição do ponto 
$x$. Seguindo esta intuição, enunciamos o argumento de Harry:
\begin{quote}
``Para compreendermos a evolução
de nosso sistema de partículas devemos estudar o grafo 
aleatório, dado pelas partículas $\mathbf{S}$, e  cujas
 arestas sejam dadas pela seguinte regra:
 fixado um tempo $t_0$ suficientemente pequeno,
duas partículas $x, y \in \mathbf{S}$ estão conectadas
por uma aresta se, ambas forem vizinhas, e existir um tempo
$T^{x,i} < t_0$ em que ocorre  transição de estados. Ou seja, caso a partícula $x$ avalie mudar para
o estado $i$, no tempo $T^{x,i} < t_0$, então os pontos vizinhos
$x$ e $y$ serão conectados por uma aresta.''
\end{quote}

Em posse do grafo aleatório, resultante do argumento de Harry, 
e pelo resultado (vide Teorema~\cite{durret2}) o qual nos garante,
com probabilidade $1$, que
as componentes conexas %\footnote{Uma componente conexa de um grafo $G$...}
 deste grafo serão finitas para um tempo $t_0$
suficientemente pequeno (vide Figura~\ref{fig:harry}), obtemos uma 
metodologia para
estudarmos a evolução de nosso sistema de partículas, a partir
destas componentes conexas. Em outras palavras, como sabemos
que para cada partícula $x$ de uma componente conexa qualquer,
definida no intervalo $[0, t_0)$, sofre influência de uma quantidade
finita de partículas (pertencentes a mesma componente conexa que contém
a partícula $x$), podemos descrever a evolução do estado desta
partícula sem problemas. Além do mais, de forma interativa,
podemos descrever a evolução do sistema de partículas nos intervalos
de tempo $(0, t_0), [t_0, 2t_0), \ldots
[nt_0, (n+1)t_0), \ldots$, e, portanto, temos bem estabelecida a dinâmica
de nosso sistema de partículas para qualquer instante de tempo $t$.

%{{{
\begin{figure}[t!] 
\centering
\begin{tikzpicture}[scale=0.8]
\draw[->] (-3.5,0) -- (3.5,0);
\draw[->] (0,-3.5) -- (0,3.5);

%\foreach \x in {-3, ..., 3}
%	\draw[loosely dashed] (\x, -3) -- (\x, 3);

\foreach \x in {0, ..., 3} 
	\draw[color=blue, -, line width=0.1cm] (\x,-3) -- (\x,3);

\foreach \x in {-3, ..., 3} 
	\draw[color=blue, -, line width=0.1cm] (0,\x) -- (3,\x);

\foreach \x in {-3, ..., -1} 
	\draw[color=red, -, line width=0.1cm] (\x,-2) -- (\x,3);

\foreach \x in {-2, ..., 3} 
	\draw[color=red, -, line width=0.1cm] (-3,\x) -- (-1,\x);

\draw[color=yellow, -, line width=0.1cm] (-3,-3) -- (-1,-3);

\foreach \x in {-3, ..., 3} {
	\foreach \y in {-3, ..., 3}
		\draw[fill=gray] (\x, \y) circle (0.1cm);
}

%\foreach \x in {-3, ..., 3}
%	\draw[loosely dashed] (-3, \x) -- (3, \x);

\draw[->] (4.5,0) -- (11.5,0);
\draw[->] (8,-3.5) -- (8,3.5);

\foreach \x in {7, ..., 11} 
	\draw[color=blue, -, line width=0.1cm] (\x,-1) -- (\x,3);

\foreach \x in {-1, ..., 3} 
	\draw[color=blue, -, line width=0.1cm] (7,\x) -- (11,\x);

\foreach \x in {5, ..., 6} 
	\draw[color=red, -, line width=0.1cm] (\x,1) -- (\x,3);

\foreach \x in {1, ..., 3} 
	\draw[color=red, -, line width=0.1cm] (5,\x) -- (6,\x);


\draw[color=yellow	, -, line width=0.1cm] (5,-3) -- (11,-3);
\draw[color=yellow	, -, line width=0.1cm] (5,-2) -- (11,-2);
\draw[color=yellow	, -, line width=0.1cm] (5,-1) -- (6,-1);
\draw[color=yellow	, -, line width=0.1cm] (5,-0) -- (6,-0);
\draw[color=yellow	, -, line width=0.1cm] (5,-3) -- (5,0);
\draw[color=yellow	, -, line width=0.1cm] (6,-3) -- (6,-0);

\foreach \x in {7, ..., 11} 
	\draw[color=yellow, -, line width=0.1cm] (\x,-3) -- (\x,-2);


\foreach \x in {5, ..., 11} {
	\foreach \y in {-3, ..., 3}
		\draw[fill=gray] (\x, \y) circle (0.1cm);
}

%\foreach \x in {5, ..., 11}
%	\draw[loosely dashed] (\x, -3) -- (\x, 3);
%
%\foreach \x in {-3, ..., 3}
%	\draw[loosely dashed] (5, \x) -- (11, \x);

\draw [|-|] (-4,-4) node[below] {$0$} -- (4,-4) 
    node[below] {$t_0$};
\draw [|-|] (4,-4) -- (11.5,-4) node[below] {$2t_0$};

\draw[->] (11.5,-4) -- (12.5,-4) node[below right] {tempo};


\draw[fill=gray] (0,0) circle (0.1cm); 
%\draw[-] (0,0) circle (0.3pt) -- (1,1) circle (0.3pt);

\end{tikzpicture}
\caption{Exemplo de um sistema de partículas nos intervalos de tempo
$[0, t_0)$ e $[t_0, 2t_0)$. Para cada intervalo de tempo, destacamos, 
com cores distintas três componentes conexas do grafo. Também ressaltamos
que tais componentes conexas do grafo aleatório podem mudar com o tempo.}
\label{fig:harry}
\end{figure}
%}}}

\section{Propriedades e o Gerador de um Sistema de Part\'iculas}
Decorre da forma como estabelecemos a dinâmica de um sistema de partículas
%as seguintes propriadades (cujas demonstrações podem ser vistas em Durret):
que a evolução temporal  de seus estados, dados pela função
$\xi_t$, é uma cadeia de Markov a tempo contínuo
(a demonstração deste fato, como as demais demonstrações das
afirmações feitas  nesta 
seção, podem ser vistas em \cite{durret2} pg 119-122), i.e., sejam
$s,t \in \mathbf{R}_{\geq 0}$ instantes de tempo quaisquer, então
\[
P(\xi_{t+s} = \eta_1 |  \xi_{s} = \eta_0 ) =
P(\xi_{t} = \eta_1 |  \xi_{0} = \eta_0 ),
\]
com $\eta_1, \eta_0: \mathbf{S} \rightarrow 
\mathbf{F}$ funções representando os estados das partículas. 

Seja 
$\mathscr{D} = \left\{ \eta: \mathbf{S} \rightarrow \mathbf{F}
\right\}$ o conjunto de todas configurações possíveis de estados
para cada partícula do sistema e $f: \mathscr{D} \rightarrow \mathbb{R}$
uma função. Dizemos que $f$ é uma função local ou cilíndrica %(checar se este é o nome!!)
se existir um conjunto finito $H \subseteq \mathbf{S}$ tal que, para
$\eta, \xi \in \mathscr{D}$ e para todo $x \in 
\mathscr{S} \setminus H$, tivermos que $\eta(x) = \xi(x)$ implica
$f(\eta) = f(\xi)$ (para uma maior explicação deste tipo de funções
vide~\cite{daniela}).

Seja $\mathscr{D}_L$ o conjunto formado por todas as funções locais,
$\Psi = \{\phi: \mathscr{D} \rightarrow \mathbb{R}\}$, e $P_{\xi}$
a lei do processo com configuração inicial $\xi$.
A partir destes
conjuntos e de um instante de tempo $t$ fixado, definimos o seguinte
operador:
\begin{align*}
T_t: \mathscr{D}_L  & \rightarrow \Psi \\
   f & \mapsto T_t(f): \mathscr{D}  \rightarrow \mathbb{R}\\
     &  \qquad \qquad  \; \; \, \xi^{\xi} \mapsto E_{\xi} f(\xi_t),
\end{align*}
onde $E_{\xi}(\cdot)$ é a esperança com relação à medida de probabilidade $P_{\xi}$.
Tal operador $T_t$ satisfaz a condição de ser um semigrupo, i.e.,
o operador $T_t$ satisfaz (vide~\cite{noether}):
\begin{enumerate}[(i)]
\item $T_t$ é um operador linear, em relação ao tempo $t$, e preserva
distribuições de probabilidades;
\item $T_{s+t} = T_s T_t$, para $s, t \in \mathbb{R}_{\geq 0}$;
\item $T_0 = Id$, com $Id$ o operador identidade;
\item $T_t$ é contínua em relação ao instante de tempo $t$;
\item  Além do mais, para o operador $T_t$ em específico, vale:
$T_t$ é um semigrupo de Feller, i.e., se $f$ for uma função
contínua em relação a topologia produto do conjunto $\mathscr{D}$,
então $T_t$ é uma função contínua.
\end{enumerate}

Para encerrar esta seção, estabelecemos o gerador do semigrupo. Seja $f$ uma 
função local. Então o operador $\mathscr{L}$ dado por
\[
\mathscr{L}f(\eta) = \sum_{ \eta' \in \mathscr{D}\setminus{\eta} } C(\eta, \eta')\; ( f(\eta') - f(\eta) ),
\]
com $C(\eta, \eta')$ a taxa de transição da configuração $\eta$ para a $\eta'$, é chamado de gerador
do processo markoviano a tempo contínuo de taxas $C(\eta, \eta')$. Em notação matricial, este gerador é dado pela L-matriz
\[
\bm{\mathscr{L}} = (L_{\eta \xi})_{\eta, \xi \in \mathscr{D}} \quad \text{ com } 
L_{\eta \xi} = 
\begin{cases}
C(\eta, \xi) & \text{se } \eta \neq \xi;\\
-\sum_{\eta' \neq \eta} C(\eta, \eta') & c.c.
\end{cases}
\]


\section {Estabelecendo um sistema de part\'iculas a partir da matriz de taxas da se\c{c}\~ao 5.3}
\label{sec:Npart}

Dada a matriz de taxas 
\[
\bm{L}' = 
\begin{pmatrix}
   -6 &  0 & 3 &  0 &  0 &  0 & 3 &  0 &  0\\ 
   0 &  -6 & 3 &  0 &  0 &  0 &  0 & 3 &  0\\
   0 &  0 &  -3 &  0 &  0 &  0 &  0 &  0 & 3\\
   0 &  0 &  0 &  -6 &  0 & 3 & 3 &  0 &  0\\
   0 &  0 &  0 &  0 &  -6 & 3 &  0 & 3 &  0\\
   0 &  0 &  0 &  0 &  0 &  -3 &  0 &  0 & 3\\
   0 &  0 &  0 &  0 &  0 &  0 &  -3 &  0 & 3\\
   0 &  0 &  0 &  0 &  0 &  0 &  0 &  -3 & 3\\
   0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0
\end{pmatrix}
\]
obtida na seção~\ref{sec:matrizL}  a partir do coproduto do elemento do Casimir da álgebra universal envelopante $U(\mathfrak{s})$,
resultante
da álgebra de Lie 
\[ \mathfrak{s} = \spn \left\{
X_1:=
\begin{pmatrix}
0 & -1 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{pmatrix}
,
X_2:=
\begin{pmatrix}
1 & -1 & 0 \\
1 & -1& 0 \\
0 & 0 & 0 
\end{pmatrix}
,
X_3:=
\begin{pmatrix}
1 & -1 & 0 \\
0 & -1& 1 \\
0 & 0 & 0 
\end{pmatrix}
\right\},
\]
iremos construir um sistema de partículas 
a partir desta matriz.

Nosso sistema de partículas será da seguinte forma: teremos duas partículas
interagindo no sistema as quais podem assumir três estados distintos, ou seja,
$\bm{S} = \{0,1\}$ e $\bm{F} = \{0, 1, 2\}$. A partir deste sistema, teremos
que as possíveis configurações de estados para cada partícula serão dadas por
\[
\mathscr{D} = \left\{ \eta: \mathbf{S} \rightarrow \mathbf{F}
\right\} = \{ (0,0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0), (2,1), (2,2) \},
\]
com $\eta = (0,2)$, por exemplo, representando a configuração ao qual a
partícula $0$ está no estado $0$ e a partícula $1$ está no estado $2$. 
Agora, ordenando o conjunto $\mathscr{D}$ de acordo com a regra
\[
i_2(\eta) = 1 + \sum_{k=1}^2 \eta(k)3^{k-1},
\]
o qual pode ser vista em~\cite{Nparticulas}, obtemos a seguinte ordenação das funções $\eta: \bm{S} \rightarrow \bm{F}$:
\[
\eta_1 = (0,0), 
\eta_2 = (1,0), 
\eta_3 = (2,0), 
\eta_4 = (0,1), 
\eta_5 = (1,1), 
\]
\[
\eta_6 = (2,1), 
\eta_7 = (0,2), 
\eta_8 = (1,2), 
\eta_9 = (2,2).
\]
Resulta desta ordem %para nosso sistema de partículas e da matriz de taxa $\bm{L}$
a cadeia de Markov a tempo contínuo dada pela matriz geradora:
\[
\bm{L}' = 
\bordermatrix{~ & (0,0) & (1,0) & (2,0) & (0,1) & (1,1) & (2,1) & (0,2) & (1,2) & (2,2) \cr
   (0,0) &-6 &  0 & 3 &  0 &  0 &  0 & 3 &  0 &  0\cr 
   (1,0) &0 &  -6 & 3 &  0 &  0 &  0 &  0 & 3 &  0\cr
   (2,0) &0 &  0 &  -3 &  0 &  0 &  0 &  0 &  0 & 3\cr
   (0,1) &0 &  0 &  0 &  -6 &  0 & 3 & 3 &  0 &  0\cr
   (1,1) &0 &  0 &  0 &  0 &  -6 & 3 &  0 & 3 &  0\cr
   (2,1) &0 &  0 &  0 &  0 &  0 &  -3 &  0 &  0 & 3\cr
   (0,2) &0 &  0 &  0 &  0 &  0 &  0 &  -3 &  0 & 3\cr
   (1,2) &0 &  0 &  0 &  0 &  0 &  0 &  0 &  -3 & 3\cr
   (2,2) &0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0\cr }.
\]
Portanto, pela matriz geradora acima, temos que, dado uma configuração do sistema
de partículas qualquer, digamos $\eta_t^\eta$ (com o sobrescrito indicando a configuração
inicial e $t$ o tempo), teremos, por exemplo, que
\[
\lim_{s \rightarrow 0} \frac{ P[ \eta_{t+s}^\eta = (2,0) | \eta_t^\eta = (0,1) ]} {s} = 3,
\]
ou seja, a taxa de transição da configuração $(0,1)$ para a configuração $(2,0)$ vale $3$.

Além do mais, podemos estender nosso sistema de duas partículas para um sistema de $N$-partículas.
Para isto, utilizaremos a metodologia e a mesma notação apresentada em~\cite{Nparticulas}, o qual
estabelece que a matriz geradora para o sistema de $N$-partículas é dada por
\[
\bm{L} = \sum_{k = 1}^{N-1} h_{k, k+1},
\]
com $h_{k, k+1} = \bm{I}_3^{\otimes (k-1)} \otimes \bm{L}' \otimes \bm{I}_3^{ \otimes (N-k-1) }$.
($\bm{I}_3$ representa a matriz identidade com $\bm{I}_3^{\otimes 0} = 1$, $1$ a identidade
do corpo, e $\bm{I}_3^{\otimes k} = \bm{I}_3 \otimes \ldots \otimes \bm{I}_3$ $k$ vezes.)
Utilizando a base canônica de $\mathbb{R}^3$ dada pelos vetores coluna
\[
e_1 = 
\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad
e_2 = 
\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \quad
e_3 = 
\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}
\]
e estabelecendo as matrizes auxiliares
 %(cuja notação manteremos igual a proposta em~\cite{Nparticulas})
\begin{align*}\tag{ $\ast$}
a^+ = e_1 \otimes e_2^T,\quad  a^- = e_2 \otimes e_1^T,\quad  \widehat{a} = e_1 \otimes e_1^T\\
b^+ = e_3 \otimes e_2^T,\quad  b^- = e_2 \otimes e_3^T,\quad  \widehat{b} = e_3 \otimes e_3^T\\
c^+ = e_1 \otimes e_3^T,\quad  c^- = e_3 \otimes e_1^T,\quad  \widehat{v} = e_2 \otimes e_2^T
\end{align*}
que nada mais são que a base canônica do espaço vetorial das matrizes $3\times3$, podemos
escrever nossa matriz $\bm{L}'$ em termos destas matrizes:
\begin{align*} 
\bm{L}' =& -6 ( \widehat{a} \otimes \widehat{a} + \widehat{a} \otimes \widehat{v} + \widehat{v} \otimes \widehat{a} + 
		\widehat{v} \otimes \widehat{v}) 
     +3 ( \widehat{a} \otimes c^+ + \widehat{a} \otimes b^- - \widehat{a} \otimes \widehat{b} 
     + \widehat{v} \otimes c^+ + \widehat{v} \otimes b^- - \widehat{v} \otimes \widehat{b} \\
  & + c^+ \otimes \widehat{a} + b^- \otimes \widehat{a} - \widehat{b} \otimes \widehat{a} 
   + c^+ \otimes \widehat{v} + b^- \otimes \widehat{v} - \widehat{b} \otimes \widehat{v}
   + c^+ \otimes \widehat{b} + b^- \otimes \widehat{b} + \widehat{b} \otimes c^+ + \widehat{b} \otimes b^-).
\end{align*}

Agora, seja $u, v$ alguma das matrizes definidas em ($\ast$), temos %, vide~\cite{Nparticulas},
que as matrizes 
\begin{align*}
u_k = \bm{I}_3^{\otimes (k-1)} \otimes u \otimes \bm{I}_3^{ \otimes (N-k) }, \qquad
v_{k+1} = \bm{I}_3^{\otimes (k)} \otimes v \otimes \bm{I}_3^{ \otimes (N-k-1) }
\end{align*}
satisfazem $u_k v_{k+1}=
\bm{I}_3^{\otimes (k-1)} \otimes (u \otimes v) \otimes \bm{I}_3^{ \otimes (N-k-1) }$.
Sendo assim, o operador
$h_{k, k+1} = \bm{I}_3^{\otimes (k-1)} \otimes \bm{L}' \otimes \bm{I}_3^{ \otimes (N-k-1) }$ pode 
ser escrito da seguinte forma
\begin{align*} 
h_{k, k+1} =& -6 ( \widehat{a}_k \widehat{a}_{k+1} + \widehat{a}_k \widehat{v}_{k+1} + \widehat{v}_k \widehat{a}_{k+1} + 
		\widehat{v}_k \widehat{v}_{k+1}) 
     +3 ( \widehat{a}_k c^+_{k+1} + \widehat{a}_k b^-_{k+1} - \widehat{a}_k \widehat{b}_{k+1}
     + \widehat{v}_k c^+_{k+1} + \widehat{v}_k b^-_{k+1} \\
	 & - \widehat{v}_k \widehat{b}_{k+1} 
   + c^+_k \widehat{a}_{k+1} + b^-_k \widehat{a}_{k+1} - \widehat{b}_k \widehat{a}_{k+1} 
   + c^+_k \widehat{v}_{k+1} + b^-_k \widehat{v}_{k+1} - \widehat{b}_k \widehat{v}_{k+1}
   + c^+_k \widehat{b}_{k+1} + b^-_k \widehat{b}_{k+1}\\
   & + \widehat{b}_k c^+_{k+1} + \widehat{b}_k b^-_{k+1})
\end{align*}
e, consequentemente, temos o gerador de um sistema de $N$-partículas
\[
\bm{L} = \sum_{k = 1}^{N-1} h_{k, k+1}
\]
bem definido.

\section{Autodualidade}
Nesta seção iremos estabelecer um operador de autodualidade $D'$ para nosso sistema de duas
partículas e, a partir do mesmo, definiremos um dual para o sistema de N-partículas
cujo gerador foi estabelecido no final da seção anterior. 
Nossa metodologia será estabelecida de forma similar a construída em~\cite{daniela}.
Desta forma, iniciamos nossa discussão definindo o conceito de autodualidade:
\begin{definicao}
Sejam as cadeias de Markov a tempo contínuo, e independentes, $(\eta_t)_{t \geq 0}$ e 
$(\xi_t)_{t \geq 0}$, com espaço de estados $\Omega$, e $D: \Omega \times \Omega \rightarrow \mathbb{R}$
uma função mensurável. A função $D$ é dita ser um operador de autodualidade entre os processos
$(\eta_t)$ e $(\xi_t)$ se
\[
E_{\eta}[ D(\eta_t, \xi) ] = E_{\xi}[ D(\eta, \xi_t) ]
\]
para todo $t \geq 0$ e para todo par de processos markovianos $(\eta, \xi) \in \Omega^2$.
%%Aqui, $E_{\eta}[ D(\eta_t, \xi) ]$ representa a esperança em relação a variável
%%aleatória $\eta$ enquanto que 
%%$E_{\xi}[ D(\eta, \xi_t) ]$ representa a esperança em relação a variável
%%$\xi$.
\end{definicao}

Desta definição temos a seguinte caracterização:
\begin{prop}\label{prop:dual}
Seja $\Omega$ um espaço de estados, $\mathscr{L}$ um gerador
de um processo markoviano e $D: \Omega \times \Omega \rightarrow \mathbb{R}$ uma função. Então,
se a função $D$ pertence ao domínio do gerador, temos que $D$ é um operador autodual 
se, e somente se,
\[
\mathscr{L}D(\eta, \cdot)(\xi) = \mathscr{L}D(\cdot, \xi)(\eta),
\]
para todo $\eta, \xi \in \Omega$. 
\end{prop}
\begin{proof}
Vide \cite{daniela}.
\end{proof}

\begin{obs*}
Relembremos que para todo gerador $\mathscr{L}$ de um processo markoviano a tempo contínuo, e com 
espaço de estados $\Omega$, existe
uma matriz associada ao gerador, denotada por $\bm{L} = (L_{\eta \xi})_{\eta, \xi \in \Omega}$, satisfazendo:
\[
L_{\eta \xi} = 
\begin{cases}
C(\eta, \xi) & \text{se } \eta \neq \xi;\\
-\sum_{\eta' \neq \eta} C(\eta', \eta) & c.c.
\end{cases}
\]
com $C(\eta, \xi)$ as taxas de transição da cadeia de Markov.
\end{obs*}

\begin{obs*}
Note que a igualdade descrita na Proposição~\ref{prop:dual} pode ser reescrita como
\[
\sum_{\sigma \in \Omega} \mathscr{L}(\xi, \sigma) D(\eta, \sigma)=
\sum_{\sigma \in \Omega} \mathscr{L}(\eta, \sigma) D(\sigma, \xi),
\]
para todo $\xi, \eta \in \Omega$. Note, também, que a igualdade acima é a mesma 
se considerarmos a matriz geradora $\bm{L}$, associada ao gerador $\mathscr{L}$, i.e.,
\[
\sum_{\sigma \in \Omega} \bm{L}_{\xi, \sigma} D(\eta, \sigma)=
\sum_{\sigma \in \Omega} \bm{L}_{\eta, \sigma} D(\sigma, \xi).
\]
\end{obs*}


Em posse desta definição, a Proposição abaixo (vide \cite{dual2}) será útil para estabelecermos operadores de autodualidade.
\begin{prop}
Dado o espaço de estados finito $\Omega$, a matriz geradora $\bm{L}$ de uma cadeia de Markov a tempo contínuo
e uma função $D: \Omega \times \Omega \rightarrow \mathbb{R}$. Se considerarmos a matriz
$\bm{D} = (D_{\eta, \xi})_{\eta, \xi \in \Omega}$ como a matriz dada por  $\bm{D}_{\eta, \xi} = D(\eta, \xi)$,
então, teremos que a função $D$ é um operador de autodualidade se, e somente se, a igualdade 
\[
\bm{L} \bm{D} = \bm{D} \bm{L}^T
\]
for valida.
\end{prop}
%\begin{teo}
%Dado um processo markoviano  a tempo contínuo com espaço de estados $\Omega$ enumerável e $\bm{L}$ sua
%matriz geradora. Agora, tome uma matriz inversível $\bm{Q}$ a qual satisfaz a condição 
%$\bm{L}^T \bm{Q} = \bm{Q} \bm{L}$, e considere a matriz $\bm{S}$ que comuta com
%a matriz $\bm{L}^T$. Então, se a função $D: \Omega \times \Omega \rightarrow \mathbb{R}$ associada
%a matriz $\bm{D} = \bm{Q}^{-1} \bm{S}$ é uma função mensurável tal que $D(\eta, \cdot)$ e
%$D(\cdot, \xi)$ pertencem ao domínio do operador gerador associado a matriz geradora $\bm{L}$
%para todo $t \geq 0$ e para todos $\eta, \xi \in \Omega$, resulta, então, que o operador
%$D$ é uma função de autodualidade.
%\end{teo}

Desta maneira, para definirmos uma autodualidade para nosso sistema de duas partículas,
dado pela matriz geradora 
\[
\bm{L}^\prime = 
\bordermatrix{~ & (0,0) & (1,0) & (2,0) & (0,1) & (1,1) & (2,1) & (0,2) & (1,2) & (2,2) \cr
   (0,0) &-6 &  0 & 3 &  0 &  0 &  0 & 3 &  0 &  0\cr 
   (1,0) &0 &  -6 & 3 &  0 &  0 &  0 &  0 & 3 &  0\cr
   (2,0) &0 &  0 &  -3 &  0 &  0 &  0 &  0 &  0 & 3\cr
   (0,1) &0 &  0 &  0 &  -6 &  0 & 3 & 3 &  0 &  0\cr
   (1,1) &0 &  0 &  0 &  0 &  -6 & 3 &  0 & 3 &  0\cr
   (2,1) &0 &  0 &  0 &  0 &  0 &  -3 &  0 &  0 & 3\cr
   (0,2) &0 &  0 &  0 &  0 &  0 &  0 &  -3 &  0 & 3\cr
   (1,2) &0 &  0 &  0 &  0 &  0 &  0 &  0 &  -3 & 3\cr
   (2,2) &0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0\cr },
\]
devemos achar uma matriz $\bm{D}$ tal que  a igualdade
\[\bm{L}^\prime \bm{D} = \bm{D} \bm{L}^{\prime T}\] seja satisfeita. 
Resolvendo este sistema linear, é possível determinar o seguinte operador autodual para nosso sistema de duas
partículas
\[
D'( (\eta_1, \eta_2), (\xi_1, \xi_2) ) = \prod_{l=1}^2 (2 - \eta_l) (2 - \xi_l)
\]
com $\eta = (\eta_1, \eta_2), \xi = (\xi_1, \xi_2)$ processos pertencentes ao espaço de estados de 
duas partículas. Para o operador $D'$, temos a seguinte 
representação matricial:
\[
\bm{D'} = 
\bordermatrix{~ & (0,0) & (1,0) & (2,0) & (0,1) & (1,1) & (2,1) & (0,2) & (1,2) & (2,2) \cr
   (0,0) &16 &  8 & 0 &  8 &  4 &  0 & 0 &  0 &  0\cr 
   (1,0) &8 &  4 & 0 &  4 &  2 &  0 &  0 & 0 &  0\cr
   (2,0) &0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 & 0\cr
   (0,1) &8 &  4 &  0 &  4 &  2 & 0 & 0 &  0 &  0\cr
   (1,1) &4 &  2 &  0 &  2 &  1&  0 & 0 &  0 &0\cr
   (2,1) &0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 & 0\cr
   (0,2) &0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 & 0\cr
   (1,2) &0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 & 0\cr
   (2,2) &0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0\cr }
\]
que satisfaz $\bm{L}'\bm{D} = \bm{D} \bm{L}^{\prime  T}$.

Agora, sendo que para um sistema de duas partículas, com gerador dado pela matriz $\bm{L}'$,
somos capazes de definir um operador de autodualidade $D'$, nosso objetivo, a seguir, será estender este operador 
para um sistema de N-partículas de  gerador definido por
\[
\bm{L} = \sum_{k=1}^{N-1} h_{k,k+1},
\]
com $h_{k,k+1} = \bm{I}_3^{\otimes(k-1)}\otimes \bm{L}' \otimes \bm{I}_3^{\otimes (N-k-1)}$ 
($\bm{I}_3$ a matriz identidade $3 \times 3$).
Uma forma natural, para o operador de autodualidade de um sistema de N-partículas, é a
seguinte função:
\[
D(\eta, \xi) = \prod_{l=1}^N (2 - \eta_l) (2- \xi_l),
\]
com $\eta = (\eta_1, \ldots, \eta_N)$ e $\xi = (\xi_1, \ldots, \xi_N)$ pertencentes ao espaço
de estados de um sistema de N-partículas. Abaixo mostraremos que $D$ é uma função de autodualidade.


\begin{prop}
Seja o sistema de N-partículas, de espaço de estados $\Omega$, dado pelo gerador
\[
\bm{L} = \sum_{k=1}^{N-1} h_{k,k+1},
\]
com $h_{k,k+1} = \mathbbm{1}^{\otimes(k-1)}\otimes \bm{L}' \otimes \mathbbm{1}^{\otimes (N-k-1)}$, onde
$\bm{L}'$ é nossa matriz geradora, já definida no início desta seção, e $D'$  o autodual
definido para o sistema de duas partículas.
Então, o operador $D$ dado por
\[
D(\eta, \xi) = \prod_{i=1}^N (2 - \eta_i) (2 - \xi_i),
\]
com $\xi, \eta \in \Omega$, é um operador de autodualidade.
\end{prop}

\begin{proof}
Observação, para não sobrecarregarmos a notação,  denotaremos a entrada da matriz 
$L_{\eta \xi}$ por apenas $L(\eta, \xi) = L_{\eta \xi}$ (respectivamente, fazemos o mesmo para as matrizes $h_{k, k+1}$).

Considerando o gerador $\mathscr{L}$ associado a matriz geradora $\bm{L}$,
e $\mathscr{L}'$ ao gerador referente a matriz $\bm{L}'$,  temos,
para todos $\eta, \xi \in \Omega$, a igualdade
\begin{align*}
\mathscr{L}D(\eta, \cdot)(\xi) &= \sum_{\sigma \in \Omega} L(\xi, \sigma) D(\eta, \sigma)\\
						  &= \sum_{k=1}^{N-1} \sum_{\sigma \in \Omega} h_{k,k+1}(\xi, \sigma) D(\eta, \sigma)\\
						  &= \sum_{k=1}^{N-1} \sum_{\sigma \in \Omega} L'(\xi^k, \sigma^k) \prod_{i=k}^{k+1} (2-\eta_i)(2- \sigma_i) 
						     \prod_{i \in \{1,2,\ldots N\}\setminus\{k,k+1\}} (2-\eta_i)(2- \xi_i),
\end{align*}
com $\sigma^k = (\sigma_k, \sigma_{k+1})$ e $\xi^k = (\xi_1, \xi_2)$.
Chamando de 
\[A_k(\eta, \xi) = \prod_{i \in \{1,2,\ldots N\}\setminus\{k,k+1\}} (2-\eta_i)(2- \xi_i),\]
obtemos
\[
\mathscr{L}D(\eta, \cdot)(\xi)= \sum_{k=1}^{N-1} A_k(\eta, \xi) \sum_{\sigma \in \Omega} L'(\xi^k, \sigma^k)D'(\eta^k, \sigma^k).
\]
Pela autodualidade de $D'$, conseguimos a igualdade
\[
\mathscr{L}D(\eta, \cdot)(\xi)= \sum_{k=1}^{N-1} A_k(\eta, \xi) \mathscr{L}'D'(\cdot, \xi^k)(\eta^k).
\]
Porém,
\begin{align*}
\mathscr{L}D(\cdot, \xi)(\eta) &= \sum_{\sigma \in \Omega} L(\eta, \sigma) D(\sigma, \xi)\\
						  &= \sum_{k=1}^{N-1} \sum_{\sigma \in \Omega} h_{k,k+1}(\eta, \sigma) D(\sigma, \xi)\\
						  &= \sum_{k=1}^{N-1} \sum_{\sigma \in \Omega} L'(\eta^k, \sigma^k) \prod_{i=k}^{k+1} (2-\sigma_i)(2-\xi_i) 
						     \prod_{i \in \{1,2,\ldots N\}\setminus\{k,k+1\}} (2-\eta_i)(2-\xi_i)\\
						  &= \sum_{k=1}^{N-1} A_k(\eta, \xi) \sum_{\sigma \in \Omega} L'(\eta^k, \sigma^k) D'(\sigma^k, \xi^k) \\
						  &= \sum_{k=1}^{N-1} A_k(\eta, \xi) \mathscr{L}'D'(\cdot, \xi^k)(\eta^k).
\end{align*}

Ou seja, 
$\mathscr{L}D(\cdot, \xi)(\eta)  = \mathscr{L}D(\eta, \cdot)(\xi)$ e, portanto, $D$ é um operador
de autodualidade.
\end{proof}

\section{Uma Vers\~ao do  Teorema de Noether para o caso estoc\'astico}

Em mecânica quântica, o Teorema de Noether é utilizado para estabelecer
relações entre medidas que se conservam com o tempo e suas simetrias
(vide \cite{noether}).  
No entanto, este Teorema também possui uma variante 
para processos estocásticos, o qual estudaremos nesta seção. % a versão estocástica
(Aqui, toda notação, quanto a ideia para demonstrar
o teorema, foram retirados de~\cite{noether}.)

%o qual estabelece condições para 
%a simetria entre matrizes geradoras de cadeias de Markov
%a tempo contínuo, denotadas por $H$, 
%e funções $O$, ditas ``observáveis'', quando 
%todas medidas de probabilidade $\phi$ ($\phi$ um vetor) satisfazendo
%\[
%\frac{d}{dt}\phi(t) = \phi(t) H 
%\]
%admitem observáveis cujo valor esperado e variância são invariantes em
%relação ao tempo. Aqui, toda notação, quanto a ideia para demonstrar
%o teorema, foram retirados de~\cite{noether}.

Antes de enunciarmos e demonstrarmos a versão estocástica do Teorema de Noether, devemos estabelecer algumas definições e
um pouco de notação. Dado um espaço de probabilidade finito, denotado
por $X = \{ x_1, x_2, \ldots, x_n \}$ e com medida de probabilidade
$\phi: X \rightarrow [0,1]$, dada pelo vetor
\[
\bm{\phi} = [ \phi(x_1) \; \phi(x_2) \;\ldots\; \phi(x_n) ].
\]
Dizemos que toda  função que mapeia
o espaço $X$ nos reais,  $O: X \rightarrow \mathbb{R}$, é um observável, 
com representação matricial dada por
\[
\bm{O} = (O_{ij}), \;
\text{com } \;
O_{ij} = \delta_{ij} O(x_i)
\]
e,
realizaremos o abuso de notação, definindo o vetor formado
pelos valores $O(x_i)$ pelo mesmo símbolo que representa a matriz
do observável, i.e., denotaremos também
por $\bm{O}$ o vetor
\[
\bm{O} = [ O(x_1) \; O(x_2) \;\ldots\; O(x_n) ].
\]
%o qual saberemos se tratar pela matriz ou vetor coluna do observável
%$O$ pelo contexto de nossas contas.

Ainda, em relação ao observável $O: X \rightarrow \mathbb{R}$, 
%o qual  sabemos ser uma variável aleatória,
 escreveremos sua média e segundo momento em 
função do produto linear canônico $\langle, \rangle$ em $\mathbb{R}^n$: 
\[
\E[O] = \sum_{i=1}^{n} O(x_i) \phi(x_i) = \langle \bm{O}, \bm{\phi} \rangle
\quad \text{ e} \quad
\E[O^2] = \sum_{i=1}^{n} O^2(x_i) \phi(x_i) = \langle \bm{O}^2, \bm{\phi} \rangle.
\]
Para finalizar, seja $\pi = [\pi_1 \, 
\pi_2 \, \ldots \, \pi_n]^T$ um vetor e $f: 
\mathbb{R} \rightarrow \mathbb{R}$ uma função qualquer. Denotaremos,
em negrito, o vetor $\bm{f(\pi)}$ como sendo o vetor
\[
\bm{f(\pi)} = [ f(\pi_1) \; f(\pi_2) \;\ldots\; f(\pi_n) ].
\]

%Agora,
%relembremos a definição de uma L-matriz: uma matriz $\bm{H} \in \mathbb{R}^{n \times n}$
%é  chamada de L-matriz quando todas suas linhas somam zero e seus
%valores fora da diagonal são positivos. %Por meio de uma L-matriz
%Em posse das L-matrizes estabelecemos a seguinte definição:

\begin{definicao}
Dada uma L-matriz\footnote{ relembre que uma matriz $\bm{H} \in \mathbb{R}^{n \times n}$
é  chamada de L-matriz quando todas suas linhas somam zero e seus
valores fora da diagonal são positivos.} %Por meio de uma L-matriz
 $\bm{H}$, definimos o grafo orientado, chamado de grafo de
transição, como o grafo cujos vértices são formados pelo
conjunto $X = \{x_1, x_2, \ldots, x_n \}$ e, dados dois pontos quaisquer $x_i, x_j \in X$, existe uma aresta conectando eles se
$H_{ij} \neq 0$
ou $H_{ji} \neq 0$. 
\end{definicao}

Uma componente conexa do grafo de transição
será um grafo, cujos vértices são dados por um subconjunto
$Y \subseteq X$ tal que, para dois pontos $x_i, x_j \in Y$ quaisquer, deverá
existir
uma sequência de pontos $x_i = x_{l_1}, x_{l_2}, \ldots, x_{l_m}, 
x_{l_{m+1}} = x_j$,
com $x_{l_1}, x_{l_2}, \ldots, x_{l_{m+1}} \in Y$,  satisfazendo
\[
H_{l_pl_{p+1}} \neq 0 \; \text{ou} \; H_{l_{p+1}l_p} \neq 0
\quad \text{para } p = 1, 2, \ldots, m.
\]


Antes de demonstrarmos a versão estocástica do Teorema de Noether, 
utilizaremos o seguinte resultado:

\begin{teo}
Dado um conjunto finito $X$ de cardinalidade
$|X| = n$, uma L-matriz $\bm{H} \in \mathbb{R}^{n \times n}$,
e um observável $O: X \rightarrow \mathbb{R}$. As seguintes afirmações
são equivalentes:
\begin{enumerate}[(i)]
\item O comutador da matriz do observável $O$ pela L-matriz $\bm{H}$ é
nulo,i.e, 
\[ [\bm{O}, \bm{H}] = \bm{O}\bm{H} - \bm{H}\bm{O} = \bm{0}; \]
\item Para todo polinômio $f: \mathbb{R} \rightarrow \mathbb{R}$ e
para toda medida de probabilidade $\phi: X \rightarrow \mathbb{R}$
satisfazendo $\frac{d}{dt}\bm{\phi}(t) = \bm{\phi}(t)\bm{H}$, temos
\[
\frac{d}{dt} \langle \bm{f(O)}, \bm{\phi}(t) \rangle = 0;
\]
\item para toda medida de probabilidade $\phi: X \rightarrow \mathbb{R}$
satisfazendo $\frac{d}{dt}\bm{\phi}(t) = \bm{\phi}(t)\bm{H}$, temos
\[
\frac{d}{dt} \langle\bm{O}, \bm{\phi}(t) \rangle = 
\frac{d}{dt} \langle\bm{O}^2, \bm{\phi}(t) \rangle = 0;
\]
\item $O(x_i) = O(x_j)$ se $x_i, x_j$ pertencem a mesma componente conexa
do grafo construído a partir da L-matriz $\bm{H}$ e do conjunto de pontos
$X$.
\end{enumerate}
\end{teo}
\begin{proof}
Mostremos que $(i) \implies (ii)$.
Observe que $[\bm{O}, \bm{H}] = \bm{0}$ implica que 
$\bm{O}\bm{H} = \bm{H}\bm{O}$, ou seja para $i,j = 1, 2, \ldots, n$
quaisquer, vale
\[
\sum_{k=1}^n O_{ik} H_{kj} = \sum_{l=1}^n H_{il}O_{lj} \implies
O_{ii}H_{ij} = H_{ij}O_{jj}.
\]
Agora, calculemos a derivada do produto interno
\[
\frac{d}{dt} \langle \bm{f(O)}, \bm{\phi}(t) \rangle  \;=\;
\langle \bm{f(O)}, \frac{d}{dt}\bm{\phi}(t)  \rangle  \;= \;
\langle \bm{f(O)}, \bm{\phi}(t)\bm{H}\rangle.
\]
Observe que 
\[
\bm{\phi}(t)\bm{H} = \left[ \sum_{k=1}^n \phi_k H_{k1} \; \ldots \;
\sum_{k=1}^n \phi_k H_{kn} \right],
\]
com $\phi_k = \phi(x_k)$. Desta forma, o produto interno escrito 
acima fica igual a
\[
\langle \bm{f(O)}, \bm{\phi}(t)\bm{H} \rangle = 
\sum_{l=1}^n \left( f(O_l) \sum_{k=i}^n \phi_k H_{kl}  \right),
\]
com $O_l = O(x_l)$. Decorre de $f$ ser um polinômio de grau $m$ que
$f(O_l) = \sum_{i=0}^m a_i O_l^i$, com $a_i$ os coeficientes do polinômio.
Assim, obtemos de 
$O_{i}H_{ij} = H_{ij}O_{j}$ que
\[
f(O_l) H_{lk} = \sum_{i=0}^m a_i ( O_l^i H_{lk} ) =
\sum_{i=0}^m a_i ( O_k^i H_{lk} ) = 
f(O_k) H_{lk}.
\]
Portanto,
\[
\langle \bm{f(O)}, \bm{\phi}(t)\bm{H} \rangle = 
\sum_{l=1}^n \left( \sum_{k=1}^n \phi_k f(O_l) H_{kl}  \right) = 
\sum_{l=1}^n \left( \sum_{k=1}^n \phi_k f(O_k) H_{kl}  \right) =
\sum_{k=1}^n \left( \phi_k f(O_k) \sum_{l=1}^n H_{kl}  \right),
\]
ou seja, $
\langle \bm{f(O)}, \bm{\phi}(t)\bm{H} \rangle = 0$,
pois $\sum_{l=1}^n H_{kl}=0$, já que $\bm{H}$ é uma L-matriz.
Desta forma, concluímos que 
\[\frac{d}{dt} \langle \bm{f(O)}, \bm{\phi}(t) \rangle  = 0.\]

Para mostrarmos que $(ii) \implies (iii)$ basta tomarmos os
polinômios $f_1(x) = x$ e $f(x) = x^2$, caso trivial. Agora,
para obtermos que $(iii) \implies (iv)$ notemos que se os
pontos $x_i, x_j$ pertencem a mesma componente conexa então 
existe 
uma sequência de pontos $x_i = x_{l_1}, x_{l_2}, \ldots, x_{l_m}, 
x_{l_{m+1}} = x_j$
tal que 
\[
H_{l_pl_{p+1}} \neq 0 \; \text{ou} \; H_{l_{p+1}l_p} \neq 0
\quad \text{para } p = 1, 2, \ldots, m.
\]
Desta forma, precisamos mostrar apenas que 
\[
O(x_{l_1}) = O(x_{l_2}), \; 
O(x_{l_2}) = O(x_{l_3}), \; \ldots \;
O(x_{l_m}) = O(x_{l_{m+1}}).
\]
Para isto, utilizaremos o seguinte truque. Fixado um $p \in \{
1, 2, 3, \ldots, m\}$ e assumindo, sem perda de generalidade,
que $H_{l_pl_{p+1}} \neq 0$, 
 analisemos a somatória abaixo
\[
\sum_{k=1}^n (O_k - O_{l_p})^2 H_{l_pk}.
\]
A somatória pode ser reescrita como
\begin{align*}
\sum_{k=1}^n (O_k - O_{l_p})^2 H_{l_pk} & =
\sum_{k=1}^n O_k^2 H_{l_pk} +
\sum_{k=1}^n O_{l_p}^2 H_{l_pk} -
2\sum_{k=1}^n O_k O_{l_p} H_{l_pk}\\
&=
\sum_{k=1}^n O_k^2 H_{l_pk} +
 O_{l_p}^2 \sum_{k=1}^n  H_{l_pk} -
 2O_{l_p} \sum_{k=1}^n O_k H_{l_pk}\\
\end{align*}
e, observando que o semigrupo $U_i(t) = e^{\bm{H}t}e_i$,
com $e_1, \ldots, e_n$ a base canônica do $\mathbb{R}^n$, satisfaz 
$\frac{d}{dt} U_i(t) = \bm{H}U(t)$, obtemos, pelo item $(iii)$ e pelo
fato de $\bm{H}$ ser uma L-matriz, as seguintes identidades
\begin{itemize}
\item 
$\sum_{k=1}^n H_{l_pk} = 0$;
\item 
$0 = \frac{d}{dt} \langle \bm{O}, \bm{U}_i(t) \rangle \; = \;
\langle \bm{O}, \bm{HU}_i(t) \rangle  $ para todo instante de tempo $t$. Tomando $t=0$
e $i = l_p$
temos $\bm{U}_{l_p}(0) = \bm{I}_n e_{l_p}$. Logo
\[
0 \;=\; \langle \bm{O}, \bm{H} e_{l_p} \rangle \; = \; 
\sum_{k=1}^n O_k H_{l_pk};
\]
\item 
$0 = \frac{d}{dt} \langle \bm{O}, \bm{U}_i(t) \rangle \; = \;
\langle \bm{O}^2, \bm{HU}_i(t) \rangle  $ para todo instante de tempo $t$. Tomando $t=0$
e $i = l_p$,
temos, novamente, $\bm{U}_{l_p}(0) = \bm{I}_n e_{l_p}$. Logo
\[
0 \;=\; \langle \bm{O}^2, \bm{H} e_{l_p} \rangle \; = \; 
\sum_{k=1}^n O_k^2 H_{l_pk}.
\]
\end{itemize}
Desta forma, concluímos que 
$
\sum_{k=1}^n (O_k - O_{l_p})^2 H_{l_pk} = 0
$
e, portanto, como cada parcela desta soma é maior ou igual a zero pois
$H_{ij} \geq 0$ para $ i \neq j$ e para $i = j$ temos $(O_i -O_j) = 0$,
então, concluímos que $(O_{l_{p+1}} - O_{l_p}) = 0$, já que assumimos
que $H_{l_pl_{p+1}} \neq 0$. Sendo assim, para cada ponto da sequência 
$x_i = x_{l_1}, x_{l_2}, \ldots, x_{l_m}, 
x_{l_{m+1}} = x_j$ obtemos
\[
O(x_{l_1}) = O(x_{l_2}), \; 
O(x_{l_2}) = O(x_{l_3}), \; \ldots \;
O(x_{l_m}) = O(x_{l_{m+1}}).
\]
A partir daí concluímos que $O_i = O_j$ se $x_i, x_j$ pertencerem a mesma
componente conexa.

Finalmente, para encerrarmos a demonstração do Teorema, devemos mostrar
que $(iv) \implies (i)$.
Sabendo que $H_{ij} \neq 0$ significa que os pontos $x_i, x_j$
 pertencem a mesma componente conexa que, pelo item $(iv)$, significa
$O_i = O_j$, obtemos
\[
\bm{OH} = \left( \sum_{m=1}^n O_{km} H_{ml} \right)_{kl},
\; \text{com} \;
\sum_{m=1}^n O_{km} H_{ml} =
\begin{cases}
O_k H_{kl}, & \text{se } H_{kl} \neq 0\\
0, & \text{caso contrário.}
\end{cases}
\]
Pelo  item $(iv)$, a somatória acima fica igual a
\[
\sum_{m=1}^n O_{km} H_{ml} =
\begin{cases}
O_l H_{kl}, & \text{se } H_{kl} \neq 0\\
0, & \text{caso contrário.}
\end{cases}
\]
Porém, o produto $\bm{HO}$ é dado por
\[
\bm{HO} = \left( \sum_{m=1}^n H_{km}O_{ml} \right)_{kl},
\; \text{com} \;
\sum_{m=1}^n H_{km}O_{ml} =
\begin{cases}
O_l H_{kl}, & \text{se } H_{kl} \neq 0\\
0, & \text{caso contrário.}
\end{cases}
\]
Ou seja, $[\bm{O}, \bm{H}] = \bm{0}$.
\end{proof}

Em posse deste Teorema enunciamos o Teorema de Noether (cuja
demonstração resulta imediatamente do Teorema acima):
\begin{teo}[Teorema de Noether, versão estocástica]
Dado um conjunto finito $X$ de cardinalidade
$|X| = n$, uma L-matriz $\bm{H} \in \mathbb{R}^{n \times n}$,
e um observável $O: X \rightarrow \mathbb{R}$. 
Então, a matriz do observável $O$ comuta com a L-matriz $\bm{H}$,
i.e., $[\bm{O}, \bm{H}]$, se e somente se para toda medida de
probabilidade $\phi: X \rightarrow [0,1]$ satisfazendo
$\frac{d}{dt}\phi(t) = \bm{H}\phi(t)$ tivermos que o valor
esperado e o segundo momento do observável $O$ são invariantes
com  o tempo, i.e.,
\[\frac{d}{dt} \langle \bm{O}, \phi(t) \rangle \;=\;
\langle \bm{O}^2, \phi(t) \rangle \; = 0.\]
\end{teo}
%(rever o negrito da notação!!!)

% \endfold

\chapter{Conclus\~ao}

Inicialmente, neste trabalho, generalizamos (com relação a dimensão das matrizes
em estudo), quando possível, os resultados propostos
em \cite{paper1} envolvendo os grupos de matrizes de Lie, relativo as matrizes estocásticas, e seus espaços tangentes.
Como resultado principal, mostramos que a componente conexa da identidade, denotada por $\mathcal{G}^0$
admite a seguinte decomposição
\[ \mathcal{G}^0 = \bigcup_{t \in \mathbb{R}}  e^{Qt} \mathcal{H}, \]
para $Q$ uma L-matriz, de dimensão $n \times n$ ($n \geq 2$), e $\mathcal{H}$ um subgrupo normal da
componente conexa. Além do mais, mostramos que a componente conexa $\mathcal{G}^0$ é igual a
todas as matrizes estocásticas $n \times n$ de determinante positivo.

Em seguida, utilizando os resultados propostos em \cite{paper2}, obtivemos uma matriz
geradora $\bm{L}'$ a partir do espaço tangente $T_1(\mathcal{G})$ do grupo das matrizes de Lie relativo
as matrizes estocásticas de dimensão $3 \times 3$. O processo para obtenção da matriz $\bm{L}'$ baseou-se
em, primeiro, calcular uma subálgebra semi-simples $\mathfrak{s}$ de $T_1(\mathcal{G})$, o qual garante uma fórmula
fechada para o cálculo de seu elemento do Casimir $C$. Em posse da matriz $C$, estabelecemos a 
álgebra universal envelopante de $\mathfrak{s}$ e, através do coproduto
\[ \Delta(x) = I_n \otimes x + x \otimes I_n, \]
com $I_n$ a matriz identidade $n \times n$,
%e o qual confere a álgebra universal envelopante 
%estrutura de biálgebra,
%observamos a possibilidade de se gerar uma matriz geradora de um processo
%markoviano a tempo contínuo, calculando-se o coproduto do elemento do Casimir.
calculamos a matriz $\bm{L}'$, dada por $\bm{L}' = \Delta(C)$.

Na parte final do trabalho estabelecemos um sistema de partículas de dois sítios, cujo gerador é
definido   pela matriz geradora $\bm{L}'$,
e construímos um operador de autodualidade para o sistema. Em seguida, mostramos ser possível
estender o sistema de duas para N partículas e, consequentemente, estendemos a função de autodualidade anterior
para o nosso novo sistema de N partículas.

%%%  Se não for utilizar apêndices, comentar a linha abaixo.
% \\beginfold Apendice
\backmatter
\appendix
\chapter{Ap\^endice}

% \beginfold teorema 5.7
\chapter{A. Demonstra\c{c}\~ao da Proposi\c{c}\~ao $5.1$  } \label{myproof}

\section{Nota\c{c}\~ao}

Seja $GL(n+1,\mathbb{R})$ o grupo linear geral das matrizes
$(n+1) \times (n+1)$.
Denotemos por $\mathbf{W}^{(1)}, \ldots, \mathbf{W}^{(n)}
\in GL(n+1,\mathbb{R})$  as matrizes que podem ser escritas 
da seguinte forma:

\[
\mathbf{W}^{(1)} = 
\begin{pmatrix}
1-a_1 & a_1   & 0 & 0 & 0 & \cdots & 0 & 0 & 0\\
b_1   & 1-b_1 & 0 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 1 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 0 & 1 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 0 & 0 & 1 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots
& \vdots & \vdots\\
0   & 0   & 0 & 0 & 0 & \cdots & 1 & 0 & 0\\
0   & 0   & 0 & 0 & 0 & \cdots & 0 & 1 & 0\\
0   & 0   & 0 & 0 & 0 & \cdots & 0 & 0 & 1\\
\end{pmatrix}
,
\]

\[
\mathbf{W}^{(2)}=
\begin{pmatrix}
1 & 0   & 0 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & 1-a_2 & a_2 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & b_2   & 1-b_2 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 0 & 1 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 0 & 0 & 1 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots
& \vdots & \vdots\\
0   & 0   & 0 & 0 & 0 & \cdots & 1 & 0 & 0\\
0   & 0   & 0 & 0 & 0 & \cdots & 0 & 1 & 0\\
0   & 0   & 0 & 0 & 0 & \cdots & 0 & 0 & 1\\
\end{pmatrix}
,
\]

\[
\mathbf{W}^{(3)}=
\begin{pmatrix}
1 & 0   & 0 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & 1 & 0 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 1-a_3 & a_3 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & b_3 & 1-b_3 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 0 & 0 & 1 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots
& \vdots & \vdots\\
0   & 0   & 0 & 0 & 0 & \cdots & 1 & 0 & 0\\
0   & 0   & 0 & 0 & 0 & \cdots & 0 & 1 & 0\\
0   & 0   & 0 & 0 & 0 & \cdots & 0 & 0 & 1\\
\end{pmatrix}
,
\]

\begin{align*}
 \vdots %\\[-0.45cm] \vdots
\end{align*}

\[
\mathbf{W}^{(n)}=
\begin{pmatrix}
1 & 0   & 0 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & 1 & 0 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 1 & 0 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 0 & 1 & 0 & \cdots & 0 & 0 & 0\\
0   & 0   & 0 & 0 & 1 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots
& \vdots & \vdots\\
0   & 0   & 0 & 0 & 0 & \cdots & 1 & 0 & 0\\
0   & 0   & 0 & 0 & 0 & \cdots & 0 & 1-a_n & a_n\\
0   & 0   & 0 & 0 & 0 & \cdots & 0 & b_n & 1-b_n\\
\end{pmatrix}
,
\]
com $a_1, \ldots, a_n, b_1, \ldots, b_n$ valores reais.

Em outras palavras,
as matrizes $\mathbf{W}^{(1)}, \ldots, \mathbf{W}^{(n)}$ podem ser 
vistas como as matrizes de blocos (aqui, a matriz
$\mathbf{I}_{k \times k}$ denota a matriz identidade, de $k$ linhas
e $l$ colunas, enquanto que $\mathbf{0}_{k \times l}$ denota a 
matriz de zeros, com $k$ linhas e $l$ colunas):
\[
\mathbf{W}^{(1)} = 
\begin{pmatrix}
\mathbf{P}^{(1)}_{2 \times 2} & \mathbf{0}_{2 \times (n-1)}\\
\mathbf{0}_{(n-1) \times (n-1)} & \mathbf{I}_{(n-1)\times (n-1)}
\end{pmatrix}
,
\]

\[
\mathbf{W}^{(i)} = 
\begin{pmatrix}
\mathbf{I}_{(i-1)\times (i-1)}& \mathbf{0}_{(i-1) \times 2} &
\mathbf{0}_{(i-1) \times (n-i)} \\
\mathbf{0}_{2 \times (i-1)} & \mathbf{P}^{(i)}_{2 \times 2} &
 \mathbf{0}_{2 \times (n-i)}\\
\mathbf{0}_{(n-i) \times (i-1)} & \mathbf{0}_{(n-i) \times (2)}
 & \mathbf{I}_{(n-i)\times (n-i)}
\end{pmatrix}
,
\]
com $ i \in \{2, 3, \ldots, n-1\}$ e

\[
\mathbf{W}^{(n)} = 
\begin{pmatrix}
\mathbf{I}_{(n-1)\times (n-1)}& \mathbf{0}_{(n-1) \times 2} \\
\mathbf{0}_{2 \times (n-1)}   & \mathbf{P}^{(n)}_{2 \times 2}   
\end{pmatrix}
,
\]
com $\mathbf{P}^{(1)}_{2 \times 2}, \ldots, 
\mathbf{P}^{(n)}_{2 \times 2}$ matrizes estocásticas $ 2 \times 2$.

Dizemos que uma matriz $\mathbf{X}$ é do tipo $\mathbf{W}^{(i)}$,
para $i \in \{1, \ldots, n\}$, se $\mathbf{X}$ puder ser escrita
como
$\mathbf{X}= \mathbf{W}^{(i)}$.

%Tome $Q$ a matriz
%\[
%\begin{pmatrix}
%Q_{1,1} & Q_{1,2} & \cdots & Q_{1,n-1} & Q_{1,n} & Q_{1,n+1}\\
%Q_{2,1} & Q_{2,2} & \cdots & Q_{2,n-1} & Q_{2,n} & Q_{2,n+1}\\
%\vdots  & \vdots  & \ddots & \vdots    & \vdots  & \vdots \\
%Q_{n+1,1} & Q_{n+1,n+1} & \cdots & Q_{n+1,n-1} &
% Q_{n+1,n} & Q_{n+1,n+1}\\
%\end{pmatrix}
%.
%\]
\section{Demonstra\c{c}\~ao}
Seja $\mathcal{G}_n$ o grupo das matrizes estocásticas 
$n \times n$ dado por
\[
\mathcal{G}_n = \left\{ \mathbf{X} \in GL(n,\mathbb{R}); \;
					  \mathbf{X} \cdot \mathbf{1} = \mathbf{1} \right\},
\]
com $\mathbf{1} = [1, 1, \ldots, 1]^T$ 
o vetor cujas entradas são os inteiros $1$.
Mostremos a seguinte Proposição:

\begin{prop*}
O conjunto das matrizes estocásticas de determinante positivo
\[
\left\{ \mathbf{X} \in \mathcal{G}_n; \; \det(\mathbf{X}) > 0 \right\}
\]
é conexo por caminhos.
%Seja $\mathcal{G}_n^0$ a componente conexa da identidade no grupo
%das matrizes estocásticas $\mathcal{G}_n$. Então, 
%\[ \mathcal{G}^0 = \left\{ X \in \mathcal{G}; \; \det(X) > 0 
%	\right\}
%\]
\end{prop*}

Antes de provarmos a Proposição acima, provemos o seguinte Lema:
\begin{lema*}
Seja $\mathbf{Q} \in \mathcal{G}_{(n+1)}$ uma matriz de 
determinante positivo e $n \geq 2$. Então, existe um 
sequência finita de matrizes de determinante positivo
 $\mathbf{X}_1, 
\mathbf{X}_2, \ldots, \mathbf{X}_m$ tal que
%$\mathbf{W}^{(1)}_1, 
%\mathbf{W}^{(1)}_2,
%\mathbf{W}^{(2)}_1,
%\mathbf{W}^{(2)}_2, \ldots,
%\mathbf{W}^{(n)}_1,
%\mathbf{W}^{(n)}_2$ tal que
\[
\mathbf{Q} \cdot \mathbf{X}_1 \cdot
\mathbf{X}_2 \cdots
\mathbf{X}_m
=
\begin{pmatrix}
1 & 0 & \cdots & 0\\
y_{2,1} & y_{2,2} & \cdots & y_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots\\
y_{n+1,1} & y_{n+1,2} & \cdots & y_{n+1,n+1}
\end{pmatrix}
.
\]
Além do mais, as matrizes $\mathbf{X}_i$, $ i \in 
\{1, \ldots, m\}$, são do tipo
\[
\mathbf{X}_i = \mathbf{W}^{(j)},
\]
para algum $j \in \{1, \ldots, n\}$.
\end{lema*}

\begin{proof}
Tome $\mathbf{Q}
\in \mathcal{G}_{(n+1)}$ ($n \geq 2$) a matriz de 
determinante positivo 
\[
\mathbf{Q} =
\begin{pmatrix}
Q_{1,1} & Q_{1,2} & \cdots & Q_{1,n-1} & Q_{1,n} & Q_{1,n+1}\\
Q_{2,1} & Q_{2,2} & \cdots & Q_{2,n-1} & Q_{2,n} & Q_{2,n+1}\\
\vdots  & \vdots  & \ddots & \vdots    & \vdots  & \vdots \\
Q_{n+1,1} & Q_{n+1,n+1} & \cdots & Q_{n+1,n-1} &
 Q_{n+1,n} & Q_{n+1,n+1}\\
\end{pmatrix}
.
\]

Nosso primeiro passo é mostrar que existe as matrizes 
$\mathbf{X}_1$ e $\mathbf{X}_2$ 
tal que
\[
\mathbf{Q} \cdot \mathbf{X}_1 \cdot
\mathbf{X}_2 =
\mathbf{Q}^{(1)},
\]
com 
\[
\mathbf{Q}^{(1)} = 
\begin{pmatrix}
Q^{(1)}_{1,1} & Q^{(1)}_{1,2} & \cdots & Q^{(1)}_{1,n} &0\\
Q^{(1)}_{2,1} & Q^{(1)}_{2,2} & \cdots & Q^{(1)}_{2,n+1}&
Q^{(1)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots \\
Q^{(1)}_{n+1,1} & Q^{(1)}_{n+1,2} & \cdots & Q^{(1)}_{n+1,n+1} &
Q^{(1)}_{n+1,n+1}
\end{pmatrix}
.
\]

Para mostrarmos esta igualdade devemos observar as seguintes
possibilidades:
\begin{enumerate}[(i)]
\item  $Q_{1,n} + Q_{1, n+1} > 0$. \hfill \\
Neste caso, tome a matriz $\mathbf{X}_1$ dada por
\[
\mathbf{X}_1 = 
\mathbf{W}^{(n)} = 
\begin{pmatrix}
\mathbf{I}_{(n-1)\times (n-1)}& \mathbf{0}_{(n-1) \times 2} \\
\mathbf{0}_{2 \times (n-1)}   & \mathbf{P}^{(n)}_{2 \times 2}   
\end{pmatrix}
,
\]
com 
\[
\mathbf{P}^{(n)}_{2 \times 2}
=
\begin{pmatrix}
1 + Q_{1, n+1} & -Q_{1, n+1} \\
1 - Q_{1,n}    & Q_{1,n}
\end{pmatrix}
\]
e $\bm{X}_2 = \bm{I}_{(n+1) \times (n+1)}$.
Desta forma,
\[
\bm{Q} \cdot \bm{X}_1 \cdot \bm{X}_2 =
\mathbf{Q}^{(1)} = 
\begin{pmatrix}
Q^{(1)}_{1,1} & Q^{(1)}_{1,2} & \cdots & Q^{(1)}_{1,n} &0\\
Q^{(1)}_{2,1} & Q^{(1)}_{2,2} & \cdots & Q^{(1)}_{2,n+1}&
Q^{(1)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots \\
Q^{(1)}_{n+1,1} & Q^{(1)}_{n+1,2} & \cdots & Q^{(1)}_{n+1,n+1} &
Q^{(1)}_{n+1,n+1}
\end{pmatrix}
,
\]
com $\det(\mathbf{X}_1) = Q_{1,n} + Q_{1, n+1} > 0$.

\item  $Q_{1,n} + Q_{1, n+1} < 0$. \hfill \\
Para esta desigualdade, temos algo similar ao exposto
acima no  item (i).
Tome a matriz $\mathbf{X}_1$ dada por
\[
\mathbf{X}_1 = 
\mathbf{W}^{(n)} = 
\begin{pmatrix}
\mathbf{I}_{(n-1)\times (n-1)}& \mathbf{0}_{(n-1) \times 2} \\
\mathbf{0}_{2 \times (n-1)}   & \mathbf{P}^{(n)}_{2 \times 2}   
\end{pmatrix}
,
\]
com 
\[
\mathbf{P}^{(n)}_{2 \times 2}
=
\begin{pmatrix}
1 - Q_{1, n+1} & Q_{1, n+1} \\
1 + Q_{1,n}    & -Q_{1,n}
\end{pmatrix}
\]
e $\bm{X}_2 = \bm{I}_{(n+1) \times (n+1)}$.
Desta forma,
\[
\bm{Q} \cdot \bm{X}_1 \cdot \bm{X}_2 =
\mathbf{Q}^{(1)} = 
\begin{pmatrix}
Q^{(1)}_{1,1} & Q^{(1)}_{1,2} & \cdots & Q^{(1)}_{1,n} &0\\
Q^{(1)}_{2,1} & Q^{(1)}_{2,2} & \cdots & Q^{(1)}_{2,n+1}&
Q^{(1)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots \\
Q^{(1)}_{n+1,1} & Q^{(1)}_{n+1,2} & \cdots & Q^{(1)}_{n+1,n+1} &
Q^{(1)}_{n+1,n+1}
\end{pmatrix}
,
\]
com $\det(\mathbf{X}_1) = -( Q_{1,n} + Q_{1, n+1}) > 0$.

\item  $Q_{1,n} + Q_{1, n+1} = 0$. \hfill \\
Dada a restrição $Q_{1,n} = -Q_{1, n+1} $, consideramos a matriz
$\mathbf{X}_1$ dada por
\[
\mathbf{X}_1 = \mathbf{W}^{(n-1)}=
\begin{pmatrix}
\mathbf{I}_{(n-2)\times (n-2)}& \mathbf{0}_{(n-2) \times 2} &
\mathbf{0}_{(n-2) \times 1} \\
\mathbf{0}_{2 \times (n-2)} & \mathbf{P}^{(n-1)}_{2 \times 2} &
 \mathbf{0}_{2 \times 1}\\
\mathbf{0}_{1 \times (n-2)} & \mathbf{0}_{1 \times 2}
 & \mathbf{I}_{1\times 1}
\end{pmatrix}
,
\]
com
\[
\mathbf{P}^{(n-1)}_{2 \times 2}=
\begin{pmatrix}
1 & 0\\
0,5 & 0,5
\end{pmatrix}
.
\]
Logo, a matriz
\[
\mathbf{\widetilde{Q}} := \mathbf{Q} \cdot \mathbf{X}_1
\]
satisfaz a condição de que 
$\mathbf{\widetilde{Q}}_{1,n} + \mathbf{\widetilde{Q}}_{1,n+1}
\neq 0$. Portanto, aplicando o item (i) ou (ii) para 
a matriz $\mathbf{\widetilde{Q}}$, obtemos uma matriz
$\mathbf{X}_2$ do tipo $\mathbf{W}^{(n)}$, i.e.,
\[
\mathbf{X}_2= \mathbf{W}^{(n)} =
\begin{pmatrix}
\mathbf{I}_{(n-1)\times (n-1)}& \mathbf{0}_{(n-1) \times 2} \\
\mathbf{0}_{2 \times (n-1)}   & \mathbf{P}^{(n)}_{2 \times 2}   
\end{pmatrix}
\]
tal que
\[
\mathbf{Q}^{(1)} = \mathbf{\widetilde{Q}} \cdot \mathbf{X}_2 =
\mathbf{Q} \cdot \mathbf{X}_1 \cdot \mathbf{X}_2 =
\begin{pmatrix}
Q^{(1)}_{1,1} & Q^{(1)}_{1,2} & \cdots & Q^{(1)}_{1,n} &0\\
Q^{(1)}_{2,1} & Q^{(1)}_{2,2} & \cdots & Q^{(1)}_{2,n}&
Q^{(1)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots \\
Q^{(1)}_{n+1,1} & Q^{(1)}_{n+1,2} & \cdots & Q^{(1)}_{n+1,n} &
Q^{(1)}_{n+1,n+1}
\end{pmatrix}
.
\]

\end{enumerate}

Dada a matriz $\mathbf{Q}^{(1)}$
realizamos o mesmo procedimento feito acima. Ou seja, dependendo
do valor de $\mathbf{Q}^{(1)}_{1,n} + \mathbf{Q}^{(1)}_{1,n-1}$,
obtemos matrizes $\mathbf{X}_{3}, \mathbf{X}_{4}$ tal que
\[
\mathbf{Q}^{(1)} \cdot \mathbf{X}_3 \cdot
\mathbf{X}_4 :=
\mathbf{Q}^{(2)},
\]
com 
\[
\mathbf{Q}^{(2)} = 
\begin{pmatrix}
Q^{(2)}_{1,1} & Q^{(2)}_{1,2} & \cdots & Q^{(2)}_{1,n-1} & 0&0\\
Q^{(2)}_{2,1} & Q^{(2)}_{2,2} & \cdots & Q^{(2)}_{2,n-1}&
 Q^{(2)}_{2,n}&
Q^{(2)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
Q^{(2)}_{n+1,1} & Q^{(2)}_{n+1,2} & \cdots & Q^{(2)}_{n+1,n-1}&
 Q^{(2)}_{n+1,n} &
Q^{(2)}_{n+1,n+1}
\end{pmatrix}
.
\]

Para checarmos esta igualdade devemos, novamente,
 observar as seguintes
possibilidades (mesmo o procedimento da demonstração sendo similar ao anterior, iremos
checar os três casos possíveis para ilustrar a ideia dos cálculos):
\begin{enumerate}[(i)]
\item  $\mathbf{Q}^{(1)}_{1,n} + \mathbf{Q}^{(1)}_{1,n-1} > 0$. 
\hfill \\
Tome a matriz %$\mathbf{X}_3$ dada por
\[
\mathbf{X}_3 = 
\mathbf{W}^{(n-1)} = 
\begin{pmatrix}
\mathbf{I}_{(n-2)\times (n-2)}& \mathbf{0}_{(n-2) \times 2} &
\mathbf{0}_{(n-2) \times 1} \\
\mathbf{0}_{2 \times (n-2)} & \mathbf{P}^{(n-1)}_{2 \times 2} &
 \mathbf{0}_{2 \times 1}\\
\mathbf{0}_{1 \times (n-2)} & \mathbf{0}_{1 \times 2}
 & \mathbf{I}_{1\times 1}
\end{pmatrix}
,
\]
com 
\[
\mathbf{P}^{(n-1)}_{2 \times 2}
=
\begin{pmatrix}
1 + Q_{1, n+1} & -Q_{1, n+1} \\
1 - Q_{1,n}    & Q_{1,n}
\end{pmatrix}
\]
e $\bm{X}_4 = \bm{I}_{(n+1) \times (n+1)}$.
Logo,
\[
\bm{Q}^{(1)} \cdot \bm{X}_3 \cdot \bm{X}_4 =
\mathbf{Q}^{(2)} = 
\begin{pmatrix}
Q^{(2)}_{1,1} & Q^{(2)}_{1,2} & \cdots & Q^{(2)}_{1,n-1} & 0&0\\
Q^{(2)}_{2,1} & Q^{(2)}_{2,2} & \cdots & Q^{(2)}_{2,n-1}&
 Q^{(2)}_{2,n}&
Q^{(2)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
Q^{(2)}_{n+1,1} & Q^{(2)}_{n+1,2} & \cdots & Q^{(2)}_{n+1,n-1}&
 Q^{(2)}_{n+1,n} &
Q^{(2)}_{n+1,n+1}
\end{pmatrix}
,
\]
com $\det(\mathbf{X}_3) = Q^{(1)}_{1,n-1} + Q_{1, n} > 0$.

\item  $\mathbf{Q}^{(1)}_{1,n} + \mathbf{Q}^{(1)}_{1,n-1} < 0$. 
\hfill \\
Para esta desigualdade %, temos algo similar ao exposto
%acima no  item (i).
tome a matriz $\mathbf{X}_3$ dada por
\[
\mathbf{X}_3 = 
\mathbf{W}^{(n-1)} = 
\begin{pmatrix}
\mathbf{I}_{(n-2)\times (n-2)}& \mathbf{0}_{(n-2) \times 2} &
\mathbf{0}_{(n-2) \times 1} \\
\mathbf{0}_{2 \times (n-2)} & \mathbf{P}^{(n-1)}_{2 \times 2} &
 \mathbf{0}_{2 \times 1}\\
\mathbf{0}_{1 \times (n-2)} & \mathbf{0}_{1 \times 2}
 & \mathbf{I}_{1\times 1}
\end{pmatrix}
,
\]
com 
\[
\mathbf{P}^{(n-1)}_{2 \times 2}
=
\begin{pmatrix}
1 - Q_{1, n+1} & Q_{1, n+1} \\
1 + Q_{1,n}    & -Q_{1,n}
\end{pmatrix}
\]
e $\bm{X}_4 = \bm{I}_{(n+1) \times (n+1)}$.
Sendo assim,
\[
\bm{Q}^{(1)} \cdot \bm{X}_3 \cdot \bm{X}_4 =
\mathbf{Q}^{(2)} = 
\begin{pmatrix}
Q^{(2)}_{1,1} & Q^{(2)}_{1,2} & \cdots & Q^{(2)}_{1,n-1} & 0&0\\
Q^{(2)}_{2,1} & Q^{(2)}_{2,2} & \cdots & Q^{(2)}_{2,n-1}&
 Q^{(2)}_{2,n}&
Q^{(2)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
Q^{(2)}_{n+1,1} & Q^{(2)}_{n+1,2} & \cdots & Q^{(2)}_{n+1,n-1}&
 Q^{(2)}_{n+1,n} &
Q^{(2)}_{n+1,n+1}
\end{pmatrix}
,
\]
com 
$\det(\mathbf{X}_3) = -( Q^{(1)}_{1,n-1} + Q^{(1)}_{1, n}) > 0$.

\item  $\mathbf{Q}^{(1)}_{1,n} + \mathbf{Q}^{(1)}_{1,n-1} = 0$. 
\hfill \\
Dada a restrição $Q^{(1)}_{1,n-1} = -Q^{(1)}_{1, n} $,
 tomamos a matriz
%$\mathbf{X}_3$ dada por
\[
\mathbf{X}_3 = \mathbf{W}^{(n-2)}=
\begin{pmatrix}
\mathbf{I}_{(n-3)\times (n-3)}& \mathbf{0}_{(n-3) \times 2} &
\mathbf{0}_{(n-3) \times 2} \\
\mathbf{0}_{2 \times (n-3)} & \mathbf{P}^{(n-2)}_{2 \times 2} &
 \mathbf{0}_{2 \times 2}\\
\mathbf{0}_{2 \times (n-3)} & \mathbf{0}_{2 \times 2}
 & \mathbf{I}_{2\times 2}
\end{pmatrix}
,
\]
com
\[
\mathbf{P}^{(n-2)}_{2 \times 2}=
\begin{pmatrix}
1 & 0\\
0,5 & 0,5
\end{pmatrix}
.
\]
Portanto, a matriz
\[
\mathbf{\widetilde{Q}} := \mathbf{Q}^{(1)} \cdot \mathbf{X}_3
\]
satisfaz a condição de que 
$\mathbf{\widetilde{Q}}_{1,n-1} + \mathbf{\widetilde{Q}}_{1,n}
\neq 0$. Consequentemente, aplicando o item (i) ou (ii) para 
a matriz $\mathbf{\widetilde{Q}}$, obtemos uma matriz
$\mathbf{X}_4$ do tipo $\mathbf{W}^{(n-1)}$
\[
\mathbf{X}_2= \mathbf{W}^{(n)} =
\begin{pmatrix}
\mathbf{I}_{(n-1)\times (n-1)}& \mathbf{0}_{(n-1) \times 2} \\
\mathbf{0}_{2 \times (n-1)}   & \mathbf{P}^{(n)}_{2 \times 2}   
\end{pmatrix}
\]
tal que
\[
\mathbf{Q}^{(2)} := \mathbf{\widetilde{Q}} \cdot \mathbf{X}_4 =
\mathbf{Q}^{(1)} \cdot \mathbf{X}_3 \cdot \mathbf{X}_4 =
\begin{pmatrix}
Q^{(2)}_{1,1} & Q^{(2)}_{1,2} & \cdots & Q^{(2)}_{1,n-1} & 0&0\\
Q^{(2)}_{2,1} & Q^{(2)}_{2,2} & \cdots & Q^{(2)}_{2,n-1}&
 Q^{(2)}_{2,n}&
Q^{(2)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
Q^{(2)}_{n+1,1} & Q^{(2)}_{n+1,2} & \cdots & Q^{(2)}_{n+1,n-1}&
 Q^{(2)}_{n+1,n} &
Q^{(2)}_{n+1,n+1}
\end{pmatrix}
,
\]
\end{enumerate}

Podemos continuar esse processo indutivamente obtendo a 
sequência de matrizes: $\mathbf{Q}^{(1)}$, $\mathbf{Q}^{(2)}$,
\ldots, $\mathbf{Q}^{(n-1)}$.
A matriz $\mathbf{Q}^{(n-1)}$ será da forma:
\[
\mathbf{Q}^{(n-1)}
\begin{pmatrix}
Q^{(n-1)}_{1,1} & Q^{(n-1)}_{1,2} &0 &  \cdots & 0 & 0&0\\
Q^{(n-1)}_{2,1} & Q^{(n-1)}_{2,2} &Q^{(n-1)}_{2,3} & \cdots &
 Q^{(n-1)}_{2,n-1}&
 Q^{(n-1)}_{2,n}&
Q^{(n-1)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
Q^{(n-1)}_{n+1,1} & Q^{(n-1)}_{n+1,2} & Q^{(n-1)}_{2,3} & 
\cdots & Q^{(n-1)}_{n+1,n-1}&
 Q^{(n-1)}_{n+1,n} &
Q^{(n-1)}_{n+1,n+1}
\end{pmatrix}
.
\]
Dado o fato de $\mathbf{Q}^{(n-1)}$ ser uma matriz estocástica
temos que 
$Q_{1,1}^{(n-1)} + Q_{1, 2}^{(n-1)} = 1$, 
ou seja, como já visto no item (i) ou (ii) anteriores, existe
uma matriz $\mathbf{X}_{(n-1) \cdot 2  + 1}$ de determinante
positivo e da forma $\mathbf{W}^{(1)}$ tal que

\[
\mathbf{Q}^{(n)} := 
\mathbf{Q}^{(n-1)} \cdot \mathbf{X}_{(n-1) \cdot 2  + 1} =
\begin{pmatrix}
1 & 0 &0 &  \cdots & 0 & 0&0\\
Q^{(n)}_{2,1} & Q^{(n)}_{2,2} &Q^{(n)}_{2,3} & \cdots &
 Q^{(n)}_{2,n-1}&
 Q^{(n)}_{2,n}&
Q^{(n)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
Q^{(n)}_{n+1,1} & Q^{(n)}_{n+1,2} & Q^{(n)}_{2,3} & 
\cdots & Q^{(n)}_{n+1,n-1}&
 Q^{(n)}_{n+1,n} &
Q^{(n)}_{n+1,n+1}
\end{pmatrix}
.
\]

Sendo assim, obtemos uma sequência de matrizes $\mathbf{X}_1$,
$\mathbf{X}_2$, \ldots, $\mathbf{X}_{(n-1) \cdot 2  + 1}$
de determinante positivo e do tipo $\mathbf{W}^{(i)}$,
para $i = 1, 2, \ldots, (n-1)\cdot 2 +1$, satisfazendo
\[
\mathbf{Q} \cdot \mathbf{X}_1,
\mathbf{X}_2, \ldots, \mathbf{X}_{(n-1) \cdot 2  + 1}=
\mathbf{Q}^{(n)} = 
\begin{pmatrix}
1 & 0 &0 &  \cdots & 0 & 0&0\\
Q^{(n)}_{2,1} & Q^{(n)}_{2,2} &Q^{(n)}_{2,3} & \cdots &
 Q^{(n)}_{2,n-1}&
 Q^{(n)}_{2,n}&
Q^{(n)}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
Q^{(n)}_{n+1,1} & Q^{(n)}_{n+1,2} & Q^{(n)}_{2,3} & 
\cdots & Q^{(n)}_{n+1,n-1}&
 Q^{(n)}_{n+1,n} &
Q^{(n)}_{n+1,n+1}
\end{pmatrix}
.
\]

\end{proof}


Provemos agora a Proposição.
\begin{prop*}
O conjunto das matrizes estocásticas de determinante positivo
\[
\left\{ \mathbf{X} \in \mathcal{G}_n; \; \det(\mathbf{X}) > 0 \right\}
\]
é conexo por caminhos.
\end{prop*}


\begin{proof}
Façamos a prova por indução em relação a $n$, cujo valor
denota a dimensão das matrizes estocásticas $n \times n$. 

Para $ n = 2$ sabemos que este 
resultado é verdadeiro (vide \cite{paper1}).

Suponhamos que o resultado valha para $n$ qualquer e 
analisemos o caso $\mathcal{G}_{n+1}$. 
Dada uma matriz $\mathbf{Q} \in \mathcal{G}_{n+1}$
de determinante positivo, sabemos, pelo Lema anterior,
 que  existe uma sequência finita
de matrizes de determinante positivo $\mathbf{X}_1, \ldots,
\mathbf{X}_m$ tal que
\[
\mathbf{Q} \cdot \mathbf{X}_m \cdot \mathbf{X}_{m-1}  \cdots
 \mathbf{X}_1 = 
\begin{pmatrix}
1 & 0 & \cdots & 0\\
Q'_{2,1} & Q'_{2,2} & \cdots & Q'_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots\\
Q'_{n+1,1} & Q'_{n+1,2} & \cdots & Q'_{n+1,n+1}
\end{pmatrix}
:= \mathbf{Q'}.
\]

Dada a matriz $\mathbf{Q'}$ teremos que realizar algumas 
transformações na matriz tal que tenhamos a
 seguinte desigualdade:
\[
\mathbf{Q'}_{i,1} < 1, \forall i = 2, 3, \ldots, n+1.
\]
Tal propriedade será útil para nós. Para isso, 
devemos tomar o conjunto
\[
\mathcal{I} := \{ i \in \{2, 3, \ldots, n+1\}; \;
Q'_{i,1} \geq Q'_{j,1} \forall j \in \{2, 3, \ldots, n+1\}\;\}
%i_0 = \argmax_{i = 2, \ldots, n+1} Q'_{i, 1} 
\]
que representa as linhas $i$ tais que $Q'_{i,1}$ é maximal.
Agora, definimos por $i_0 \in \mathcal{I}$ como uma linha qualquer
cujo valor $Q'_{i,1}$ é maximal
e avaliamos as seguintes possibilidades:
\begin{enumerate}[(i)]
\item $ Q_{i_0,1}' > 1$

Nesta situação podemos ter dois casos:
\[
\mathcal{I} = \{2, 3, \ldots, n+1\} \, \text{ ou } \,
 \{2, 3, \ldots, n+1\} \setminus \mathcal{I} \neq \emptyset.
\]
Consideremos inicialmente o caso em que
$ \{2, 3, \ldots, n+1\} \setminus \mathcal{I} \neq \emptyset$. 
Para todo 
$l \in \{2, 3, \ldots, n+1\} \setminus \mathcal{I}$
tomamos a matriz estocástica
\[
\mathbf{Y}^{(l)} = \left(Y^{(l)}_{i,j}\right)_{i,j} \;
\text{ tal que }
Y^{(l)}_{i,j} = 
\begin{cases}
1 & \text{ se } i = j \text{ e } i \neq l;\\
\alpha &\text{ se } i = j = l;\\
1 - \alpha & \text{ se } i = l \text{ e } j = i_0;\\
0 & \text{ caso contrário },
\end{cases}
\]
com $\alpha$ satisfazendo: $ \alpha > \frac{Q'_{i_0,1} - 1}
{ Q'_{i_0,1} - Q'_{l,1}}$. Desta forma, se tomarmos o 
produto
\[
\mathbf{Y}^{(l)} \cdot \mathbf{Q'}
\]
que nada mais é que copiar a matriz $\mathbf{Q'}$ trocando a 
linha $l$ pela soma \mbox{$\alpha v_l + ( 1 - \alpha) v_{i_0}$},
com $v_l$ e $v_{i_0}$ vetores representando a linha $l$ e
a linha $i_0$ da matriz $\mathbf{Q'}$, respectivamente.
Observe também que $\det(\mathbf{Y}^{(l)}) > 0 $ e
que a matriz $\mathbf{Y}^{(l)}$ pode ser escrita pela
matriz de blocos:
\[
\mathbf{Y}^{(l)} = 
\begin{pmatrix}
 \mathbf{I}_{1 \times 1} &  \mathbf{0}_{1 \times n}\\
\mathbf{0}_{n \times 1} & \mathbf{A}^{(l)}_{n \times n} 
\end{pmatrix}
\]
com $\mathbf{A}^{(l)}_{n \times n}$ uma matriz estocástica 
de determinante positivo.

Ao tomarmos a multiplicação 
$\mathbf{Y}^{(l)} \cdot \mathbf{Q'}$ teremos, na primeira
coluna com a linha $l$, o valor
\[
(1 - \alpha) Q'_{i_0,1} + \alpha Q'_{l,1},
\]
o qual satisfaz 
$(1 - \alpha) Q'_{i_0,1} + \alpha Q'_{l,1} < 1$.
Sendo assim, definindo por 
\[r_1 := | \{2, 3, \ldots, n+1\} \setminus \mathcal{I}|,\]
 temos
uma sequência de matrizes estocásticas  de determinante
positivo $\mathbf{Y}^{(1)}, \mathbf{Y}^{(2)},$
$\ldots, \mathbf{Y}^{(r_1)}$ tal que a matriz
\[
 \mathbf{Q''} :=
\mathbf{Y}^{(r_1)} \cdot \mathbf{Y}^{(r_1 - 1)}  \cdots
 \mathbf{Y}^{(1)} \cdot \mathbf{Q'} = 
\begin{pmatrix}
1 & 0 & \cdots & 0\\
Q''_{2,1} & Q''_{2,2} & \cdots & Q''_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots\\
Q''_{n+1,1} & Q''_{n+1,2} & \cdots & Q''_{n+1,n+1}
\end{pmatrix}
\]
satisfaz $Q''_{l, 1} < 1$, para todo $l \in 
 \{2, 3, \ldots, n+1\} \setminus \mathcal{I}$.

Resta agora avaliar as linhas $k \in \mathcal{I}$.
Para isto, fixe a linha $i_1 \in
 \{2, 3, \ldots, n+1\} \setminus \mathcal{I} $ da matriz 
$\mathbf{Q''}$, que satisfaz $Q''_{i_1, 1} < 1$.
O argumento a seguir é o mesmo descrito acima. 
Para cada $k \in \mathcal{I}$
definimos a matriz
\[
\mathbf{Z}^{(k)} = \left(Z^{(k)}_{i,j}\right) \;
\text{ tal que }
Z^{(k)}_{i,j} = 
\begin{cases}
1 & \text{ se } i = j \text{ e } i \neq k;\\
\alpha &\text{ se } i = j = k;\\
1 - \alpha & \text{ se } i = k \text{ e } j = i_1;\\
0 & \text{ caso contrário },
\end{cases}
\]
com $ 0 < \alpha < \frac{1 - Q''_{i_1, 1}}{Q''_{k,1} - 
Q''_{i_1,1}}$. Como anteriormente, $\mathbf{Z}^{(k)}$
será uma matriz estocástica de determinante positivo
o qual pode ser escrita como uma matriz de blocos:
\[
\mathbf{Z}^{(k)} = 
\begin{pmatrix}
 \mathbf{I}_{1 \times 1} &  \mathbf{0}_{1 \times n}\\
\mathbf{0}_{n \times 1} & \mathbf{A}^{(k)}_{n \times n}
\end{pmatrix}
\]
com $\mathbf{A}^{(k)}_{n \times n}$ uma matriz estocástica 
de determinante positivo. 

Finalmente, se multiplicarmos a matriz $\mathbf{Q''}$
pela matriz $\mathbf{Z}^{(k)}$, i.e.,
\[
	\mathbf{Z}^{(k)} \cdot \mathbf{Q''},
\]
teremos como entrada na linha $k$ e coluna $1$ o
valor:
\[
(1 - \alpha) Q''_{i_1,1} + \alpha Q''_{k,1} < 1.
\]
Desta forma, definindo $r_2 := | \mathcal{I} |$,
temos uma sequência de matrizes $\mathbf{Z}^{(1)},
\mathbf{Z}^{(2)}, \ldots, \mathbf{Z}^{(r_2)}$ tal
que, ao multiplicarmos $\mathbf{Q''}$ por estas matrizes, obtemos
\[
\mathbf{Q'''} : = 
\mathbf{Z}^{(r_2)} \cdot \mathbf{Z}^{(r_2 - 1)}  \cdots
 \mathbf{Z}^{(1)} \cdot \mathbf{Q''} 
\]
que satisfaz
\[
Q'''_{i,1} < 1, \, \forall i \in \{2, 3, \ldots, n+1\}.
\]

Assim encerramos o caso em que 
$\{2, 3, \ldots, n+1\} \setminus \mathcal{I}
\neq \emptyset$. Agora, consideremos quando
\[
\mathcal{I} = \{2, 3, \ldots, n+1\}.
\]
Observe que neste caso basta tomarmos a matriz (estocástica
e de determinante positivo)
\[
\mathbf{F} = (F_{i,j}) \;
\text{ tal que }
F_{i,j} = 
\begin{cases}
1 & \text{ se } i = j \text{ e } i \neq 2;\\
0,5 &\text{ se } i = j = 2;\\
0,5 & \text{ se } i = 2 \text{ e } j = 1;\\
0 & \text{ caso contrário }
\end{cases}
\]
pois, se a multiplicarmos por $\mathbf{Q'}$, i.e., se tomarmos
\[
\mathbf{F} \cdot \mathbf{Q'},
\]
teremos que a entrada da matriz $\bm{F} \cdot \bm{Q}'$ da linha $2$ e coluna $1$ será
menor que os outros valores das linhas $3, 4, \ldots
n+1$ da coluna $1$. Logo, ao invés de consideramos
a matriz $\mathbf{Q'}$, basta realizarmos o mesmo
procedimento feito no início do item (i) para a matriz 
$\mathbf{F} \cdot \mathbf{Q'}$.

%Resumindo. É possível encontrarmos uma sequência finita
%de matrizes estocásticas de determinante positivo
%$\mathbf{Y}^{(1)}, \mathbf{Y}^{(2)},
%\ldots, 
%\mathbf{Y}^{(p)}$ (aqui denoto todas as sequências de matrizes
%encontradas neste item por $\mathbf{Y}$ para facilitar
%a notação), que podem ser escritas como
%\[
%\mathbf{Y}^{(l)} = 
%\begin{pmatrix}
% \mathbf{I}_{1 \times 1} &  \mathbf{0}_{1 \times n}\\
%\mathbf{0}_{n \times 1} & \mathbf{A}^{(l)}_{n \times n}, 
%\end{pmatrix}
%\, \text{ ou } \,
%\mathbf{Y}^{(l)} = 
%\begin{pmatrix}
%\mathbf{A}^{(l)}_{n \times n} & \mathbf{0}_{n \times 1} \\
% \mathbf{0}_{1 \times n} & \mathbf{I}_{1 \times 1}
%\end{pmatrix}
%,
%\]
%com $\mathbf{A}^{(l)}_{n \times n}$ matrizes estocásticas 
%de determinante positivo, tal que a matriz
%\[
%\mathbf{\widetilde{Q}} := 
%\mathbf{Y}^{(p)} \cdot \mathbf{Y}^{(p-1)}
%\cdots \mathbf{Y}^{(1)} \cdot \mathbf{Q'}=
%\begin{pmatrix}
%1 & 0  &  \cdots & 0 \\
%\widetilde{Q}_{2,1} & \widetilde{Q}_{2,2}  & \cdots &
%\widetilde{Q}_{2,n+1}\\
%\vdots & \vdots & \ddots & \vdots \\
%\widetilde{Q}_{n+1,1} & \widetilde{Q}_{n+1,2} &  
%\cdots & 
%\widetilde{Q}_{n+1,n+1}
%\end{pmatrix}
%\]
%satisfaz $\widetilde{Q}_{i,1} < 1$, para todo
%$i = 2, 3, \ldots, n+1$.

\item $ Q_{i_0,1}' = 1$

Sabemos que deve haver algum $j_0 \in \{2, 3, \ldots, n+1\}$
tal que $Q_{i_0,j_0}' \neq 0$, pois, caso contrário, teríamos
que a primeira linha e  a linha $i_0$ da matriz $\mathbf{Q'}$
seriam iguais, ou seja, 	$\mathbf{Q'}$ teria determinante
zero e, portanto, a matriz $\mathbf{Q}$ também teria determinante
zero, porém, isto é falso por hipótese. Sabemos, também, que
$\sum_{j=2}^{n+1} Q'_{i_0,j}= 0$, pois $Q'_{i_0,1}=1$
 e $\mathbf{Q'}$
é uma matriz estocástica. Sendo assim, deve existir
$l_0, l_1 \in \{2, 3, \ldots, n+1\}$ tal que 
$Q'_{i_0,l_0} \neq 0$ e $Q'_{i_0,l_1} \neq 0$. 
Portanto, existe um elemento na 
linha $i_0$ da matriz $\mathbf{Q'}$ que é diferente
 de zero e que não
se posiciona na última coluna. 
Chamemos de $k_0$ tal elemento, i.e., 
$Q'_{i_0,k_0} \neq 0$ e $k_0 < n+1$.

Consideremos a matriz
\[
	\mathbf{Y} = \left(Y_{i,j}\right) \;
\text{ tal que }
Y_{i,j} = 
\begin{cases}
1 & \text{ se } i = j \text{ e } i \neq k_0;\\
1- \alpha &\text{ se } i = k_0 \text{ e } j = 1;\\
 \alpha &\text{ se } i = k_0 \text{ e } j = k_0;\\
0 & \text{ caso contrário },
\end{cases}
\]
onde $\alpha$ satisfaz a seguinte condição:
\[
0 < \alpha < 1 \text{ se } Q'_{i_0,k_0} > 0, \text{ ou }
\]
\[
\alpha > 1 \text{ se } Q'_{i_0,k_0} < 0.
\]
A matriz definida desta forma é estocástica, tem
determinante positivo e pode ser escrita como uma matriz de 
blocos:
\[
\mathbf{Y} = 
\begin{pmatrix}
\mathbf{A}_{n \times n} & \mathbf{0}_{1 \times 1}\\
\mathbf{0}_{1 \times n} & \mathbf{I}_{1 \times 1}
\end{pmatrix}
\]
o qual $\mathbf{A}_{n \times n}$ é uma matriz estocástica de
determinante positivo.

Agora, tome a matriz
\[
\mathbf{Q''} := \mathbf{Q'} \cdot \mathbf{Y}.
\]
Tal matriz terá a forma
\[
\mathbf{Q''} = 
\begin{pmatrix}
1 & 0 & \cdots & 0\\
Q''_{2,1} & Q''_{2,2} & \cdots & Q''_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots\\
Q''_{n+1,1} & Q''_{n+1,2} & \cdots & Q''_{n+1,n+1}
\end{pmatrix}
\]
com $Q''_{i_0,1} > 1$. A partir daqui, o procedimento é igual 
ao do item (i).

\item $Q_{i_0, 1}' < 1$

Neste caso não há nada a fazer.
\end{enumerate}

Pelo item (i), (ii) e (iii) acima
concluímos que existe uma sequência finita
de matrizes estocásticas de determinante positivo
$\mathbf{Y}, \mathbf{Y}^{(1)}, \mathbf{Y}^{(2)},
\ldots, 
\mathbf{Y}^{(p)}$ (aqui denoto todas as matrizes
encontradas nos itens (i), (ii) e (iii) por $\mathbf{Y}$
 para facilitar
a notação), que podem ser escritas como
\[
 \mathbf{Y}^{(l)} = 
\begin{pmatrix}
 \mathbf{I}_{1 \times 1} &  \mathbf{0}_{1 \times n}\\
\mathbf{0}_{n \times 1} & \mathbf{A}^{(l)}_{n \times n}
\end{pmatrix}
\, \text{ ou } \,
\mathbf{Y}, \mathbf{Y}^{(l)} = 
\begin{pmatrix}
\mathbf{A}^{(l)}_{n \times n} & \mathbf{0}_{n \times 1} \\
 \mathbf{0}_{1 \times n} & \mathbf{I}_{1 \times 1}
\end{pmatrix}
,
\]
com $\mathbf{A}^{(l)}_{n \times n}$ matrizes estocásticas 
de determinante positivo e $l = 1, 2, \ldots p$. Deste resultado,
obtemos a matriz
\[
\mathbf{\widetilde{Q}} := 
\mathbf{Y}^{(p)} \cdot \mathbf{Y}^{(p-1)}
\cdots \mathbf{Y}^{(1)} \cdot \mathbf{Q'} \cdot \mathbf{Y}=
\begin{pmatrix}
1 & 0  &  \cdots & 0 \\
\widetilde{Q}_{2,1} & \widetilde{Q}_{2,2}  & \cdots &
\widetilde{Q}_{2,n+1}\\
\vdots & \vdots & \ddots & \vdots \\
\widetilde{Q}_{n+1,1} & \widetilde{Q}_{n+1,2} &  
\cdots & 
\widetilde{Q}_{n+1,n+1}
\end{pmatrix}
\]
que satisfaz $\widetilde{Q}_{i,1} < 1$, para todo
$i = 2, 3, \ldots, n+1$, e $\det(\mathbf{\widetilde{Q}}) >0$.

Agora definimos a matriz diagonal
\[
\mathbf{P} = (P_{ij}) \; \text{ com } \;
P_{ij} = 
\begin{cases}
1 & \text{ se } i = j = 1;\\
 \sum_{l = 2}^{n+1} \widetilde{Q}_{il} & \text{ se } i = j
\text{ e } i > 1 ;\\
0 & \text{ caso contrário}.
\end{cases}
\]
Observe que, para $i = 1, 2, \ldots n+1$, $P_{ii} > 0$. Isto é 
verdade
pois, para $i = 1$ temos $P_{11} = 1$ e, para $i > 1$,
obtemos da desigualdade $\widetilde{Q}_{i,1} < 1$ que $P_{ii} > 0$.
Desta forma, a matriz $\mathbf{P}$ é invertível e com determinante
positivo.
Multipliquemos a matriz $\mathbf{\widetilde{Q}}$
por $\mathbf{P}^{-1}$:
\[
\mathbf{P}^{-1} \cdot \mathbf{\widetilde{Q}} = 
\begin{pmatrix}
1 & 0 & \cdots & 0\\
\frac{\widetilde{Q}_{2,1}}{\sum_{l = 2}^{n+1} \widetilde{Q}_{2,l}}
&
\frac{\widetilde{Q}_{2,2}}{\sum_{l = 2}^{n+1} \widetilde{Q}_{2,l}}
& \cdots &
\frac{\widetilde{Q}_{2,n+1}}
{\sum_{l = 2}^{n+1} \widetilde{Q}_{2,l}}
\\
\vdots & \vdots & \ddots & \vdots\\
\frac{\widetilde{Q}_{n+1,1}}
{\sum_{l = 2}^{n+1} \widetilde{Q}_{n+1,l}}
&
\frac{\widetilde{Q}_{n+1,2}}
{\sum_{l = 2}^{n+1} \widetilde{Q}_{n+1,l}}
& \cdots &
\frac{\widetilde{Q}_{n+1,n+1}}
{\sum_{l = 2}^{n+1} \widetilde{Q}_{n+1,l}}
\end{pmatrix}
:= \mathbf{Z}.
\]
%Podemos escrever a matriz $\mathbf{Z}$ como a matriz de blocos

Observe que podemos escrever a matriz $\mathbf{Z}$ como a 
matriz de blocos:
\[
\mathbf{Z} = 
\begin{pmatrix}
\mathbf{I}_{1 \times 1} & \mathbf{0}_{1 \times n} \\
\mathbf{Z}_{n \times 1}^{(1)} & \mathbf{Z}_{n \times n}^{(2)}
\end{pmatrix}
,
\]
com 
\[
\mathbf{Z}_{n \times 1}^{(1)} = 
\begin{pmatrix}
\frac{\widetilde{Q}_{2,1}}{\sum_{l = 2}^{n+1} \widetilde{Q}_{2,l}}
\\
\vdots
\\
\frac{\widetilde{Q}_{n+1,1}}
{\sum_{l = 2}^{n+1} \widetilde{Q}_{n+1,l}}
\end{pmatrix}
\quad \text{e} \quad
\mathbf{Z}_{n \times n}^{(2)} = 
\begin{pmatrix}
\frac{\widetilde{Q}_{2,2}}{\sum_{l = 2}^{n+1} \widetilde{Q}_{2,l}}
& \cdots &
\frac{\widetilde{Q}_{2,n+1}}
{\sum_{l = 2}^{n+1} \widetilde{Q}_{2,l}}
\\
 \vdots & \ddots & \vdots\\
\frac{\widetilde{Q}_{n+1,2}}
{\sum_{l = 2}^{n+1} \widetilde{Q}_{n+1,l}}
& \cdots &
\frac{\widetilde{Q}_{n+1,n+1}}
{\sum_{l = 2}^{n+1} \widetilde{Q}_{n+1,l}}
\end{pmatrix},
\]
uma matriz estocástica de determinante positivo.
Note que, para $i=2, 3, \ldots, n+1$, podemos escrever
$\widetilde{Q}_{i,1} = 1 - \sum_{l = 2}^{n+1} \widetilde{Q}_{i,l}$
e, portanto, reescrevemos 
$\mathbf{Z}_{n \times 1}^{(1)}$ como
\[
\mathbf{Z}_{n \times 1}^{(1)} = 
\begin{pmatrix}
\frac{1}{\sum_{l = 2}^{n+1} \widetilde{Q}_{2,l}} - 1
\\
\vdots
\\
\frac{1}
{\sum_{l = 2}^{n+1} \widetilde{Q}_{n+1,l}} - 1
\end{pmatrix}
.
\]
Tomando as constantes $a_2 = \sum_{l = 2}^{n+1} \widetilde{Q}_{2,l},
a_3 = \sum_{l = 2}^{n+1} \widetilde{Q}_{3,l}, \ldots, 
a_{n+1} = \sum_{l = 2}^{n+1} \widetilde{Q}_{n+1,l}$ temos
\[
\mathbf{Z}_{n \times 1}^{(1)} = 
\begin{pmatrix}
\frac{1}{a_2} - 1
\\
\vdots
\\
\frac{1}
{a_{n+1}} - 1
\end{pmatrix}
\; \text{ e } \;
\mathbf{P} =
\begin{pmatrix}
1 & 0   & 0   & \ldots & 0 & 0\\
0 & a_2 & 0   & \ldots & 0 & 0\\
0 & 0   & a_3 & \ldots & 0 & 0\\
\vdots & \vdots   & \vdots   & \ddots & \vdots & \vdots\\
0 & 0   & 0 & \ldots & a_n & 0\\
0 & 0   & 0 & \ldots & 0 & a_{n+1}\\
\end{pmatrix}
.
\]

Após todas estas contas, faremos o caminho reverso agora.
Da igualdade $ 
\mathbf{P}^{-1} \cdot \mathbf{\widetilde{Q}} = \mathbf{Z}$
temos que
\[
\mathbf{\widetilde{Q}} = 
(\mathbf{P} \cdot \mathbf{Z}).
\]
Porém, como
$
\mathbf{\widetilde{Q}} = 
\mathbf{Y}^{(p)} \cdot \mathbf{Y}^{(p-1)}
\cdots \mathbf{Y}^{(1)} \cdot \mathbf{Q'} \cdot \mathbf{Y}
$, obtemos
%ou seja,
\[
\mathbf{Q'} =
[\mathbf{Y}^{(p)} \cdot \mathbf{Y}^{(p-1)}
\cdots \mathbf{Y}^{(1)} ]^{-1} 
(\mathbf{P} \cdot \mathbf{Z}) \mathbf{Y}^{-1}.
\]
Finalmente, como 
$\mathbf{Q} \cdot \mathbf{X}_m \cdot \mathbf{X}_{m-1}  \cdots
 \mathbf{X}_1 =  \mathbf{Q'}$, concluímos que
\[
\mathbf{Q} =
[\mathbf{Y}^{(p)} \cdot \mathbf{Y}^{(p-1)}
\cdots \mathbf{Y}^{(1)} ]^{-1} 
(\mathbf{P} \cdot \mathbf{Z}) \mathbf{Y}^{-1}
[\mathbf{X}_m \cdot \mathbf{X}_{m-1}  \cdots
 \mathbf{X}_1]^{-1}
\]

Agora relembre que
\begin{itemize}
\item As matrizes 
 $\mathbf{X}_1, \mathbf{X}_2, \ldots,  \mathbf{X}_m$ terão a forma de alguma das matrizes
abaixo (vide o Lema inicial deste Apêndice):
\[
\mathbf{W}^{(1)} = 
\begin{pmatrix}
\mathbf{P}^{(1)}_{2 \times 2} & \mathbf{0}_{2 \times (n-1)}\\
\mathbf{0}_{(n-1) \times (n-1)} & \mathbf{I}_{(n-1)\times (n-1)}
\end{pmatrix}
\quad
\mathbf{W}^{(n)} = 
\begin{pmatrix}
\mathbf{I}_{(n-1)\times (n-1)}& \mathbf{0}_{(n-1) \times 2} \\
\mathbf{0}_{2 \times (n-1)}   & \mathbf{P}^{(n)}_{2 \times 2}   
\end{pmatrix}
\]
\[
\mathbf{W}^{(j)} = 
\begin{pmatrix}
\mathbf{I}_{(j-1)\times (j-1)}& \mathbf{0}_{(j-1) \times 2} &
\mathbf{0}_{(j-1) \times (n-j)} \\
\mathbf{0}_{2 \times (j-1)} & \mathbf{P}^{(j)}_{2 \times 2} &
 \mathbf{0}_{2 \times (n-j)}\\
\mathbf{0}_{(n-j) \times (j-1)} & \mathbf{0}_{(n-j) \times (2)}
 & \mathbf{I}_{(n-j)\times (n-j)}
\end{pmatrix}
\]
com $ j \in \{2, 3, \ldots, n-1\}$ e
$\mathbf{P}^{(1)}_{2 \times 2}, \ldots, 
\mathbf{P}^{(n)}_{2 \times 2}$ matrizes estocásticas $ 2 \times 2$.
Como as matrizes 
$\mathbf{X}_1, \mathbf{X}_2, \ldots,  \mathbf{X}_m$ tem
determinante positivo obtemos, por hipótese de indução,
os caminhos contínuos $\bm{X}_i(t)$ satisfazendo
\[
\mathbf{X}_i(0) = \mathbf{I}_{n+1} \quad \text{ e } \quad
\mathbf{X}_i(1) = \mathbf{X}_i,
\]
com $i = 1, 2, \ldots, m$ e $ t \in [0,1]$.
Isto é verdade pois $\mathbf{X}_i$ é uma matriz formada pelo bloco 
$\mathbf{P}^{(i)}_{2 \times 2}$ que, por hipótese de 
indução, admite um caminho contínuo o ligando a identidade.

\item
O mesmo vale para a nossa sequência de matrizes
$\mathbf{Y}, \mathbf{Y}^{(1)}, \mathbf{Y}^{(2)},
\ldots, 
\mathbf{Y}^{(p)}$, que podem ser escritas como
\[
 \mathbf{Y}^{(l)} = 
\begin{pmatrix}
 \mathbf{I}_{1 \times 1} &  \mathbf{0}_{1 \times n}\\
\mathbf{0}_{n \times 1} & \mathbf{A}^{(l)}_{n \times n}, 
\end{pmatrix}
\, \text{ ou } \,
\mathbf{Y}, \mathbf{Y}^{(l)} = 
\begin{pmatrix}
\mathbf{A}^{(l)}_{n \times n} & \mathbf{0}_{n \times 1} \\
 \mathbf{0}_{1 \times n} & \mathbf{I}_{1 \times 1}
\end{pmatrix}
,
\]
com $\mathbf{A}^{(l)}_{n \times n}$ matrizes estocásticas 
de determinante positivo. Ou seja, como as matrizes 
 $\mathbf{Y}$ e $\mathbf{Y}^{(i)}$, para $i = 1, 2, \ldots p$,
podem ser escritas por matrizes bloco que satisfazem a 
hipótese de indução, obtemos os caminhos contínuos
$\bm{Y}^{(i)}(t)$ e $\bm{Y}(t)$ satisfazendo:
\begin{align*}
\mathbf{Y}^{(i)}(0) = \mathbf{I}_{n+1}, \quad &  \mathbf{Y}^{(i)}(1) = \mathbf{Y}^{(i)} \; \text{e} \\
\mathbf{Y}(0) = \mathbf{I}_{n+1}, \quad &  \mathbf{Y}(1) = \mathbf{Y},
\end{align*}
com $t \in [0,1]$.
\item
Para encerrar, olhemos para o produto $(\mathbf{PZ})$. Inicialmente,
definimos a função contínua
\[
\mathbf{P}(t) = 
\begin{pmatrix}
1 & 0   & 0   & \ldots & 0 & 0\\
0 & e^{t \cdot ln(a_2)} & 0   & \ldots & 0 & 0\\
0 & 0   & e^{t \cdot ln(a_3)} & \ldots & 0 & 0\\
\vdots & \vdots   & \vdots   & \ddots & \vdots & \vdots\\
0 & 0   & 0 & \ldots & e^{t \cdot ln(a_n)} & 0\\
0 & 0   & 0 & \ldots & 0 & e^{t \cdot ln(a_{n+1})}\\
\end{pmatrix}
\]
com $t \in [0,1]$. Em seguida, dado que
\[
\mathbf{Z} = 
\begin{pmatrix}
\mathbf{I}_{1 \times 1} & \mathbf{0}_{1 \times n} \\
\mathbf{Z}_{n \times 1}^{(1)} & \mathbf{Z}_{n \times n}^{(2)}
\end{pmatrix}
,
\]
temos que a matriz estocástica
de determinante positivo $\mathbf{Z}_{n \times n}^{(2)}$ admite,
por hipótese de indução, o caminho contínuo
\[
\mathbf{Z}_{n \times n}^{(2)}(t) \;\text{ satisfazendo }\;
\mathbf{Z}_{n \times n}^{(2)}(0) = \mathbf{I}_{n+1} \text{ e }
\mathbf{Z}_{n \times n}^{(2)}(1) = \mathbf{{Z}_{n \times n}^{(2)}},
\]
$ \text{ com } t \in [0,1] $.
Agora, fixemos o caminho contínuo para 
$\mathbf{Z}_{n \times 1}^{(1)}$ como sendo
\[
\mathbf{Z}_{n \times 1}^{(1)}(t) = 
\begin{pmatrix}
\frac{1}{e^{t \cdot ln(a_2)}} - 1
\\
\vdots
\\
\frac{1}
{e^{t \cdot ln(a_{n+1})}} - 1
\end{pmatrix}
.
\]
Sendo assim, obtemos o caminho contínuo
\[
\mathbf{Z}(t) = 
\begin{pmatrix}
\mathbf{I}_{1 \times 1} & \mathbf{0}_{1 \times n} \\
\mathbf{Z}_{n \times 1}^{(1)}(t) & \mathbf{Z}_{n \times n}^{(2)}(t)
\end{pmatrix}
,
\]
com $
\mathbf{Z}(0) = \mathbf{I}_{n+1}$ e 
$\mathbf{Z}(1) = \mathbf{Z}$.
Portanto, a função $(\mathbf{PZ})(t) = \mathbf{P}(t) \mathbf{Z}(t) $
é contínua, estocástica, e satisfaz 
$(\mathbf{PZ})(0) = \mathbf{I}_{n+1}$ e 
$(\mathbf{PZ})(1) = \mathbf{PZ}$. 
\end{itemize}

Logo, concluímos que a função
\[
\mathbf{Q}(t) =
[\mathbf{Y}^{(p)}(t) \cdot \mathbf{Y}^{(p-1)}(t)
\cdots \mathbf{Y}^{(1)}(t) ]^{-1} 
(\mathbf{P} \mathbf{Z})(t) \mathbf{Y}^{-1}(t)
[\mathbf{X}_m(t) \cdot \mathbf{X}_{m-1}(t)  \cdots
 \mathbf{X}_1(t)]^{-1}
\]
é contínua, tem determinante positivo e é uma matriz estocástica
 em todos os seus pontos. Além do mais, temos 
$ \mathbf{Q}(0) = \mathbf{I}_{n+1}$ e 
$ \mathbf{Q}(1) = \mathbf{Q}$.
Com isto encerramos a demonstração de que o 
 conjunto das matrizes estocásticas de determinante positivo
\[
\left\{ \mathbf{X} \in \mathcal{G}_n; \; \det(\mathbf{X}) > 0 \right\}
\]
é conexo por caminhos.
%Por hipótese de indução, existe
%uma função contínua, $g: [0,1] \rightarrow \mathcal{G}_n$ tal
%que $g(0) = \mathbf{I}_n $ e 
%$g(1) = \mathbf{Z}_{n \times n}^2$.

\end{proof}
% \endfold

%%
%%% \beginfold Demonstracao do teorema 5.7
%%
%%\chapter{Demonstra\c{c}\~ao do Teorema 5.7 }
%%\begin{teo*}
%%Seja $G$ um grupo de matrizes de Lie, conexo, com centro $Z(G)$ discreto e com espaço tangente $T_1(G) \neq \{\mathbf{0}\}$
%%(aqui, $\mathbf{0}$ representa a matriz cuja todas entradas valem zero). Então,
%%se $\mathcal{H} \subseteq G$ for um subgrupo normal e não discreto, teremos que o
%%espaço tangente do subgrupo $\mathcal{H}$ será um ideal não trivial de $T_1(G)$, i.e.,
%%$T_1(\mathcal{H}) \neq \{ \mathbf{0} \}$ e, para todo $A \in T_1(G)$ e para
%%todo $B \in T_1(\mathcal{H})$, vale $[A, B] \in T_1(\mathcal{H})$.
%%\end{teo*}
%%
%%\begin{proof}\label{ideal_tg}
%%Façamos a demonstração em duas partes.
%%
%%\textbf{(i)} $T_1(H)$ é um ideal de $T_1(G)$. Tome $A \in T_1(H)$ e $B \in T_1(\mathcal{H})$ matrizes quaisquer. 
%%Sabemos que existem os caminhos diferenciáveis
%%$g_A:[0,1] \rightarrow G$ e $g_B:[0,1] \rightarrow \mathcal{H}$, com
%%\[
%%    \begin{cases}
%%     g_A(0)  = I_n\\
%%     g_A'(0)  = A 
%%   \end{cases}
%%	\qquad 
%%	\text{ e }
%%	\qquad
%%    \begin{cases}
%%     g_B(0)  = I_n\\
%%     g_B'(0)  = B 
%%   \end{cases}
%%   .
%%\]
%%Definimos a função
%%\begin{align*}
%%\xi: [0,1] \times [0,1] & \rightarrow \mathcal{H} \\
%%(s,t) & \mapsto g_A(s) \cdot g_B(t) \cdot g_A^{-1}(s).
%%\end{align*}
%%
%%Esta função tem imagem em $\mathcal{H}$, pois $\mathcal{H}$ é um subgrupo normal de $G$. 
%%Desta forma, procedendo da mesma maneira ao qual provamos que $T_1(G)$ é 
%%uma álgebra de Lie (parte final da demonstração do Teorema~\ref{tg_lie}), obtemos
%%\[ [A, B] = AB - BA \in T_1( \mathcal{H} ).\]
%%Logo, $T_1( \mathcal{H} )$ é um ideal de $T_1( G )$.
%%
%%\textbf{(ii)} Provemos que $T_1( \mathcal{H} ) \neq \{\mathbf{0} \}$.
%%Suponhamos, por absurdo, que $T_1( \mathcal{H} ) = \{ \mathbf{0} \}$. 
%%Sabemos, pelo Teorema~\ref{bijecao_exp}, que existe um real positivo $r > 0$ e um aberto
%%$U$ de $T_1(G)$ contendo a matriz $\mathbf{0}$, tal que a função
%%\[ \exp: U \subseteq T_1(G) \rightarrow B(I_n, r) \]
%%é uma bijeção. Pelo fato de $Z(G)$ ser discreto, e $\mathcal{H}$ ser um conjunto não discreto, 
%%temos $\mathcal{H} \setminus Z(G) \neq \emptyset$. Logo, existe uma matriz $B \in \mathcal{H} \setminus Z(G)$
%%tal que $B \in B(I_n, r)$.
%%
%%Afirmamos que existe uma matriz $A \in B(I_n, r)$ que não comuta com $B$. 
%%Isto é verdade, pois se $B$ comutasse com todas as matrizes $C \in B(I_n, r)$, teríamos, para toda
%%sequência $g_1, \ldots g_n \in B(I_n, r)$, que 
%%\[ B \cdot ( g_1 \cdots g_n) = ( g_1 \cdots g_n) \cdot B. \]
%%Porém, pelo fato de $G$ ser um grupo topológico conexo, temos, pelo Teorema~\ref{gerador}, que a bola $B(I_n, r)$ gera $G$. 
%%Ou seja, todo $X \in G$ pode ser escrito como $X = g_1 \cdots g_n$,
%%para $g_1, \ldots g_n \in B(I_n, r)$. Logo, tomando o produto de $X$ e $B$, obtemos
%%\[ B \cdot X = B \cdot ( g_1 \cdots g_n) = ( g_1 \cdots g_n) \cdot B = X \cdot B, \]
%%i.e., $B$ comuta com todos elementos de $G$ e, portanto, $B$ pertence ao centro de $G$. 
%%Todavia, isto é um absurdo, pois, por hipótese, $B \in \mathcal{H} \setminus Z(G)$.
%%
%%Dada a matriz $A \in B(I_n, r)$ tal que $ A B \neq B A$, observemos que existe uma matriz $A_t \in T_1(G)$ tal que $\exp(A_t) = A$.
%%Em posse destes resultados, definimos o seguinte caminho diferenciável
%%\begin{align*}
%%g: [0,1] \times [0,1] & \rightarrow G \\
%%t & \mapsto e^{t \cdot A_t} \,B\, e^{-t \cdot A_t}\, B^{-1}.
%%\end{align*}
%%Decorre de $\mathcal{H}$ ser um subgrupo normal de $G$ que, para todo $t \in [0,1]$, vale
%%$e^{t \cdot A_t} B e^{-t \cdot A_t} \in \mathcal{H}$ e, portanto, $e^{t \cdot A_t} B e^{-t \cdot A_t} B^{-1} \in \mathcal{H}$.
%%Logo, a imagem da função $g$ é um subconjunto de $\mathcal{H}$. 
%%Como $g(0) = I_n$, temos $g'(0) \in T_1( \mathcal{H} )$, ou seja, $g'(0) = \mathbf{0}$.
%%Porém, calculando a derivada de $g$, obtemos
%%\[ g'(t) = A_t e^{t \cdot A_t} B e^{-t \cdot A_t} B^{-1} - e^{t \cdot A_t} B A_t e^{-t \cdot A_t} B^{-1}. \]
%%Tomando esta derivada no ponto zero, concluímos que 
%%\[0 = g'(0) = A_t - BA_tB^{-1}.\]
%%Dada está igualdade, podemos aplicar a exponencial em ambos os lados, obtendo
%%\[ \exp(A_t) = \exp(BA_tB^{-1}) = B\exp(A_t)B^{-1}.\]
%%Agora, utilizando o fato de que $\exp(A_t) = A$, podemos reescrever a igualdade acima como
%%\[ A =  BAB^{-1}, \]
%%ou seja, $AB = BA$. Porém, isto contradiz a hipótese de que as matrizes $A$ e $B$ não comutam. 
%%Portanto, provamos, por contradição, que $T_1(\mathcal{H}) \neq \{ \mathbf{0} \}$.
%%
%%\end{proof}
%%%\endfold
%%
%%% \\beginfold Produto tensorial
%%\chapter{B. O Produto Tensorial entre Espa\c{c}os Vetoriais}\label{prodtens}
%%A importância de se estudar o produto tensorial entre espaços
%%vetoriais dá-se pelo fato de este possibilitar reduzir o estudo de 
%%transformações bilineares para o estudo de transformações lineares,
%%reduzindo a complexidade do problema. Em outras palavras,
%%dados os $\mathbb{R}$-espaços vetoriais $A, B$ e $C$ e 
%%a aplicação bilinear $\otimes: A \times B \rightarrow A \otimes B$ 
%%que define o produto tensorial entre os espaços $A, B$, denotado
%%por $A \otimes B$. Então, dada uma  aplicação
%%bilinear qualquer $h:A \times B \rightarrow C$ existe uma transformação
%%linear $\psi: A \otimes B \rightarrow C$ tal que o diagrama
%%\begin{displaymath}
%%\begin{tikzcd}
%%    A \times B \arrow{rd }{h} \arrow{r}{ \otimes} & A \otimes B  \arrow{d} {\psi} \\
%%    & C
%%\end{tikzcd}
%%\end{displaymath}
%%comuta.
%%
%%A seguir faremos a construção do produto tensorial entre espaços vetoriais.
%%Sejam $A, B$ $\mathbb{R}$-espaços vetoriais. Definimos o seguinte conjunto de funções:
%%\[
%%    F = \{ f: A\times B \rightarrow \mathbb{R}; \text{ f é diferente de zero em um 
%%          número finito de pontos}\}.
%%\]
%%Dado $(a,b) \in A \times B$ um ponto qualquer, tomamos a função
%%\begin{align*}
%%    \mathbb{1}_{(a,b)}:  A \times B & \rightarrow  \mathbb{R} \\
%%                         (x,y)      & \mapsto      \begin{cases} 1 & \quad \text{se } (x,y) = (a,b); \\ 0 & \quad \text{c.c,} \end{cases}
%%\end{align*}
%%e observamos que, dada $f \in F$ uma função qualquer, vale
%%\[
%%    f = \sum_{(a,b)} f(a,b) \mathbb{1}_{(a,b)},
%%\]
%%onde a somatória está definida nos pontos $(a,b)$ tais que $f(a,b) \neq 0$, ou seja, a somatória acima é finita e, portanto,
%%está bem definida. Sendo assim, obtemos que o conjunto
%%\[
%%    \{ \mathbb{1}_{(a,b)}: A \times B \rightarrow \mathbb{R}; (a,b) \in A \times B\}
%%\]
%%é uma base de $F$.
%%
%%Agora, tome o subespaço $S \subseteq F$ gerado por
%%\begin{align}
%%\tag{*}
%%S =  \spn\{ & \mathbb{1}_{(a_1 + \lambda_1 b_1,c_1)} -\mathbb{1}_{(a_1 ,c_1)} - \lambda_1 \mathbb{1}_{(b_1 ,c_1)},\\ \nonumber
%%       & \mathbb{1}_{(a_2, c_2 + \lambda_2 d_2)} -\mathbb{1}_{(a_2 ,c_2)} - \lambda_2 \mathbb{1}_{(a_2 ,d_2)};\\ \nonumber
%%       & a_1, b_1, a_2 \in A, c_1, c_2, d_2 \in B \text{ e } \lambda_1, \lambda_2 \in \mathbb{R} \} \nonumber
%%\end{align}
%%e analisemos a coclasse $F/S$. $F/S$ será um espaço vetorial dado por $ F/S = \{f + S; f \in F \}$ e,
%%dada a projeção $\pi: F \rightarrow F/S$ com $f \mapsto f + S$, definiremos a seguinte função
%%\begin{align*}
%%    \otimes :  A \times B &\rightarrow F/S \\
%%               (a,b) & \mapsto \pi( \mathbb{1}_{(a,b)} ).
%%\end{align*}
%%Logo, definiremos o produto tensorial entre $A$ e $B$ como sendo o conjunto
%%\[ A \otimes B := \otimes( A \times B ) .\]
%%
%%Note que
%%\begin{enumerate}[(i)]
%%    \item $ \otimes: A \times B \rightarrow F/S$ é bilinear pois, dados $a, b \in A$, $c \in B$ e $ \lambda \in \mathbb{R}$,
%%          temos que
%%          \[ \mathbb{1}_{(a + \lambda b,c)} -\mathbb{1}_{(a ,c)} - \lambda \mathbb{1}_{(b ,c)} \in S, \]
%%          portanto $ \pi(\mathbb{1}_{(a + \lambda b,c)} -\mathbb{1}_{(a ,c)} - \lambda \mathbb{1}_{(b ,c)}) = 0.$
%%          Como a projeção é uma transformação linear, obtemos
%%          \[ \pi( \mathbb{1}_{(a + \lambda b,c)}) =  \pi(\mathbb{1}_{(a ,c)}) + \lambda \pi(\mathbb{1}_{(b ,c)}), \]
%%          ou seja, $\otimes(a + \lambda b, c) = \otimes(a, c) + \lambda \otimes(b, c).$ Para mostrarmos que $\otimes$
%%          é linear na segunda coordenada usamos o mesmo procedimento acima.
%%    \item $A \otimes B = F/S$. Isto decorre da projeção $\pi$ ser sobrejetora e 
%%          $\{ \mathbb{1}_{(a,b)}: A \times B \rightarrow \mathbb{R}; (a,b) \in A \times B\}$ ser uma base de $F$, pois,
%%          para todo $f + S \in F/S$ temos $\pi(f) = f + S$ e 
%%          \[ f = \sum_{(a,b)} f(a,b) \mathbb{1}_{(a,b)}. \]
%%          Logo,
%%          \[ \pi(f) = \sum_{(a,b)} f(a,b) \pi(\mathbb{1}_{(a,b)}). \]
%%          Como $\pi(\mathbb{1}_{(a,b)}) \in A\otimes B$ e $A \otimes B$ é um espaço vetorial temos que 
%%          $ \sum_{(a,b)} f(a,b) \mathbb{1}_{(a,b)} \in A \otimes B$, ou seja, $f+ S \in A \otimes B$.
%%          Logo, $A \otimes B = F/S$.
%%\end{enumerate}
%%
%%Sendo assim, denotaremos todo elemento de $A \otimes B$ como $a \otimes b$, onde $a \in A, b \in B$ e $a \otimes b := \otimes(a,b)$.
%%Abaixo segue o principal resultado sobre o produto tensorial.
%%
%%\begin{teo}[Propriedade Universal do Produto Tensorial]
%%Sejam $A, B, C$ $\mathbb{R}$-espaços vetoriais e $h: A \times B \rightarrow C$ uma transformação bilinear.
%%Então, existe uma única transformação linear $\psi: A \otimes B \rightarrow C$ tal que o diagrama
%%\begin{displaymath}
%%\begin{tikzcd}
%%    A \times B \arrow{rd }{h} \arrow{r}{ \otimes} & A \otimes B  \arrow{d} {\psi} \\
%%    & C
%%\end{tikzcd}
%%\end{displaymath}
%%comuta.
%%\end{teo}
%%\begin{proof}
%%Tomando $F$ como o conjunto das funções de $A \times B$ em $\mathbb{R}$ que assumem valores não nulos em um número finito
%%de vezes, definimos as seguintes funções
%%\begin{align*}
%%u:  A \times B & \rightarrow F           & \qquad & s:  F  \rightarrow C \\
%%    (a,b) & \mapsto \mathbb{1}_{(a,b)}   & \qquad &  \quad      \mathbb{1}_{(a,b)} \mapsto h(a,b).  
%%\end{align*}
%%Como $\{ \mathbb{1}_{(a,b)}: A \times B \rightarrow \mathbb{R}; (a,b) \in A \times B\}$ é uma base de $F$ temos que
%%a transformação linear $s: F \rightarrow C$ está bem definida e é única. Desta forma, o diagrama abaixo comuta.
%%\begin{displaymath}
%%\begin{tikzcd}
%%    A \times B \arrow{rd }{h} \arrow{r}{ u} & F  \arrow{d} {s} \\
%%    & C
%%\end{tikzcd}
%%\end{displaymath}
%%Observe que o conjunto $S$ definido em $(*)$ está contido no núcleo da transformação
%%$s$, pois, decorre do diagrama comutar que $ h = s \circ u$
%%e, para todo $a, b \in A, c \in B$ e $\lambda \in \mathbb{R}$, temos
%%\begin{align*}
%%h( a + \lambda b, c) - h(a, c) - \lambda h(b, c) = & \; s( \mathbb{1}_{(a + \lambda b, c)}) -s( \mathbb{1}_{(a , c)})-
%%                                                      \lambda s(\mathbb{1}_{(b, c)}) \\
%%                                                 = & \; s( \mathbb{1}_{(a + \lambda b, c)} - \mathbb{1}_{(a , c)}-
%%                                                      \lambda \mathbb{1}_{(b, c)}). 
%%\end{align*}
%%Como $h$ é bilinear, obtemos $s( \mathbb{1}_{(a + \lambda b, c)} - \mathbb{1}_{(a , c)}- \lambda \mathbb{1}_{(b, c)}) = 0$.
%%Portanto, $ \mathbb{1}_{(a + \lambda b, c)} - \mathbb{1}_{(a , c)}- \lambda \mathbb{1}_{(b, c)} \in Ker(s)$.
%%Agora, dados $\widetilde{a}  \in A, \widetilde{b}, \widetilde{c} \in B$ e $\lambda \in \mathbb{R}$ pontos
%%quaisquer, obtemos, de forma análoga, que
%% $ \mathbb{1}_{(\widetilde{a}, \lambda \widetilde{c}\widetilde{b}) } - \mathbb{1}_{(\widetilde{a} , \widetilde{b})}-
%% \lambda \mathbb{1}_{(\widetilde{a}, \widetilde{c})} \in Ker(s).$ Logo, $ S \subseteq Ker(s)$.
%%Desta forma, existe $\psi: F/S \rightarrow C$ única tal que o diagrama
%%\begin{displaymath}
%%\begin{tikzcd}
%%    F \arrow{d }{s} \arrow{r}{ \pi} & F/S= A\otimes B \arrow{ld} {\psi} \\
%%    C& 
%%\end{tikzcd}
%%\end{displaymath}
%%comuta. Sendo assim, obtemos que $s = \psi \circ \pi$ e, portanto, resulta de $h = s \circ u$ que
%%\[ h = \psi \circ \pi \circ u. \]
%%Como $ \otimes = \pi \circ u$, pois $\otimes(a,b) = \pi( \mathbb{1}_{(a,b)})$ e $\pi \circ u(a,b) = \pi( \mathbb{1}_{(a,b)}) $,
%%temos que existe, e é única, uma transformação linear $\psi: A \otimes B \rightarrow C$ tal que o diagrama abaixo comuta.
%%\begin{displaymath}
%%\begin{tikzcd}
%%    A \times B \arrow{r }{\otimes} \arrow{rd}{ h} &  A\otimes B \arrow{d} {\psi} \\
%%    & C 
%%\end{tikzcd}
%%\end{displaymath}
%%\end{proof}
%%
%%\textbf{Observação:} Sejam $A, B$ $\mathbb{R}$-espaços vetoriais, então valem as seguintes propriedades:
%%\begin{enumerate}[(P1)]
%%    \item Dadas as bases $\{e_i\}$ de $A$ e $\{ w_j \}$ de $B$ temos que $\{ e_i \otimes w_j \}$ será uma base de
%%          $A \otimes B$. Que $\{ e_i \otimes w_j \}$ gera $A \otimes B$ é imediato, pois
%%          \begin{align*}
%%                A \otimes B \ni a \otimes b & = \left( \sum_i a_i e_i \right) \otimes \left( \sum_j b_j w_j \right)\\
%%                                            & = \sum_{i, j} a_i b_j e_i \otimes w_j.
%%          \end{align*}
%%          Resta mostrarmos que $\{ e_i \otimes w_j \}$ é L.I.
%%          
%%          Tomando as seguintes transformações lineares
%%          \begin{align*}
%%          \psi_i: & A \rightarrow \mathbb{R}   & \qquad & \phi_j:  B  \rightarrow \mathbb{R} \\
%%                  & e_l \mapsto \delta_{il}    & \qquad &  \quad \;   w_l \mapsto \delta_{jl} 
%%          \end{align*}
%%          temos que a transformação 
%%          \begin{align*}
%%            \xi_{ij}: & A \times B \rightarrow \mathbb{R} \\
%%                      & (x,y) \mapsto \psi_i(x) \phi_j(y)
%%          \end{align*}
%%          é bilinear. Pela propriedade universal do produto tensorial temos que para cada $\xi_{ij}$ existe uma transformação
%%          linear $\overline{\xi}_{ij}: A \otimes B \rightarrow \mathbb{R}$ tal que
%%          \[ \xi_{ij}(a,b) = \overline{\xi}_{ij}(a\otimes b),\]
%%          para todo $a \in A$ e $b \in B$.
%%          
%%          Agora, dada a combinação linear finita $\sum_{ij} \alpha_{ij} e_i \otimes w_j$, mostremos que 
%%          \[ \sum_{ij} \alpha_{ij} e_i \otimes w_j = 0 \implies \alpha_{ij} = 0. \tag{**}\]
%%          Para cada índice $(l,k)$ na somatória acima temos que
%%          \begin{align*}
%%            0 & = \overline{\xi}_{lk}( \sum_{ij} \alpha_{ij} e_i \otimes w_j )\\
%%              & = \sum_{ij} \alpha_{ij} \overline{\xi}_{lk}( e_i \otimes w_j).
%%          \end{align*}
%%         Porém, $\overline{\xi}_{lk}( e_i \otimes w_j) = \xi_{lk}( e_i, w_j)$. Logo
%%         $\overline{\xi}_{lk}( e_i \otimes w_j) = \delta_{li} \delta_{kj}$, ou seja, $\alpha_{lk} = 0$.
%%         Assim sendo, $\alpha_{lk} = 0$ para todo índice $(l, k)$ da somatória $(**)$. Logo, $\{e_i \otimes w_j \}$
%%         é uma base de $A \otimes B$.
%%
%%    \item Se $\dim A < \infty $ e $\dim B < \infty$. Então, pela propriedade (P1) vale
%%          \[ \dim A \otimes B = (\dim A) (\dim B). \]
%%    \item O produto tensorial $\mathbb{R} \otimes A$ é isomorfo ao próprio espaço vetorial $A$, i.e.,
%%          $\mathbb{R} \otimes A \cong A$.
%%
%%
%%\end{enumerate}
%%
%%
%%% \\endfold Produto tensorial
%%
%
%%%% \\beginfold
\chapter{B. Programa Para C\'alculo da \'Algebra Semi-Simples da Se\c{c}\~ao 5.3} \label{code1}
\begin{lstlisting}	
# script escrito para rodar no octave versao 4.0.0
# Necessario o pacote "linear-algebra" para rodar
# a funcao cartprod().
# Dada uma base de um ideal soluvel, o programa ira tentar calcular uma
# base para uma algebra semi-simples, tendo em vista o teorema de Levi.
# Para avaliar se uma algebra eh semi-simples inspecionamos, via forca
# bruta,  se a matriz  da forma de Cartan-Killing eh inversivel.
#
# O programa, assim que achar uma algebra semi-simples, ira imprimir os
# resultados em um arquivo txt. 
1;

pkg load all; # carrega o pacote linear-algebra 

######### Base do espaco tangente 
N = 3;
bases = zeros(N, N, N*(N-1) );
k = 1;
for i = 1:N
	for j = 1:N
		if ( i != j)
			bases(i,i,k) = -1;
			bases(i,j,k) = 1;
			k++;
		endif
	endfor
endfor
###########

###### Funcoes auxiliares

### Operador colchete
function ret = colchete(a, b)
	ret = a*b - b*a;
endfunction

### Calcula a matriz da transformacao adjunta	
function transf_matrix = transformacao(base, X_sist_linear, n)
	transf_matrix = zeros(n*(n-1), n*(n-1), n*(n-1) );
	for i = 1:(n*(n-1))
		for j = 1:(n*(n-1))
			y = ( colchete(base(:,:,i), base(:,:,j ) ) )';
			y = y(:);
			transf_matrix(:,j,i) = X_sist_linear \ y;
		endfor
	endfor
endfunction

### Calcula a matriz da forma de Killing
function matriz_killing = forma_killing( index_2, n)
	matriz_killing = zeros(n, n);
	for i = 1:n
		for j = 1:n
			matriz_killing( i, j) = trace( index_2(:,:, i) * index_2(:,:, j) );
		endfor
	endfor
endfunction


#################
Pontos = cartprod (1:6, 1:6)
Total =6**2;

### Testando Bases para achar a algebra semi-simples
LIbases = zeros(Total, 6);
for i = 1:Total
	LIbases(i, Pontos(i,1) ) = 1;
	LIbases(i, Pontos(i,2) ) = -1;
endfor

LI = zeros(6,6);
LI(1,:) = [0,1,-1,1,-1,0];
LI(2,:) = [-1,1,0,1,0,-1];

novaBase = zeros(N, N, N*(N-1) );
novaBase(:,:,1) = bases(:,:,2) + bases(:,:,4) - ( bases(:,:,5) + bases(:,:,3) );
novaBase(:,:,2) = bases(:,:,2) + bases(:,:,4) - ( bases(:,:,1) + bases(:,:,6) );

interacao = 1;
for i = unique(1:Total)
	for j = setdiff(1:Total, i)
		for k = setdiff(1:Total, union(i, j) )
			for l = setdiff(1:Total, union( union(i,j), k ) )
				novaBase(:,:,3) = bases(:,:, Pontos(i,1) ) - bases(:,:, Pontos(i,2) ) ;
				novaBase(:,:,4) = bases(:,:, Pontos(j,1) ) - bases(:,:, Pontos(j,2) ) ; 
				novaBase(:,:,5) = bases(:,:, Pontos(k,1) ) - bases(:,:, Pontos(k,2) ) ; 
				novaBase(:,:,6) = bases(:,:, Pontos(l,1) ) - bases(:,:, Pontos(l,2) ) ; 
				interacao++;

				LI(3,:) = LIbases(i,:);						
				LI(4,:) = LIbases(j,:);						
				LI(5,:) = LIbases(k,:);						
				LI(6,:) = LIbases(l,:);						

				if ( rank(LI) == 6 )
					X = zeros(N**2, N*(N-1) );
					
					for i = 1:(N*(N-1))
						aux = novaBase(:,:,i)';
						X(:, i) = aux(:);
					endfor
					transformacaoMatriz = transformacao(novaBase, X, N);
	
					transformacaoMatriz2 = zeros(3,3,3);
					transformacaoMatriz2(:,:,1) = transformacaoMatriz(4:6,4:6,4);
					transformacaoMatriz2(:,:,2) = transformacaoMatriz(4:6,4:6,5);
					transformacaoMatriz2(:,:,3) = transformacaoMatriz(4:6,4:6,6);
					
					a1 =  round( transformacaoMatriz(1:3,4:6,4) .* 100000) ./100000;
					a2 =  round( transformacaoMatriz(1:3,4:6,5) .* 100000) ./100000;
					a3 =  round( transformacaoMatriz(1:3,4:6,6) .* 100000) ./100000;
					
					if ( all( all(a1 == 0 ) ) && all( all(a2 == 0 ) ) &&
					     all( all(a3 == 0 ) ) )
						resultado = forma_killing( transformacaoMatriz2, 3 );
						rank(resultado)
						if (rank (resultado ) == 3 )
							filename1 = strcat(  num2str(interacao), '.txt');
	
							myfile = fopen(filename1,'a');
							fdisp (myfile, '************************************************************');
							fdisp (myfile, 'i j k l');
							fdisp (myfile, [i j k l] );
							fdisp (myfile, LI );
							fdisp (myfile, transformacaoMatriz );
							fdisp (myfile, '  ' );
				
							fdisp (myfile, resultado );
							fclose(myfile);	
						endif

					endif
				endif
			endfor
		endfor
	endfor
endfor
\end{lstlisting}
%%%% \\endfold
%
% \\endfold Apendice


% \\beginfold Bibliografia
\addcontentsline{toc}{chapter}{Referência Bibliográfica}

%%%%%%%%%%%%%%%%%%% BIbliografia
\begin{thebibliography}{9999}  

\bibitem[Elon1]{elon}Lima, Elon Lages;
\emph{Espaços Métricos, }Projeto Euclides, CNPQ, 2003.

\bibitem[Elon2]{elon2}Lima, Elon Lages;
\emph{Elementos de Topologia Geral, }Editora SBM, Rio de Janeiro, 2009.

\bibitem[Pedro Fernandez]{fernandez}Fernandez, Pedro J.;
\emph{Medida e Integração, }Projeto Euclides, Rio de Janeiro, 2007.

\bibitem[Carlos Isnard]{isnard}Isnard, Carlos;
\emph{Introdução à Medida e Integração, }Projeto Euclides, Rio de Janeiro, 2007.

\bibitem[Stillwell]{stillwell}Stillwell, John;
\emph{Naive Lie Theory, }Springer Verlag, 2008.

\bibitem[Summer]{paper1}Summer, Jeremy G.;
\emph{Lie geometry of $2 \times 2$ Markov matrices, }2017.

\bibitem[Baez and Fong]{noether}Baez, John C. and Fong, Brendan;
\emph{A Noether Theorem for Markov Processes, }2012.

\bibitem[San Martin]{algebra}San Martin, Luiz Antonio Barrera;
\emph{Álgebras de Lie, }Editora UNICAMP, 2010.

\bibitem[Octave]{octave} {John W. Eaton, David Bateman, S{\o}ren Hauberg, and Rik Wehbring};
\emph{{GNU Octave} version 4.0.0 manual: a high-level interactive language for numerical computations, } 2015.

\bibitem[Barry James]{barry}James, Barry R.;
\emph{Probabilidade: um curso em nível intermediário, }Projeto Euclides, Rio de Janeiro, 2008.

\bibitem[John L. Kelley]{kelley}Kelley, John L.;
\emph{General Topology, }Dover edition, Mineola, New York, 2017.

\bibitem[Lang]{lang}Lang, Serge;
\emph{$SL_2(\mathbb{R})$, }Springer-Verlag New York Inc., 1985.

\bibitem[Paulo A. Martin]{agozzine}MArtin, Paulo A.;
\emph{Grupos, Corpos e Teoria de Galois, } Editora Livraria da Física, São Paulo, 2010.

\bibitem[Feller]{feller}Feller, William;
\emph{An introduction to Probability Theory and Its Applications Volume I, }John Wiley \& Sons, Inc.,  1968.

\bibitem[Durret1]{durret1}Durret, Richard;
\emph{Essentials of Stochastic Processes, }Springer,  2012.

\bibitem[Biane and Durret]{durret2}Durret, Richard and  Biane, Philippe;
\emph{Lectures on Probability theory, }Springer,  1993.

\bibitem[Belitsky and Sch{\"u}tz]{Nparticulas}V. Belitsky and G.M Sch{\"u}tz;
\emph{Quantum algebra symmetry of the ASEP with second-class particles, }  2015.

\bibitem[Daniela]{daniela}Cuesta, Daniela Sabrina;
\emph{Proceso de inclusión en Z: dualidad,
acoplamiento y propagación de equilibrio local, } Tesis de Licenciatura, Universidad de Buenos Aires,  2015.

\bibitem[Redig]{paper2}Redig, Frank;
\emph{Duality and exactly solvable models in
non-equilibrium, } 2015.

\bibitem[Aidan Sudbury and Peter Lloyd ]{dual2}Sudbury, Aidan and  Lloyd, Peter;
\emph{Quantum Operators in Classical Probability Theory: II. 
The Concept of Duality in Interacting Particle Systems, } The Annals of Probability,
Vol. 23, $\text{N}^o$ 4 (oct., 1995), pp 1816-1830.

\bibitem[Mac Lane and Birkhoff]{tensor}Mac Lane, Saunders and Birkhoff, Garret;
\emph{Algebra, }Chelsea Publishing Company, New York, Third Edition, 1988.

\bibitem[Frank Redig and Federico Sau]{redig}Redig, Frank and Sau, Federico;
\emph{Stochastic duality and eigenvectors,} 2018.

\bibitem[Ken Brown]{coalgebra}Brown, Ken A.;
\emph{Hopf Algebras, }Lecture Notes.
%\bibitem[GJT]{Guivarc'h}GUIVARC'H, Y.; JI, L.; TAYLOR, J. C.;
%\emph{Compactifications of Symmetric Spaces, }Birkh\"{a}user, 1998.

%\bibitem[Hu]{Hu}HUMPHREYS, J.E., \emph{Linear Algebraic \ Groups, }Springer
%Verlag, New York, 1975.

%\bibitem[Ma]{Margulis}MARGULIS, M. A.; \emph{Discrete Subgroups of Semisimples
%Lie Groups, }Springer Verlag 1990.

%\bibitem[Ron1]{Ronan}RONAN, MARK; \emph{Lecture on Buildings, }Perspectives in
%Mathematics, Academic Press, 1989.
\end{thebibliography}
% \\endfold Bibliografia


\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Índice Remissivo}
%\printindex
\end{document}	
